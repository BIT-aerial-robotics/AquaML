<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://aquaml.github.io/BaseRLAlgo/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>rlalgo:BaseRLAlgo - AquaML</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "rlalgo:BaseRLAlgo";
        var mkdocs_page_input_path = "BaseRLAlgo.md";
        var mkdocs_page_url = "/BaseRLAlgo/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> AquaML
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../DataPool/">data:DataPool</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../DataUnit/">data:DataUnit</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">rlalgo:BaseRLAlgo</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ExplorePolicy/">rlalgo:ExplorePolicy</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">AquaML</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>rlalgo:BaseRLAlgo</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-class">


<a id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo"></a>
  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="AquaML.BaseClass.BaseAlgo">BaseAlgo</span></code>, <code>abc.<span title="abc.ABC">ABC</span></code></p>

  
      <p>create base for reinforcement learning algorithm.
    This base class provides exploration policy, data pool(multi thread).
    Some tools are also provided for reinforcement learning algorithm such as
    calculate general advantage estimation.</p>
<pre><code>When you create a reinforcement learning algorithm, you should inherit this class. And do the following things:

1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function.

2. You need to point out which model is the actor, then in __init__ function, you should write:
   "self.actor = actor"

3. Explore policy is a function which is used to generate action. You should use or create a explore policy.
Then in __init__ function, you should write:
"self.explore_policy = explore_policy"
You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase).

4. Notice: after optimize the model, you should update optimize_epoch.
   The same as sample_epoch.

5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by
   setting  self._sync_model_dict


Some recommends for off-policy algorithm:
1. mini_buffer_size should have given out.
</code></pre>


        <details class="quote">
          <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
          <pre class="highlight"><code class="language-python">class BaseRLAlgo(BaseAlgo, abc.ABC):
    """
    create base for reinforcement learning algorithm.
        This base class provides exploration policy, data pool(multi thread).
        Some tools are also provided for reinforcement learning algorithm such as
        calculate general advantage estimation.

        When you create a reinforcement learning algorithm, you should inherit this class. And do the following things:

        1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function.

        2. You need to point out which model is the actor, then in __init__ function, you should write:
           "self.actor = actor"

        3. Explore policy is a function which is used to generate action. You should use or create a explore policy.
        Then in __init__ function, you should write:
        "self.explore_policy = explore_policy"
        You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase).

        4. Notice: after optimize the model, you should update optimize_epoch.
           The same as sample_epoch.

        5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by
           setting  self._sync_model_dict


        Some recommends for off-policy algorithm:
        1. mini_buffer_size should have given out.
    """

    # TODO:统一输入接口
    # TODO:判断是否启动多线程 (done)  

    def __init__(self, env, rl_io_info: RLIOInfo, name: str, update_interval: int = 0, mini_buffer_size: int = 0,
                 calculate_episodes=5,
                 display_interval=1,
                 computer_type: str = 'PC',
                 level: int = 0, thread_ID: int = -1, total_threads: int = 1, policy_type: str = 'off'):
        """create base for reinforcement learning algorithm.
        This base class provides exploration policy, data pool(multi thread).
        Some tools are also provided for reinforcement learning algorithm such as 
        calculate general advantage estimation. 

        When you create a reinforcement learning algorithm, you should inherit this class. And do the following things:

        1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function.

        2. You need to point out which model is the actor, then in __init__ function, you should write:
           "self.actor = actor"

        3. Explore policy is a function which is used to generate action. You should use or create a explore policy. 
        Then in __init__ function, you should write:
        "self.explore_policy = explore_policy"
        You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 

        4. Notice: after optimize the model, you should update optimize_epoch.
           The same as sample_epoch.

        5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by 
           setting  self._sync_model_dict


        Some recommends for off-policy algorithm:
        1. mini_buffer_size should have given out.



        Args:
            env (AquaML.rlalgo.EnvBase): reinforcement learning environment.

            rl_io_info (RLIOInfo): reinforcement learning input and output information.

            name (str): reinforcement learning name.

            update_interval (int, optional): update interval. This is an important parameter. 
            It determines how many steps to update the model. If update_interval == 1, it means update model after each step.
            if update_interval == 0, it means update model after buffer is full. Defaults to 0.
            That is unusually used in on-policy algorithm.
            if update_interval &gt; 0, it is used in off-policy algorithm.

            mini_buffer_size (int, optional): mini buffer size. Defaults to 0.

            calculate_episodes (int): How many episode will be used to summary. We recommend to use 1 when using multi thread.

            display_interval (int, optional): display interval. Defaults to 1.

            computer_type (str, optional): 'PC' or 'HPC'. Defaults to 'PC'.

            level (int, optional): thread level. 0 means main thread, 1 means sub thread. Defaults to 0.

            thread_ID (int, optional): ID is given by mpi. -1 means single thread. Defaults to -1.

            total_threads (int, optional): total threads. Defaults to 1.

            policy_type (str, optional): 'off' or 'on'. Defaults to 'off'.

        Raises:
            ValueError: if thread_ID == -1, it means single thread, then level must be 0.
            ValueError: if computer_type == 'HPC', then level must be 0.
            ValueError('Sync model must be given.')

        """

        self.recoder = None
        self.explore_policy = None
        self.tf_log_std = None
        self.log_std = None
        self.rl_io_info = rl_io_info
        # if thread_ID == -1, it means single thread
        self.name = name
        self.env = env
        self.update_interval = update_interval
        self.mini_buffer_size = mini_buffer_size
        self.display_interval = display_interval

        self.cache_path = name + '/cache'  # cache path
        self.log_path = name + '/log'

        # parameter of multithread
        self._computer_type = computer_type
        self.level = level
        self.thread_ID = thread_ID
        self.total_threads = total_threads  # main thread is not included
        if self.total_threads == 1:
            self.sample_threads = total_threads
        else:
            self.sample_threads = total_threads - 1
        self.each_thread_summary_episodes = calculate_episodes
        '''
        If it runs in single thread, self.thread_ID == 0, self.total_threads == 1
        '''

        # create data pool according to thread level
        self.data_pool = DataPool(name=self.name, level=self.level,
                                  computer_type=self._computer_type)  # data_pool is a handle

        self.actor = None  # actor model. Need point out which model is the actor.

        # TODO: 私有变量会出问题，貌似这个没用
        self._explore_dict = {}  # store explore policy, convenient for multi thread

        # TODO: 需要升级为异步执行的方式
        # TODO: 需要确认主线程和子线程得到得硬件不一样是否影响执行速度 
        # allocate start index and size for each thread
        # main thread will part in sample data
        # just used when computer_type == 'PC'
        if self._computer_type == 'PC':
            if self.thread_ID &gt; 0:
                # thread ID start from 0
                self.each_thread_size = int(self.rl_io_info.buffer_size / self.sample_threads)
                self.each_thread_start_index = int((self.thread_ID - 1) * self.each_thread_size)

                self.max_buffer_size = self.each_thread_size * self.sample_threads

                # if mini_buffer_size == 0, it means pre-sample data is disabled
                self.each_thread_mini_buffer_size = int(self.mini_buffer_size / self.sample_threads)
                self.mini_buffer_size = int(self.each_thread_mini_buffer_size * self.sample_threads)

                if self.update_interval == 0:
                    # 这种情形属于将所有buffer填充满以后再更新模型
                    # if update_interval == 0, it means update model after buffer is full
                    self.each_thread_update_interval = self.each_thread_size  # update interval for each thread
                else:
                    # if update_interval != 0, it means update model after each step
                    # then we need to calculate how many steps to update model for each thread
                    # 每个线程更新多少次等待更新模型
                    self.each_thread_update_interval = int(
                        self.update_interval / self.sample_threads)  # update interval for each thread
                if self.level &gt; 0:
                    self.sample_id = self.thread_ID - 1
                else:
                    self.sample_id = 0
            else:
                self.each_thread_size = self.rl_io_info.buffer_size
                self.each_thread_start_index = 0
                self.each_thread_mini_buffer_size = self.mini_buffer_size
                if self.update_interval == 0:
                    self.each_thread_update_interval = self.each_thread_size  # update interval for each thread
                else:
                    self.each_thread_update_interval = self.update_interval  # update interval for each thread

                self.max_buffer_size = self.each_thread_size

                self.thread_ID = 0
                self.sample_id = 0  # sample id is used to identify which thread is sampling data

                # self.each_thread_update_interval = self.update_interval # update interval for each thread

        else:
            # TODO: HPC will implement in the future
            self.each_thread_size = None
            self.each_thread_start_index = None
            self.each_thread_update_interval = None

        # create worker
        if self.total_threads &gt; 1:
            if self.level == 0:
                self.env = None
                self.worker = None
            else:
                self.worker = RLWorker(self)
        else:
            self.worker = RLWorker(self)

        # initial main thread
        if self.level == 0:
            # resample action
            # TODO: 优化此处命名
            if self.rl_io_info.explore_info == 'self-std':
                self.resample_action = self._resample_action_log_std
                self.resample_log_prob = self._resample_log_prob_with_std
            elif self.rl_io_info.explore_info == 'global-std':
                self.resample_action = self._resample_action_no_log_std
                self.resample_log_prob = self._resample_log_prob_no_std
            elif self.rl_io_info.explore_info == 'void-std':
                self.resample_action = self._resample_action_log_prob
                self.resample_log_prob = None

        # hyper parameters
        # the hyper parameters is a dictionary
        # you should point out the hyper parameters in your algorithm
        # will be used in optimize function
        self.hyper_parameters = None

        # optimizer are created in main thread
        self.optimizer_dict = {}  # store optimizer, convenient search

        self.total_segment = self.sample_threads  # total segment, convenient for multi

        self.sample_epoch = 0  # sample epoch
        self.optimize_epoch = 0  # optimize epoch

        self.policy_type = policy_type  # 'off' or 'on'

        # mini buffer size 
        # according to the type of algorithm,

        self._sync_model_dict = {}  # store sync model, convenient for multi thread
        self._sync_explore_dict = {}  # store sync explore policy, convenient for multi thread

        self._all_model_dict = {}  # store all model, convenient to record model

    # initial algorithm
    ############################# key component #############################
    def init(self):
        """initial algorithm.

        This function will be called by starter.
        """
        # multi thread communication

        reward_info_dict = {}

        for name in self.rl_io_info.reward_info:
            reward_info_dict['summary_' + name] = (
                self.total_segment * self.each_thread_summary_episodes, 1)

        # add summary reward information to data pool
        for name, shape in reward_info_dict.items():
            # this must be first level name
            buffer = DataUnit(name=self.name + '_' + name, shape=shape, dtype=np.float32, level=self.level,
                              computer_type=self._computer_type)
            self.rl_io_info.add_info(name=name, shape=shape, dtype=np.float32)
            self.data_pool.add_unit(name=name, data_unit=buffer)

        # TODO:子线程需要等待时间 check
        # multi thread initial
        if self.total_threads &gt; 1:  # multi thread
            # print(self.rl_io_info.data_info)
            self.data_pool.multi_init(self.rl_io_info.data_info, type='buffer')
        else:  # single thread
            self.data_pool.create_buffer_from_dic(self.rl_io_info.data_info)

        # just do in m main thread
        if self.level == 0:
            # initial recoder
            self.recoder = Recoder(log_folder=self.log_path)
        else:
            self.recoder = None

        # check some information
        # actor model must be given
        if self.actor is None:
            raise ValueError('Actor model must be given.')

    def optimize(self):

        # compute current reward information

        optimize_info = self._optimize_()

        # all the information update here
        self.optimize_epoch += 1

        total_steps = self.get_current_steps

        optimize_info['total_steps'] = total_steps

        if self.optimize_epoch % self.display_interval == 0:
            # display information

            epoch = int(self.optimize_epoch / self.display_interval)
            reward_info = self.summary_reward_info()
            print("###############epoch: {}###############".format(epoch))
            self.recoder.display_text(
                reward_info
            )
            self.recoder.display_text(
                optimize_info
            )

            self.recoder.record(reward_info, total_steps, prefix='reward')
            self.recoder.record(optimize_info, self.optimize_epoch, prefix=self.name)
            # record weight
            for key, model in self._all_model_dict.items():
                self.recoder.record_weight(model, total_steps, prefix=key)

    def check(self):
        """
        check some information.
        """
        if self.policy_type == 'off':
            if self.mini_buffer_size is None:
                raise ValueError('Mini buffer size must be given.')
        if self._sync_model_dict is None:
            raise ValueError('Sync model must be given.')

    ############################# key function #############################
    def store_data(self, obs: dict, action: dict, reward: dict, next_obs: dict, mask: int):
        """
        store data to buffer.

        Args:
            obs (dict): observation. eg. {'obs':np.array([1,2,3])}
            action (dict): action. eg. {'action':np.array([1,2,3])}
            reward (dict): reward. eg. {'reward':np.array([1,2,3])}
            next_obs (dict): next observation. eg. {'next_obs':np.array([1,2,3])}
            mask (int): done. eg. 1 or 0
        """
        # store data to buffer
        # support multi thread

        idx = (self.worker.step_count - 1) % self.each_thread_size
        index = self.each_thread_start_index + idx  # index in each thread

        # store obs to buffer
        self.data_pool.store(obs, index)

        # store next_obs to buffer
        self.data_pool.store(next_obs, index, prefix='next_')

        # store action to buffer
        self.data_pool.store(action, index)

        # store reward to buffer
        self.data_pool.store(reward, index)

        # store mask to buffer
        self.data_pool.data_pool['mask'].store(mask, index)

    @staticmethod
    def copy_weights(model1, model2):
        """
        copy weight from model1 to model2.
        """
        new_weights = []
        target_weights = model1.weights

        for i, weight in enumerate(model2.weights):
            new_weights.append(target_weights[i].numpy())

        model2.set_weights(new_weights)

    @staticmethod
    def soft_update_weights(model1, model2, tau):
        """
        soft update weight from model1 to model2.


        args:
        model1: source model
        model2: target model
        """
        new_weights = []
        source_weights = model1.weights

        for i, weight in enumerate(model2.weights):
            new_weights.append((1 - tau) * weight.numpy() + tau * source_weights[i].numpy())

        model2.set_weights(new_weights)

    def summary_reward_info(self):
        """
        summary reward information.
        """
        # calculate reward information

        summary_reward_info = {}

        for name in self.rl_io_info.reward_info:
            summary_reward_info[name] = np.mean(self.data_pool.get_unit_data('summary_' + name))

        summary_reward_info['std'] = np.std(self.data_pool.get_unit_data('summary_total_reward'))
        summary_reward_info['max_reward'] = np.max(self.data_pool.get_unit_data('summary_total_reward'))
        summary_reward_info['min_reward'] = np.min(self.data_pool.get_unit_data('summary_total_reward'))

        return summary_reward_info

        # calculate episode reward information

        # random sample

    def random_sample(self, batch_size: int):
        """
            random sample data from buffer.

            Args:
                batch_size (int): batch size.

            Returns:
                _type_: dict. data dict.
            """
        # if using multi thread, then sample data from each segment
        # sample data from each segment

        # compute current segment size
        running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment
        buffer_size = min(self.max_buffer_size, running_step)

        batch_size = min(batch_size, buffer_size)

        sample_index = np.random.choice(range(buffer_size), batch_size, replace=False)

        # index_bias = (sample_index * 1.0 / self.each_thread_size) * self.each_thread_size
        index_bias = sample_index / self.each_thread_size
        index_bias = index_bias.astype(np.int32)
        index_bias = index_bias * self.each_thread_size

        sample_index = sample_index + index_bias
        sample_index = sample_index.astype(np.int32)

        # get data

        data_dict = self.data_pool.get_data_by_indices(sample_index, self.rl_io_info.store_data_name)

        return data_dict

    def cal_episode_info(self):
        """
            calculate episode reward information.

            Returns:
                _type_: dict. summary reward information.
            """

        # data_dict = self.get_current_update_data(('reward', 'mask'))
        # calculate current reward information

        # get done flag
        index_done = np.where(self.data_pool.get_unit_data('mask') == 0)[0] + 1
        index_done_ = index_done / self.each_thread_size
        index_done_ = index_done_.astype(np.int32)

        # config segment
        segment_index = np.arange((0, self.total_segment))
        every_segment_index = []

        # split index_done
        for segment_id in segment_index:
            segment_index_done = np.where(index_done_ == segment_id)[0]
            every_segment_index.append(index_done[segment_index_done])

        reward_dict = {}
        for key in self.rl_io_info.reward_info:
            reward_dict[key] = []

        for each_segment_index in every_segment_index:
            # get index of done
            compute_index = each_segment_index[-self.each_thread_summary_episodes:]
            start_index = compute_index[0]

            for end_index in compute_index[1:]:
                for key in self.rl_io_info.reward_info:
                    reward_dict[key].append(np.sum(self.data_pool.get_unit_data(key)[start_index:end_index]))
                start_index = end_index

        # summary reward information
        reward_summary = {'std': np.std(reward_dict['total_reward']),
                          'max_reward': np.max(reward_dict['total_reward']),
                          'min_reward': np.min(reward_dict['total_reward'])}

        for key in self.rl_io_info.reward_info:
            reward_summary[key] = np.mean(reward_dict[key])

        # delete list
        del reward_dict

        return reward_summary

    def cal_average_batch_dict(self, data_list: list):
        """
            calculate average batch dict.

            Args:
                data_list (list): store data dict list.

            Returns:
                _type_: dict. average batch dict.
            """
        average_batch_dict = {}
        for key in data_list[0]:
            average_batch_dict[key] = []

        for data_dict in data_list:
            for key, value in data_dict.items():
                average_batch_dict[key].append(value)

        # average
        for key, value in average_batch_dict.items():
            average_batch_dict[key] = np.mean(value)

        return average_batch_dict

    # TODO: calculate by multi thread
    ############################# calculate reward information #############################
    # calculate general advantage estimation
    def calculate_GAE(self, rewards, values, next_values, masks, gamma, lamda):
        """
        calculate general advantage estimation.

        Reference:
        ----------
        [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous 
        control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8.

        Args:
            rewards (np.ndarray): rewards.
            values (np.ndarray): values.
            next_values (np.ndarray): next values.
            masks (np.ndarray): dones.
            gamma (float): discount factor.
            lamda (float): general advantage estimation factor.
        Returns:
            np.ndarray: general advantage estimation.
        """
        gae = np.zeros_like(rewards)
        n_steps_target = np.zeros_like(rewards)
        cumulated_advantage = 0.0
        length = len(rewards)
        index = length - 1

        # td_target = rewards + gamma * next_values * masks
        # td_delta = td_target - values
        # advantage = compute_advantage(self.hyper_parameters.gamma, self.hyper_parameters.lambada, td_delta)
        for i in range(length):
            index -= 1
            delta = rewards[index] + gamma * next_values[index] - values[index]
            cumulated_advantage = gamma * lamda * masks[index] * cumulated_advantage + delta
            gae[index] = cumulated_advantage
            n_steps_target[index] = gae[index] + values[index]

        return gae, n_steps_target

    # calculate discounted reward
    def calculate_discounted_reward(self, rewards, masks, gamma):
        """
        calculate discounted reward.
        Args:
            rewards (np.ndarray): rewards.
            masks (np.ndarray): dones. if done, mask = 0, else mask = 1.
            gamma (float): discount factor.
        Returns:
            np.ndarray: discounted reward.
        """
        discounted_reward = np.zeros_like(rewards)
        cumulated_reward = 0.0
        length = len(rewards)
        index = length - 1

        for i in range(length):
            index = index - 1
            cumulated_reward = rewards[index] + gamma * cumulated_reward * masks[index]
            discounted_reward[index] = cumulated_reward

        return discounted_reward

    ############################# create function #############################
    # create keras optimizer
    def create_optimizer(self, name: str, optimizer: str, lr: float):
        """
        create keras optimizer for each model.

        Reference:
            https://keras.io/optimizers/

        Args:
            name (str): name of this optimizer, you can call by this name.
            if name is 'actor', then you can call self.actor_optimizer

            optimizer (str): type of optimizer. eg. 'Adam'. For more information,
            please refer to keras.optimizers.

            lr (float): learning rate.
        """

        attribute_name = name + '_optimizer'

        # in main thread, create optimizer
        if self.level == 0:
            # create optimizer
            optimizer = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr)
        else:
            # None
            optimizer = None

        # set attribute
        setattr(self, attribute_name, optimizer)

        self.optimizer_dict[name] = getattr(self, attribute_name)

    def create_none_optimizer(self, name: str):
        """
        create none optimizer for each model.

        Args:
            name (str): name of this optimizer, you can call by this name.
        """

        attribute_name = name + '_optimizer'
        optimizer = None
        setattr(self, attribute_name, optimizer)

    # Gaussian exploration policy
    def create_gaussian_exploration_policy(self):
        # TODO: sync with tf_std
        # verity the style of log_std
        if self.rl_io_info.explore_info == 'self-std':
            # log_std provided by actor
            # create explore policy
            # self.__explore_dict = None
            self.explore_policy = GaussianExplorePolicy(shape=self.rl_io_info.actor_out_info['action'])

        elif self.rl_io_info.explore_info == 'global-std':
            # log_std provided by auxiliary variable
            # create args by data unit
            self.log_std = DataUnit(name=self.name + '_log_std', dtype=np.float32,
                                    shape=self.rl_io_info.actor_out_info['action'],
                                    level=self.level, computer_type=self._computer_type)

            self.log_std.set_value(np.zeros(self.rl_io_info.actor_out_info['action'], dtype=np.float32) - 0.5)
            self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True)
            self._explore_dict = {'log_std': self.tf_log_std}

            self.rl_io_info.add_info(name='log_std', shape=self.log_std.shape, dtype=self.log_std.dtype)
            self.data_pool.add_unit(name='log_std', data_unit=self.log_std)
            self.explore_policy = GaussianExplorePolicy(shape=self.rl_io_info.actor_out_info['action'])
        elif self.rl_io_info.explore_info == 'void-std':
            # log_std is void
            self.explore_policy = VoidExplorePolicy(shape=self.rl_io_info.actor_out_info['action'])

    ############################# get function ################################
    def get_action_train(self, obs: dict):
        """

        sample action in the training process.

        Args:
            obs (dict): observation from environment. eg. {'obs':data}.
                        The data must be tensor. And its shape is (batch, feature).

        Returns:
            _type_: _description_
        """

        input_data = []

        # get actor input
        for key in self.actor.input_name:
            input_data.append(tf.cast(obs[key], dtype=tf.float32))

        actor_out = self.actor(*input_data)  # out is a tuple

        policy_out = dict(zip(self.actor.output_info, actor_out))

        for name, value in self._explore_dict.items():
            policy_out[name] = value

        action, prob = self.explore_policy(policy_out)

        policy_out['action'] = action
        policy_out['prob'] = prob

        # create return dict according to rl_io_info.actor_out_name
        return_dict = dict()
        for name in self.rl_io_info.actor_out_name:
            return_dict[name] = policy_out[name]

        return return_dict

    def get_batch_data(self, data_dict: dict, start_index, end_index):
        """
        Get batch data from data dict.

        The data type stored in data_dict must be tuple or tensor or array.

        Example:
            &gt;&gt;&gt; data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)}
            &gt;&gt;&gt; start_index = 0
            &gt;&gt;&gt; end_index = 5
            &gt;&gt;&gt; self.get_batch_data(data_dict, start_index, end_index)
            {'obs': (array([1, 2, 3, 4, 5]),)}

        Args:
            data_dict (dict): data dict.
            start_index (int): start index.
            end_index (int): end index.
        Returns:
            batch data. dict.
        """
        batch_data = dict()
        for key, values in data_dict.items():
            if isinstance(values, tuple) or isinstance(values, list):
                buffer = []
                for value in values:
                    buffer.append(value[start_index:end_index])
                batch_data[key] = tuple(buffer)
            else:
                batch_data[key] = values[start_index:end_index]

        return batch_data

    # get trainable actor
    @property
    def get_trainable_actor(self):
        """
        get trainable weights of this model.

        actor model is special, it has two parts, actor and explore policy.
        Maybe in some times, explore policy is independent on actor model.
        """

        train_vars = self.actor.trainable_variables

        for key, value in self._explore_dict.items():
            train_vars += [value]

        return train_vars

    # optimize in the main thread

    def get_corresponding_data(self, data_dict: dict, names: tuple, prefix: str = '', tf_tensor: bool = True):
        """

        Get corresponding data from data dict.

        Args:
            data_dict (dict): data dict.
            names (tuple): name of data.
            prefix (str): prefix of data name.
            tf_tensor (bool): if return tf tensor.
        Returns:
            corresponding data. list or tuple.
        """

        data = []

        for name in names:
            name = prefix + name
            buffer = data_dict[name]
            if tf_tensor:
                buffer = tf.cast(buffer, dtype=tf.float32)
            data.append(buffer)

        return data

    # acquire current update buffer
    def get_current_update_data(self, names: tuple):
        """
        Get current update data.

        Args:
            names (tuple): data name.
        Returns:
            _type_: dict. data dict.
        """
        # running after optimize
        # compute sampling interval
        start_index = (self.optimize_epoch - 1) * self.each_thread_update_interval
        end_index = self.optimize_epoch * self.each_thread_update_interval

        index_bias = np.arange(0, self.total_segment) * self.each_thread_size

        return_dict = {}

        for name in names:
            return_dict[name] = []

        for bias in index_bias:
            start_index = start_index + bias
            end_index = end_index + bias

            data_dict = self.data_pool.get_data_by_indices(np.arange(start_index, end_index).tolist(), names)

            for key, ls in return_dict.items():
                ls.append(data_dict[key])

        # concat data
        for key, ls in return_dict.items():
            return_dict[key] = np.concatenate(ls, axis=0)

        return return_dict

    @property
    def get_all_data(self):
        """
        get all data in buffer.
        """

        return_dict = {}

        for key, unit in self.data_pool.data_pool.items():
            return_dict[key] = unit.buffer

        return return_dict

    @property
    def get_current_buffer_size(self):
        """
        compute current step.
        """
        running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment
        buffer_size = min(self.max_buffer_size, running_step)
        return buffer_size

    @property
    def get_current_steps(self):
        """
        compute current step.
        """
        running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.sample_threads
        return running_step

    ############################# resample function ################################
    # resample action method

    @tf.function
    def _resample_action_no_log_std(self, actor_obs: tuple):
        """
        Explore policy in SAC2 is Gaussian  exploration policy.

        _resample_action_no_log_std is used when actor model's out has no log_std.

        The output of actor model is (mu,).

        Args:
            actor_obs (tuple): actor model's input
        Returns:
        action (tf.Tensor): action
        log_pi (tf.Tensor): log_pi
        """

        mu = self.actor(*actor_obs)[0]

        noise, prob = self.explore_policy.noise_and_prob(self.hyper_parameters.batch_size)

        sigma = tf.exp(self.tf_log_std)
        action = mu + noise * sigma
        log_pi = tf.math.log(prob)

        return action, log_pi

    @tf.function
    def _resample_action_log_std(self, actor_obs: tuple):
        """
        Explore policy in SAC2 is Gaussian  exploration policy.

        _resample_action_log_std is used when actor model's out has log_std.

        The output of actor model is (mu, log_std).

        Args:
            actor_obs (tuple): actor model's input
        Returns:
        action (tf.Tensor): action
        log_pi (tf.Tensor): log_pi
        """

        out = self.actor(*actor_obs)

        mu, log_std = out[0], out[1]

        noise, prob = self.explore_policy.noise_and_prob(self.hyper_parameters.batch_size)

        sigma = tf.exp(log_std)

        action = mu + noise * sigma

        log_prob = tf.math.log(prob)

        return action, log_prob

    # @tf.function
    def _resample_action_log_prob(self, actor_obs: tuple):
        """
        Explore policy in SAC2 is Gaussian  exploration policy.

        _resample_action_log_prob is used when actor model's out has log_prob.

        The output of actor model is (mu, log_std).

        Args:
            actor_obs (tuple): actor model's input
        Returns:
        action (tf.Tensor): action
        log_pi (tf.Tensor): log_pi
        """

        action, log_prob = self.actor(*actor_obs)

        return action, log_prob

    def _resample_log_prob_no_std(self, obs, action):

        """
        Re get log_prob of action.
        The output of actor model is (mu,).
        It is different from resample_action.

        Args:
            obs (tuple): observation.
            action (tf.Tensor): action.
        """

        out = self.actor(*obs)
        mu = out[0]
        std = tf.exp(self.tf_log_std)
        log_prob = self.explore_policy.resample_prob(mu, std, action)

        return log_prob

    # def _resample_log_prob_with_std(self, obs, action):

    def _resample_log_prob_with_std(self, obs, action):
        """
        Re get log_prob of action.
        The output of actor model is (mu, log_std,).
        It is different from resample_action.

        """

        out = self.actor(*obs)
        mu = out[0]
        log_std = out[1]
        std = tf.exp(log_std)
        log_prob = self.explore_policy.resample_prob(mu, std, action)

        return log_prob

    def concat_dict(self, dict_tuple: tuple):
        """
        concat dict.

        Args:
            dict_tuple (tuple): dict tuple.
        Returns:
            _type_: dict. concat dict.
        """
        concat_dict = {}
        for data_dict in dict_tuple:
            for key, value in data_dict.items():
                if key in concat_dict:
                    Warning('key {} is already in concat dict'.format(key))
                else:
                    concat_dict[key] = value

        return concat_dict

    def initialize_model_weights(self, model):
        """
        initial model.
        """

        input_data_name = model.input_name

        # create tensor according to input data name
        input_data = []

        for name in input_data_name:
            shape, _ = self.rl_io_info.get_data_info(name)
            data = tf.zeros(shape=shape, dtype=tf.float32)
            input_data.append(data)

        model(*input_data)

    def sync(self):
        """
        sync.
        Used in multi thread.

        """
        if self.level == 0:
            for key, model in self._sync_model_dict.items():
                model.save_weights(self.cache_path + '/' + key + '.h5')
        else:
            for key, model in self._sync_model_dict.items():
                model.load_weights(self.cache_path + '/' + key + '.h5')

        if self.log_std is not None:
            self.sync_log_std()

    def close(self):
        """
        close.
        """
        self.data_pool.close()

    def sync_log_std(self):
        """
        sync log std.
        """

        if self.level == 0:
            self.log_std.set_value(self.tf_log_std.numpy())  # write log std to shared memory
        else:
            self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True)  # read log std from shared memory

    # optimize model
    @abc.abstractmethod
    def _optimize_(self, *args, **kwargs):
        """
        optimize model.
        It is a abstract method.

        Recommend when you implement this method, input of this method should be hyperparameters.
        The hyperparameters can be tuned in the training process.

        Returns:
            _type_: dict. Optimizer information. eg. {'loss':data, 'total_reward':data}
        """</code></pre>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.each_thread_summary_episodes" class="doc doc-heading">
<code class="highlight language-python">each_thread_summary_episodes = calculate_episodes</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>If it runs in single thread, self.thread_ID == 0, self.total_threads == 1</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_all_data" class="doc doc-heading">
<code class="highlight language-python">get_all_data</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>get all data in buffer.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_buffer_size" class="doc doc-heading">
<code class="highlight language-python">get_current_buffer_size</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>compute current step.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_steps" class="doc doc-heading">
<code class="highlight language-python">get_current_steps</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>compute current step.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_trainable_actor" class="doc doc-heading">
<code class="highlight language-python">get_trainable_actor</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>get trainable weights of this model.</p>
<p>actor model is special, it has two parts, actor and explore policy.
Maybe in some times, explore policy is independent on actor model.</p>
  </div>

</div>



<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.__init__" class="doc doc-heading">
<code class="highlight language-python">__init__(env, rl_io_info, name, update_interval=0, mini_buffer_size=0, calculate_episodes=5, display_interval=1, computer_type='PC', level=0, thread_ID=-1, total_threads=1, policy_type='off')</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>create base for reinforcement learning algorithm.
This base class provides exploration policy, data pool(multi thread).
Some tools are also provided for reinforcement learning algorithm such as 
calculate general advantage estimation. </p>
<p>When you create a reinforcement learning algorithm, you should inherit this class. And do the following things:</p>
<ol>
<li>
<p>You should run init() function in your <strong>init</strong> function. The position of init() function is at the end of <strong>init</strong> function.</p>
</li>
<li>
<p>You need to point out which model is the actor, then in <strong>init</strong> function, you should write:
   "self.actor = actor"</p>
</li>
<li>
<p>Explore policy is a function which is used to generate action. You should use or create a explore policy. 
Then in <strong>init</strong> function, you should write:
"self.explore_policy = explore_policy"
You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). </p>
</li>
<li>
<p>Notice: after optimize the model, you should update optimize_epoch.
   The same as sample_epoch.</p>
</li>
<li>
<p>Notice: if you want to use multi thread, please specify which model needs to be synchronized by 
   setting  self._sync_model_dict</p>
</li>
</ol>
<p>Some recommends for off-policy algorithm:
1. mini_buffer_size should have given out.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>env</code></td>
          <td>
                <code>AquaML.<span title="AquaML.rlalgo">rlalgo</span>.<span title="AquaML.rlalgo.EnvBase">EnvBase</span></code>
          </td>
          <td><p>reinforcement learning environment.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>rl_io_info</code></td>
          <td>
                <code><span title="AquaML.DataType.RLIOInfo">RLIOInfo</span></code>
          </td>
          <td><p>reinforcement learning input and output information.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>reinforcement learning name.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>update_interval</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>update interval. This is an important parameter. </p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>mini_buffer_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>mini buffer size. Defaults to 0.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>calculate_episodes</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>How many episode will be used to summary. We recommend to use 1 when using multi thread.</p></td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>display_interval</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>display interval. Defaults to 1.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>computer_type</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>'PC' or 'HPC'. Defaults to 'PC'.</p></td>
          <td>
                <code>&#39;PC&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>level</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>thread level. 0 means main thread, 1 means sub thread. Defaults to 0.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>thread_ID</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>ID is given by mpi. -1 means single thread. Defaults to -1.</p></td>
          <td>
                <code>-1</code>
          </td>
        </tr>
        <tr>
          <td><code>total_threads</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>total threads. Defaults to 1.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>policy_type</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>'off' or 'on'. Defaults to 'off'.</p></td>
          <td>
                <code>&#39;off&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td><p>if thread_ID == -1, it means single thread, then level must be 0.</p></td>
        </tr>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td><p>if computer_type == 'HPC', then level must be 0.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def __init__(self, env, rl_io_info: RLIOInfo, name: str, update_interval: int = 0, mini_buffer_size: int = 0,
             calculate_episodes=5,
             display_interval=1,
             computer_type: str = 'PC',
             level: int = 0, thread_ID: int = -1, total_threads: int = 1, policy_type: str = 'off'):
    """create base for reinforcement learning algorithm.
    This base class provides exploration policy, data pool(multi thread).
    Some tools are also provided for reinforcement learning algorithm such as 
    calculate general advantage estimation. 

    When you create a reinforcement learning algorithm, you should inherit this class. And do the following things:

    1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function.

    2. You need to point out which model is the actor, then in __init__ function, you should write:
       "self.actor = actor"

    3. Explore policy is a function which is used to generate action. You should use or create a explore policy. 
    Then in __init__ function, you should write:
    "self.explore_policy = explore_policy"
    You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 

    4. Notice: after optimize the model, you should update optimize_epoch.
       The same as sample_epoch.

    5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by 
       setting  self._sync_model_dict


    Some recommends for off-policy algorithm:
    1. mini_buffer_size should have given out.



    Args:
        env (AquaML.rlalgo.EnvBase): reinforcement learning environment.

        rl_io_info (RLIOInfo): reinforcement learning input and output information.

        name (str): reinforcement learning name.

        update_interval (int, optional): update interval. This is an important parameter. 
        It determines how many steps to update the model. If update_interval == 1, it means update model after each step.
        if update_interval == 0, it means update model after buffer is full. Defaults to 0.
        That is unusually used in on-policy algorithm.
        if update_interval &gt; 0, it is used in off-policy algorithm.

        mini_buffer_size (int, optional): mini buffer size. Defaults to 0.

        calculate_episodes (int): How many episode will be used to summary. We recommend to use 1 when using multi thread.

        display_interval (int, optional): display interval. Defaults to 1.

        computer_type (str, optional): 'PC' or 'HPC'. Defaults to 'PC'.

        level (int, optional): thread level. 0 means main thread, 1 means sub thread. Defaults to 0.

        thread_ID (int, optional): ID is given by mpi. -1 means single thread. Defaults to -1.

        total_threads (int, optional): total threads. Defaults to 1.

        policy_type (str, optional): 'off' or 'on'. Defaults to 'off'.

    Raises:
        ValueError: if thread_ID == -1, it means single thread, then level must be 0.
        ValueError: if computer_type == 'HPC', then level must be 0.
        ValueError('Sync model must be given.')

    """

    self.recoder = None
    self.explore_policy = None
    self.tf_log_std = None
    self.log_std = None
    self.rl_io_info = rl_io_info
    # if thread_ID == -1, it means single thread
    self.name = name
    self.env = env
    self.update_interval = update_interval
    self.mini_buffer_size = mini_buffer_size
    self.display_interval = display_interval

    self.cache_path = name + '/cache'  # cache path
    self.log_path = name + '/log'

    # parameter of multithread
    self._computer_type = computer_type
    self.level = level
    self.thread_ID = thread_ID
    self.total_threads = total_threads  # main thread is not included
    if self.total_threads == 1:
        self.sample_threads = total_threads
    else:
        self.sample_threads = total_threads - 1
    self.each_thread_summary_episodes = calculate_episodes
    '''
    If it runs in single thread, self.thread_ID == 0, self.total_threads == 1
    '''

    # create data pool according to thread level
    self.data_pool = DataPool(name=self.name, level=self.level,
                              computer_type=self._computer_type)  # data_pool is a handle

    self.actor = None  # actor model. Need point out which model is the actor.

    # TODO: 私有变量会出问题，貌似这个没用
    self._explore_dict = {}  # store explore policy, convenient for multi thread

    # TODO: 需要升级为异步执行的方式
    # TODO: 需要确认主线程和子线程得到得硬件不一样是否影响执行速度 
    # allocate start index and size for each thread
    # main thread will part in sample data
    # just used when computer_type == 'PC'
    if self._computer_type == 'PC':
        if self.thread_ID &gt; 0:
            # thread ID start from 0
            self.each_thread_size = int(self.rl_io_info.buffer_size / self.sample_threads)
            self.each_thread_start_index = int((self.thread_ID - 1) * self.each_thread_size)

            self.max_buffer_size = self.each_thread_size * self.sample_threads

            # if mini_buffer_size == 0, it means pre-sample data is disabled
            self.each_thread_mini_buffer_size = int(self.mini_buffer_size / self.sample_threads)
            self.mini_buffer_size = int(self.each_thread_mini_buffer_size * self.sample_threads)

            if self.update_interval == 0:
                # 这种情形属于将所有buffer填充满以后再更新模型
                # if update_interval == 0, it means update model after buffer is full
                self.each_thread_update_interval = self.each_thread_size  # update interval for each thread
            else:
                # if update_interval != 0, it means update model after each step
                # then we need to calculate how many steps to update model for each thread
                # 每个线程更新多少次等待更新模型
                self.each_thread_update_interval = int(
                    self.update_interval / self.sample_threads)  # update interval for each thread
            if self.level &gt; 0:
                self.sample_id = self.thread_ID - 1
            else:
                self.sample_id = 0
        else:
            self.each_thread_size = self.rl_io_info.buffer_size
            self.each_thread_start_index = 0
            self.each_thread_mini_buffer_size = self.mini_buffer_size
            if self.update_interval == 0:
                self.each_thread_update_interval = self.each_thread_size  # update interval for each thread
            else:
                self.each_thread_update_interval = self.update_interval  # update interval for each thread

            self.max_buffer_size = self.each_thread_size

            self.thread_ID = 0
            self.sample_id = 0  # sample id is used to identify which thread is sampling data

            # self.each_thread_update_interval = self.update_interval # update interval for each thread

    else:
        # TODO: HPC will implement in the future
        self.each_thread_size = None
        self.each_thread_start_index = None
        self.each_thread_update_interval = None

    # create worker
    if self.total_threads &gt; 1:
        if self.level == 0:
            self.env = None
            self.worker = None
        else:
            self.worker = RLWorker(self)
    else:
        self.worker = RLWorker(self)

    # initial main thread
    if self.level == 0:
        # resample action
        # TODO: 优化此处命名
        if self.rl_io_info.explore_info == 'self-std':
            self.resample_action = self._resample_action_log_std
            self.resample_log_prob = self._resample_log_prob_with_std
        elif self.rl_io_info.explore_info == 'global-std':
            self.resample_action = self._resample_action_no_log_std
            self.resample_log_prob = self._resample_log_prob_no_std
        elif self.rl_io_info.explore_info == 'void-std':
            self.resample_action = self._resample_action_log_prob
            self.resample_log_prob = None

    # hyper parameters
    # the hyper parameters is a dictionary
    # you should point out the hyper parameters in your algorithm
    # will be used in optimize function
    self.hyper_parameters = None

    # optimizer are created in main thread
    self.optimizer_dict = {}  # store optimizer, convenient search

    self.total_segment = self.sample_threads  # total segment, convenient for multi

    self.sample_epoch = 0  # sample epoch
    self.optimize_epoch = 0  # optimize epoch

    self.policy_type = policy_type  # 'off' or 'on'

    # mini buffer size 
    # according to the type of algorithm,

    self._sync_model_dict = {}  # store sync model, convenient for multi thread
    self._sync_explore_dict = {}  # store sync explore policy, convenient for multi thread

    self._all_model_dict = {}  # store all model, convenient to record model</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.cal_average_batch_dict" class="doc doc-heading">
<code class="highlight language-python">cal_average_batch_dict(data_list)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>calculate average batch dict.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data_list</code></td>
          <td>
                <code>list</code>
          </td>
          <td><p>store data dict list.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p>dict. average batch dict.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def cal_average_batch_dict(self, data_list: list):
    """
        calculate average batch dict.

        Args:
            data_list (list): store data dict list.

        Returns:
            _type_: dict. average batch dict.
        """
    average_batch_dict = {}
    for key in data_list[0]:
        average_batch_dict[key] = []

    for data_dict in data_list:
        for key, value in data_dict.items():
            average_batch_dict[key].append(value)

    # average
    for key, value in average_batch_dict.items():
        average_batch_dict[key] = np.mean(value)

    return average_batch_dict</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.cal_episode_info" class="doc doc-heading">
<code class="highlight language-python">cal_episode_info()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>calculate episode reward information.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p>dict. summary reward information.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def cal_episode_info(self):
    """
        calculate episode reward information.

        Returns:
            _type_: dict. summary reward information.
        """

    # data_dict = self.get_current_update_data(('reward', 'mask'))
    # calculate current reward information

    # get done flag
    index_done = np.where(self.data_pool.get_unit_data('mask') == 0)[0] + 1
    index_done_ = index_done / self.each_thread_size
    index_done_ = index_done_.astype(np.int32)

    # config segment
    segment_index = np.arange((0, self.total_segment))
    every_segment_index = []

    # split index_done
    for segment_id in segment_index:
        segment_index_done = np.where(index_done_ == segment_id)[0]
        every_segment_index.append(index_done[segment_index_done])

    reward_dict = {}
    for key in self.rl_io_info.reward_info:
        reward_dict[key] = []

    for each_segment_index in every_segment_index:
        # get index of done
        compute_index = each_segment_index[-self.each_thread_summary_episodes:]
        start_index = compute_index[0]

        for end_index in compute_index[1:]:
            for key in self.rl_io_info.reward_info:
                reward_dict[key].append(np.sum(self.data_pool.get_unit_data(key)[start_index:end_index]))
            start_index = end_index

    # summary reward information
    reward_summary = {'std': np.std(reward_dict['total_reward']),
                      'max_reward': np.max(reward_dict['total_reward']),
                      'min_reward': np.min(reward_dict['total_reward'])}

    for key in self.rl_io_info.reward_info:
        reward_summary[key] = np.mean(reward_dict[key])

    # delete list
    del reward_dict

    return reward_summary</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_GAE" class="doc doc-heading">
<code class="highlight language-python">calculate_GAE(rewards, values, next_values, masks, gamma, lamda)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>calculate general advantage estimation.</p>
<h4 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_GAE--reference">Reference:</h4>
<p>[1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous 
control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>rewards</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>rewards.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>values</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>values.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>next_values</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>next values.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>masks</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>dones.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>discount factor.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>lamda</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>general advantage estimation factor.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>np.ndarray: general advantage estimation.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def calculate_GAE(self, rewards, values, next_values, masks, gamma, lamda):
    """
    calculate general advantage estimation.

    Reference:
    ----------
    [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous 
    control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8.

    Args:
        rewards (np.ndarray): rewards.
        values (np.ndarray): values.
        next_values (np.ndarray): next values.
        masks (np.ndarray): dones.
        gamma (float): discount factor.
        lamda (float): general advantage estimation factor.
    Returns:
        np.ndarray: general advantage estimation.
    """
    gae = np.zeros_like(rewards)
    n_steps_target = np.zeros_like(rewards)
    cumulated_advantage = 0.0
    length = len(rewards)
    index = length - 1

    # td_target = rewards + gamma * next_values * masks
    # td_delta = td_target - values
    # advantage = compute_advantage(self.hyper_parameters.gamma, self.hyper_parameters.lambada, td_delta)
    for i in range(length):
        index -= 1
        delta = rewards[index] + gamma * next_values[index] - values[index]
        cumulated_advantage = gamma * lamda * masks[index] * cumulated_advantage + delta
        gae[index] = cumulated_advantage
        n_steps_target[index] = gae[index] + values[index]

    return gae, n_steps_target</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_discounted_reward" class="doc doc-heading">
<code class="highlight language-python">calculate_discounted_reward(rewards, masks, gamma)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>calculate discounted reward.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>rewards</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>rewards.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>masks</code></td>
          <td>
                <code><span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td><p>dones. if done, mask = 0, else mask = 1.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>discount factor.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>np.ndarray: discounted reward.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def calculate_discounted_reward(self, rewards, masks, gamma):
    """
    calculate discounted reward.
    Args:
        rewards (np.ndarray): rewards.
        masks (np.ndarray): dones. if done, mask = 0, else mask = 1.
        gamma (float): discount factor.
    Returns:
        np.ndarray: discounted reward.
    """
    discounted_reward = np.zeros_like(rewards)
    cumulated_reward = 0.0
    length = len(rewards)
    index = length - 1

    for i in range(length):
        index = index - 1
        cumulated_reward = rewards[index] + gamma * cumulated_reward * masks[index]
        discounted_reward[index] = cumulated_reward

    return discounted_reward</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.check" class="doc doc-heading">
<code class="highlight language-python">check()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>check some information.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def check(self):
    """
    check some information.
    """
    if self.policy_type == 'off':
        if self.mini_buffer_size is None:
            raise ValueError('Mini buffer size must be given.')
    if self._sync_model_dict is None:
        raise ValueError('Sync model must be given.')</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.close" class="doc doc-heading">
<code class="highlight language-python">close()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>close.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def close(self):
    """
    close.
    """
    self.data_pool.close()</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.concat_dict" class="doc doc-heading">
<code class="highlight language-python">concat_dict(dict_tuple)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>concat dict.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dict_tuple</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>dict tuple.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p>dict. concat dict.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def concat_dict(self, dict_tuple: tuple):
    """
    concat dict.

    Args:
        dict_tuple (tuple): dict tuple.
    Returns:
        _type_: dict. concat dict.
    """
    concat_dict = {}
    for data_dict in dict_tuple:
        for key, value in data_dict.items():
            if key in concat_dict:
                Warning('key {} is already in concat dict'.format(key))
            else:
                concat_dict[key] = value

    return concat_dict</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.copy_weights" class="doc doc-heading">
<code class="highlight language-python">copy_weights(model1, model2)</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>copy weight from model1 to model2.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">@staticmethod
def copy_weights(model1, model2):
    """
    copy weight from model1 to model2.
    """
    new_weights = []
    target_weights = model1.weights

    for i, weight in enumerate(model2.weights):
        new_weights.append(target_weights[i].numpy())

    model2.set_weights(new_weights)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.create_none_optimizer" class="doc doc-heading">
<code class="highlight language-python">create_none_optimizer(name)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>create none optimizer for each model.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>name of this optimizer, you can call by this name.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def create_none_optimizer(self, name: str):
    """
    create none optimizer for each model.

    Args:
        name (str): name of this optimizer, you can call by this name.
    """

    attribute_name = name + '_optimizer'
    optimizer = None
    setattr(self, attribute_name, optimizer)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.create_optimizer" class="doc doc-heading">
<code class="highlight language-python">create_optimizer(name, optimizer, lr)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>create keras optimizer for each model.</p>

<details class="reference">
  <summary>Reference</summary>
  <p>https://keras.io/optimizers/</p>
</details>
  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>name</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>name of this optimizer, you can call by this name.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>optimizer</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>type of optimizer. eg. 'Adam'. For more information,</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>lr</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>learning rate.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def create_optimizer(self, name: str, optimizer: str, lr: float):
    """
    create keras optimizer for each model.

    Reference:
        https://keras.io/optimizers/

    Args:
        name (str): name of this optimizer, you can call by this name.
        if name is 'actor', then you can call self.actor_optimizer

        optimizer (str): type of optimizer. eg. 'Adam'. For more information,
        please refer to keras.optimizers.

        lr (float): learning rate.
    """

    attribute_name = name + '_optimizer'

    # in main thread, create optimizer
    if self.level == 0:
        # create optimizer
        optimizer = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr)
    else:
        # None
        optimizer = None

    # set attribute
    setattr(self, attribute_name, optimizer)

    self.optimizer_dict[name] = getattr(self, attribute_name)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_action_train" class="doc doc-heading">
<code class="highlight language-python">get_action_train(obs)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>sample action in the training process.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>obs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>observation from environment. eg. {'obs':data}.
        The data must be tensor. And its shape is (batch, feature).</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p><em>description</em></p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def get_action_train(self, obs: dict):
    """

    sample action in the training process.

    Args:
        obs (dict): observation from environment. eg. {'obs':data}.
                    The data must be tensor. And its shape is (batch, feature).

    Returns:
        _type_: _description_
    """

    input_data = []

    # get actor input
    for key in self.actor.input_name:
        input_data.append(tf.cast(obs[key], dtype=tf.float32))

    actor_out = self.actor(*input_data)  # out is a tuple

    policy_out = dict(zip(self.actor.output_info, actor_out))

    for name, value in self._explore_dict.items():
        policy_out[name] = value

    action, prob = self.explore_policy(policy_out)

    policy_out['action'] = action
    policy_out['prob'] = prob

    # create return dict according to rl_io_info.actor_out_name
    return_dict = dict()
    for name in self.rl_io_info.actor_out_name:
        return_dict[name] = policy_out[name]

    return return_dict</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_batch_data" class="doc doc-heading">
<code class="highlight language-python">get_batch_data(data_dict, start_index, end_index)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Get batch data from data dict.</p>
<p>The data type stored in data_dict must be tuple or tensor or array.</p>

<details class="example">
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)}
start_index = 0
end_index = 5
self.get_batch_data(data_dict, start_index, end_index)
{'obs': (array([1, 2, 3, 4, 5]),)}</p>
</blockquote>
</blockquote>
</blockquote>
</details>
  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>data dict.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>start_index</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>start index.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>end_index</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>end index.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>batch data. dict.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def get_batch_data(self, data_dict: dict, start_index, end_index):
    """
    Get batch data from data dict.

    The data type stored in data_dict must be tuple or tensor or array.

    Example:
        &gt;&gt;&gt; data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)}
        &gt;&gt;&gt; start_index = 0
        &gt;&gt;&gt; end_index = 5
        &gt;&gt;&gt; self.get_batch_data(data_dict, start_index, end_index)
        {'obs': (array([1, 2, 3, 4, 5]),)}

    Args:
        data_dict (dict): data dict.
        start_index (int): start index.
        end_index (int): end index.
    Returns:
        batch data. dict.
    """
    batch_data = dict()
    for key, values in data_dict.items():
        if isinstance(values, tuple) or isinstance(values, list):
            buffer = []
            for value in values:
                buffer.append(value[start_index:end_index])
            batch_data[key] = tuple(buffer)
        else:
            batch_data[key] = values[start_index:end_index]

    return batch_data</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_corresponding_data" class="doc doc-heading">
<code class="highlight language-python">get_corresponding_data(data_dict, names, prefix='', tf_tensor=True)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Get corresponding data from data dict.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>data dict.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>names</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>name of data.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prefix</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>prefix of data name.</p></td>
          <td>
                <code>&#39;&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>tf_tensor</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>if return tf tensor.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>corresponding data. list or tuple.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def get_corresponding_data(self, data_dict: dict, names: tuple, prefix: str = '', tf_tensor: bool = True):
    """

    Get corresponding data from data dict.

    Args:
        data_dict (dict): data dict.
        names (tuple): name of data.
        prefix (str): prefix of data name.
        tf_tensor (bool): if return tf tensor.
    Returns:
        corresponding data. list or tuple.
    """

    data = []

    for name in names:
        name = prefix + name
        buffer = data_dict[name]
        if tf_tensor:
            buffer = tf.cast(buffer, dtype=tf.float32)
        data.append(buffer)

    return data</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_update_data" class="doc doc-heading">
<code class="highlight language-python">get_current_update_data(names)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Get current update data.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>names</code></td>
          <td>
                <code>tuple</code>
          </td>
          <td><p>data name.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p>dict. data dict.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def get_current_update_data(self, names: tuple):
    """
    Get current update data.

    Args:
        names (tuple): data name.
    Returns:
        _type_: dict. data dict.
    """
    # running after optimize
    # compute sampling interval
    start_index = (self.optimize_epoch - 1) * self.each_thread_update_interval
    end_index = self.optimize_epoch * self.each_thread_update_interval

    index_bias = np.arange(0, self.total_segment) * self.each_thread_size

    return_dict = {}

    for name in names:
        return_dict[name] = []

    for bias in index_bias:
        start_index = start_index + bias
        end_index = end_index + bias

        data_dict = self.data_pool.get_data_by_indices(np.arange(start_index, end_index).tolist(), names)

        for key, ls in return_dict.items():
            ls.append(data_dict[key])

    # concat data
    for key, ls in return_dict.items():
        return_dict[key] = np.concatenate(ls, axis=0)

    return return_dict</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.init" class="doc doc-heading">
<code class="highlight language-python">init()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>initial algorithm.</p>
<p>This function will be called by starter.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def init(self):
    """initial algorithm.

    This function will be called by starter.
    """
    # multi thread communication

    reward_info_dict = {}

    for name in self.rl_io_info.reward_info:
        reward_info_dict['summary_' + name] = (
            self.total_segment * self.each_thread_summary_episodes, 1)

    # add summary reward information to data pool
    for name, shape in reward_info_dict.items():
        # this must be first level name
        buffer = DataUnit(name=self.name + '_' + name, shape=shape, dtype=np.float32, level=self.level,
                          computer_type=self._computer_type)
        self.rl_io_info.add_info(name=name, shape=shape, dtype=np.float32)
        self.data_pool.add_unit(name=name, data_unit=buffer)

    # TODO:子线程需要等待时间 check
    # multi thread initial
    if self.total_threads &gt; 1:  # multi thread
        # print(self.rl_io_info.data_info)
        self.data_pool.multi_init(self.rl_io_info.data_info, type='buffer')
    else:  # single thread
        self.data_pool.create_buffer_from_dic(self.rl_io_info.data_info)

    # just do in m main thread
    if self.level == 0:
        # initial recoder
        self.recoder = Recoder(log_folder=self.log_path)
    else:
        self.recoder = None

    # check some information
    # actor model must be given
    if self.actor is None:
        raise ValueError('Actor model must be given.')</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.initialize_model_weights" class="doc doc-heading">
<code class="highlight language-python">initialize_model_weights(model)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>initial model.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def initialize_model_weights(self, model):
    """
    initial model.
    """

    input_data_name = model.input_name

    # create tensor according to input data name
    input_data = []

    for name in input_data_name:
        shape, _ = self.rl_io_info.get_data_info(name)
        data = tf.zeros(shape=shape, dtype=tf.float32)
        input_data.append(data)

    model(*input_data)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.random_sample" class="doc doc-heading">
<code class="highlight language-python">random_sample(batch_size)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>random sample data from buffer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>batch size.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>_type_</code></td>          <td>
          </td>
          <td><p>dict. data dict.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def random_sample(self, batch_size: int):
    """
        random sample data from buffer.

        Args:
            batch_size (int): batch size.

        Returns:
            _type_: dict. data dict.
        """
    # if using multi thread, then sample data from each segment
    # sample data from each segment

    # compute current segment size
    running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment
    buffer_size = min(self.max_buffer_size, running_step)

    batch_size = min(batch_size, buffer_size)

    sample_index = np.random.choice(range(buffer_size), batch_size, replace=False)

    # index_bias = (sample_index * 1.0 / self.each_thread_size) * self.each_thread_size
    index_bias = sample_index / self.each_thread_size
    index_bias = index_bias.astype(np.int32)
    index_bias = index_bias * self.each_thread_size

    sample_index = sample_index + index_bias
    sample_index = sample_index.astype(np.int32)

    # get data

    data_dict = self.data_pool.get_data_by_indices(sample_index, self.rl_io_info.store_data_name)

    return data_dict</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.soft_update_weights" class="doc doc-heading">
<code class="highlight language-python">soft_update_weights(model1, model2, tau)</code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>soft update weight from model1 to model2.</p>
      <p>model1: source model
model2: target model</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">@staticmethod
def soft_update_weights(model1, model2, tau):
    """
    soft update weight from model1 to model2.


    args:
    model1: source model
    model2: target model
    """
    new_weights = []
    source_weights = model1.weights

    for i, weight in enumerate(model2.weights):
        new_weights.append((1 - tau) * weight.numpy() + tau * source_weights[i].numpy())

    model2.set_weights(new_weights)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.store_data" class="doc doc-heading">
<code class="highlight language-python">store_data(obs, action, reward, next_obs, mask)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>store data to buffer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>obs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>observation. eg. {'obs':np.array([1,2,3])}</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>action</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>action. eg. {'action':np.array([1,2,3])}</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>reward</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>reward. eg. {'reward':np.array([1,2,3])}</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>next_obs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td><p>next observation. eg. {'next_obs':np.array([1,2,3])}</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>mask</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>done. eg. 1 or 0</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def store_data(self, obs: dict, action: dict, reward: dict, next_obs: dict, mask: int):
    """
    store data to buffer.

    Args:
        obs (dict): observation. eg. {'obs':np.array([1,2,3])}
        action (dict): action. eg. {'action':np.array([1,2,3])}
        reward (dict): reward. eg. {'reward':np.array([1,2,3])}
        next_obs (dict): next observation. eg. {'next_obs':np.array([1,2,3])}
        mask (int): done. eg. 1 or 0
    """
    # store data to buffer
    # support multi thread

    idx = (self.worker.step_count - 1) % self.each_thread_size
    index = self.each_thread_start_index + idx  # index in each thread

    # store obs to buffer
    self.data_pool.store(obs, index)

    # store next_obs to buffer
    self.data_pool.store(next_obs, index, prefix='next_')

    # store action to buffer
    self.data_pool.store(action, index)

    # store reward to buffer
    self.data_pool.store(reward, index)

    # store mask to buffer
    self.data_pool.data_pool['mask'].store(mask, index)</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.summary_reward_info" class="doc doc-heading">
<code class="highlight language-python">summary_reward_info()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>summary reward information.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def summary_reward_info(self):
    """
    summary reward information.
    """
    # calculate reward information

    summary_reward_info = {}

    for name in self.rl_io_info.reward_info:
        summary_reward_info[name] = np.mean(self.data_pool.get_unit_data('summary_' + name))

    summary_reward_info['std'] = np.std(self.data_pool.get_unit_data('summary_total_reward'))
    summary_reward_info['max_reward'] = np.max(self.data_pool.get_unit_data('summary_total_reward'))
    summary_reward_info['min_reward'] = np.min(self.data_pool.get_unit_data('summary_total_reward'))

    return summary_reward_info</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.sync" class="doc doc-heading">
<code class="highlight language-python">sync()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>sync.
Used in multi thread.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def sync(self):
    """
    sync.
    Used in multi thread.

    """
    if self.level == 0:
        for key, model in self._sync_model_dict.items():
            model.save_weights(self.cache_path + '/' + key + '.h5')
    else:
        for key, model in self._sync_model_dict.items():
            model.load_weights(self.cache_path + '/' + key + '.h5')

    if self.log_std is not None:
        self.sync_log_std()</code></pre>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h2 id="AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.sync_log_std" class="doc doc-heading">
<code class="highlight language-python">sync_log_std()</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>sync log std.</p>

      <details class="quote">
        <summary>Source code in <code>AquaML/rlalgo/BaseRLAlgo.py</code></summary>
        <pre class="highlight"><code class="language-python">def sync_log_std(self):
    """
    sync log std.
    """

    if self.level == 0:
        self.log_std.set_value(self.tf_log_std.numpy())  # write log std to shared memory
    else:
        self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True)  # read log std from shared memory</code></pre>
      </details>
  </div>

</div>



  </div>

  </div>

</div><p>options:
      show_source: false</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../DataUnit/" class="btn btn-neutral float-left" title="data:DataUnit"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../ExplorePolicy/" class="btn btn-neutral float-right" title="rlalgo:ExplorePolicy">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../DataUnit/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../ExplorePolicy/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
