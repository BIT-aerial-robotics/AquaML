#!/usr/bin/env python3
import torch
import torch.nn as nn
from typing import Dict

from AquaML.learning.model import Model
from AquaML.learning.model.model_cfg import ModelCfg
from AquaML.learning.reinforcement.off_policy.ddpg import DDPG, DDPGCfg
from AquaML.environment.gymnasium_envs import GymnasiumWrapper
from AquaML.learning.trainers.sequential import SequentialTrainer
from AquaML.learning.trainers.base import TrainerConfig

# è‡ªåŠ¨åˆå§‹åŒ–é»˜è®¤æ–‡ä»¶ç³»ç»Ÿ
from AquaML import coordinator


class PendulumActor(Model):
    """Pendulumç¯å¢ƒDDPGç­–ç•¥æ¨¡å‹"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()  # PendulumåŠ¨ä½œç©ºé—´æ˜¯[-2, 2]ï¼Œtanhè¾“å‡º[-1, 1]
        )
        
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
        
        # å‰å‘ä¼ æ’­
        actions = self.net(states)
        
        # ç¼©æ”¾tanhè¾“å‡ºåˆ°åŠ¨ä½œç©ºé—´[-2, 2]
        actions = actions * 2.0
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if actions.dim() == 1:
            actions = actions.unsqueeze(0)
        
        return {"actions": actions}
    
    def act(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        return self.compute(data_dict)


class PendulumCritic(Model):
    """Pendulumç¯å¢ƒDDPGä»·å€¼æ¨¡å‹"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„ - è¾“å…¥: çŠ¶æ€(3) + åŠ¨ä½œ(1) = 4
        self.net = nn.Sequential(
            nn.Linear(4, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # è·å–åŠ¨ä½œæ•°æ®
        if "taken_actions" in data_dict:
            actions = data_dict["taken_actions"]
        elif "actions" in data_dict:
            actions = data_dict["actions"]
        else:
            raise ValueError("No action found in data_dict for critic")
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
            
        if actions.dim() == 1:
            actions = actions.unsqueeze(0)
        elif actions.dim() > 2:
            actions = actions.view(-1, actions.size(-1))
        
        # è¿æ¥çŠ¶æ€å’ŒåŠ¨ä½œ
        state_action = torch.cat([states, actions], dim=-1)
        
        # å‰å‘ä¼ æ’­
        q_values = self.net(state_action)
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if q_values.dim() == 1:
            q_values = q_values.unsqueeze(-1)
        
        return {"values": q_values}
    
    def act(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        return self.compute(data_dict)


class OrnsteinUhlenbeckNoise:
    """Ornstein-Uhlenbeckå™ªå£°è¿‡ç¨‹ç”¨äºæ¢ç´¢"""
    
    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2, dt=1e-2):
        self.size = size
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.dt = dt
        self.reset()
    
    def reset(self):
        self.x = torch.ones(self.size) * self.mu
    
    def sample(self, shape):
        dx = self.theta * (self.mu - self.x) * self.dt + \
             self.sigma * torch.sqrt(torch.tensor(self.dt)) * torch.randn_like(self.x)
        self.x = self.x + dx
        return self.x.clone().view(shape)


def main():
    # 1. ç®€å•æ³¨å†Œè¿è¡Œå™¨ï¼ˆè‡ªåŠ¨ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆåç§°ï¼Œè‡ªåŠ¨åˆ›å»ºworkspaceç»“æ„ï¼‰
    runner_name = coordinator.registerRunner()
    print(f"âœ“ è¿è¡Œå™¨å·²æ³¨å†Œ: {runner_name}")
    
    # è·å–æ–‡ä»¶ç³»ç»Ÿå®ä¾‹ï¼ˆå·²è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
    fs = coordinator.getFileSystem()
    
    # 2. åˆ›å»ºç¯å¢ƒ
    env = GymnasiumWrapper("Pendulum-v1")
    
    # 3. åˆ›å»ºæ¨¡å‹é…ç½®
    model_cfg = ModelCfg(
        device="cpu",
        inputs_name=["state"],
        concat_dict=False
    )
    
    # 4. åˆ›å»ºDDPGæ¨¡å‹
    policy = PendulumActor(model_cfg)
    target_policy = PendulumActor(model_cfg)
    critic = PendulumCritic(model_cfg)
    target_critic = PendulumCritic(model_cfg)
    
    # 5. åˆ›å»ºæ¢ç´¢å™ªå£°
    exploration_noise = OrnsteinUhlenbeckNoise(size=(1,), sigma=0.3)
    
    # 6. é…ç½®DDPGå‚æ•° - æ–°æ•°æ®æµæ¶æ„çš„å…³é”®å‚æ•°
    ddpg_cfg = DDPGCfg()
    ddpg_cfg.device = "cpu"
    ddpg_cfg.memory_size = 10000
    ddpg_cfg.batch_size = 64  # ğŸ“Š å…³é”®å‚æ•°ï¼šæ‰¹é‡å¤§å°
    ddpg_cfg.learning_starts = 1000
    ddpg_cfg.gradient_steps = 1
    ddpg_cfg.discount_factor = 0.99
    ddpg_cfg.polyak = 0.005
    ddpg_cfg.actor_learning_rate = 1e-3
    ddpg_cfg.critic_learning_rate = 1e-3
    ddpg_cfg.exploration_noise = exploration_noise
    ddpg_cfg.exploration_initial_scale = 1.0
    ddpg_cfg.exploration_final_scale = 0.1
    ddpg_cfg.exploration_timesteps = 5000
    ddpg_cfg.random_timesteps = 1000
    ddpg_cfg.grad_norm_clip = 0.5
    ddpg_cfg.mixed_precision = False
    
    # 7. åˆ›å»ºDDPGæ™ºèƒ½ä½“
    models = {
        "policy": policy,
        "target_policy": target_policy,
        "critic": critic,
        "target_critic": target_critic
    }
    agent = DDPG(models, ddpg_cfg)
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨é…ç½® - ç®€åŒ–é…ç½®ï¼Œè‡ªåŠ¨ä»agentè¯»å–å‚æ•°
    trainer_cfg = TrainerConfig(
        timesteps=10000,
        headless=True,
        disable_progressbar=False
    )
    
    # 9. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ - ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„
    trainer = SequentialTrainer(env, agent, trainer_cfg)
    
    print(f"ğŸŒŠ å¼€å§‹ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„è®­ç»ƒDDPG:")
    print(f"  ğŸ“Š å†…å­˜å¤§å°: {ddpg_cfg.memory_size}")
    print(f"  ğŸ“¦ æ‰¹é‡å¤§å°: {ddpg_cfg.batch_size}")
    print(f"  ğŸ”„ æ€»æ—¶é—´æ­¥: {trainer_cfg.timesteps}")
    print(f"  ğŸ“¥ æ•°æ®ç¼“å­˜æ ¼å¼: (num_env, steps, dims)")
    print(f"  ğŸ¯ æ¢ç´¢å™ªå£°: OUå™ªå£° (sigma={exploration_noise.sigma})")
    
    trainer.train()
    
    # 10. æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    status = trainer.get_enhanced_status()
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"  æ”¶é›†æ­¥æ•°: {status['data_flow_architecture']['collected_steps']}")
    print(f"  è®­ç»ƒè½®æ¬¡: {status['data_flow_architecture']['training_episodes']}")
    print(f"  æ•°æ®æ•ˆç‡: {status['data_flow_architecture']['collected_steps']/trainer_cfg.timesteps:.2f}")
    
    # 11. ä¿å­˜æ¨¡å‹åˆ°å·¥ä½œç›®å½•
    agent.save(fs.getModelPath(runner_name, "trained_ddpg_model.pt"))
    print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")
    
    # 12. éªŒè¯æ•°æ®ç¼“å­˜åŠŸèƒ½
    print(f"\nğŸ” éªŒè¯æ•°æ®ç¼“å­˜:")
    available_buffers = trainer.list_available_buffers()
    print(f"  å¯ç”¨ç¼“å­˜: {available_buffers}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç¼“å­˜æ•°æ®ä¿¡æ¯
    for buffer_name in available_buffers[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        data = trainer.get_collected_buffer_data(buffer_name)
        if data is not None:
            print(f"  {buffer_name}: å½¢çŠ¶ {data.shape}")
    
    print(f"\nğŸŒŠ æ–°æ•°æ®æµæ¶æ„DDPGæ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()