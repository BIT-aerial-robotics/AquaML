#!/usr/bin/env python3
import torch
import torch.nn as nn
from typing import Dict

from AquaML.learning.model import Model
from AquaML.learning.model.model_cfg import ModelCfg
from AquaML.learning.reinforcement.on_policy.ppo import PPO, PPOCfg
from AquaML.learning.model.gaussian import GaussianModel
from AquaML.environment.gymnasium_envs import GymnasiumWrapper
from AquaML.learning.trainers.sequential import SequentialTrainer
from AquaML.learning.trainers.base import TrainerConfig

# è‡ªåŠ¨åˆå§‹åŒ–é»˜è®¤æ–‡ä»¶ç³»ç»Ÿ
from AquaML import coordinator


class PendulumPolicy(GaussianModel):
    """Pendulumç¯å¢ƒç­–ç•¥æ¨¡å‹"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # å­¦ä¹ logæ ‡å‡†å·®å‚æ•°
        self.log_std_parameter = nn.Parameter(torch.zeros(1))
        
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
        
        # å‰å‘ä¼ æ’­
        mean = self.net(states)
        log_std = self.log_std_parameter.expand_as(mean)
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if mean.dim() == 1:
            mean = mean.unsqueeze(0)
        if log_std.dim() == 1:
            log_std = log_std.unsqueeze(0)
        
        return {"mean_actions": mean, "log_std": log_std}


class PendulumValue(Model):
    """Pendulumç¯å¢ƒä»·å€¼æ¨¡å‹"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
            
        # å‰å‘ä¼ æ’­
        values = self.net(states)
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if values.dim() == 1:
            values = values.unsqueeze(-1)
        
        return {"values": values}
        
    
    def act(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        return self.compute(data_dict)


def main():
    # 1. ç®€å•æ³¨å†Œè¿è¡Œå™¨ï¼ˆè‡ªåŠ¨ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆåç§°ï¼Œè‡ªåŠ¨åˆ›å»ºworkspaceç»“æ„ï¼‰
    runner_name = coordinator.registerRunner()
    print(f"âœ“ è¿è¡Œå™¨å·²æ³¨å†Œ: {runner_name}")
    
    # è·å–æ–‡ä»¶ç³»ç»Ÿå®ä¾‹ï¼ˆå·²è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
    fs = coordinator.getFileSystem()
    
    # 2. åˆ›å»ºç¯å¢ƒ
    env = GymnasiumWrapper("Pendulum-v1")
    
    # 3. åˆ›å»ºæ¨¡å‹é…ç½®
    model_cfg = ModelCfg(
        device="cpu",
        inputs_name=["state"],
        concat_dict=False
    )
    
    # 4. åˆ›å»ºæ¨¡å‹
    policy = PendulumPolicy(model_cfg)
    value = PendulumValue(model_cfg)
    
    # 5. é…ç½®PPOå‚æ•° - æ–°æ•°æ®æµæ¶æ„çš„å…³é”®å‚æ•°
    ppo_cfg = PPOCfg()
    ppo_cfg.device = "cpu"
    ppo_cfg.memory_size = 200
    ppo_cfg.rollouts = 32  # ğŸ“Š å…³é”®å‚æ•°ï¼šæ¯32æ­¥è§¦å‘ä¸€æ¬¡è®­ç»ƒ
    ppo_cfg.learning_epochs = 4
    ppo_cfg.mini_batches = 2
    ppo_cfg.learning_rate = 3e-4
    ppo_cfg.mixed_precision = False
    
    # 6. åˆ›å»ºPPOæ™ºèƒ½ä½“
    models = {"policy": policy, "value": value}
    agent = PPO(models, ppo_cfg)
    
    # 7. åˆ›å»ºè®­ç»ƒå™¨é…ç½® - ç®€åŒ–é…ç½®ï¼Œè‡ªåŠ¨ä»agentè¯»å–å‚æ•°
    trainer_cfg = TrainerConfig(
        timesteps=1000,
        headless=True,
        disable_progressbar=False
    )
    # collect_intervalè‡ªåŠ¨ä»PPOçš„rolloutså‚æ•°è¯»å–ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ - ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„
    trainer = SequentialTrainer(env, agent, trainer_cfg)
    
    print(f"ğŸŒŠ å¼€å§‹ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„è®­ç»ƒ:")
    print(f"  ğŸ“Š Rollouts: {ppo_cfg.rollouts} (æ¯{ppo_cfg.rollouts}æ­¥è®­ç»ƒä¸€æ¬¡)")
    print(f"  ğŸ”„ æ€»æ—¶é—´æ­¥: {trainer_cfg.timesteps}")
    print(f"  ğŸ“¥ æ•°æ®ç¼“å­˜æ ¼å¼: (num_env, steps, dims)")
    
    trainer.train()
    
    # 9. æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    status = trainer.get_enhanced_status()
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"  æ”¶é›†æ­¥æ•°: {status['data_flow_architecture']['collected_steps']}")
    print(f"  è®­ç»ƒè½®æ¬¡: {status['data_flow_architecture']['training_episodes']}")
    print(f"  æ•°æ®æ•ˆç‡: {status['data_flow_architecture']['collected_steps']/trainer_cfg.timesteps:.2f}")
    
    # 10. ä¿å­˜æ¨¡å‹åˆ°å·¥ä½œç›®å½•
    agent.save(fs.getModelPath(runner_name, "trained_model.pt"))
    print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")
    
    # 11. éªŒè¯æ•°æ®ç¼“å­˜åŠŸèƒ½
    print(f"\nğŸ” éªŒè¯æ•°æ®ç¼“å­˜:")
    available_buffers = trainer.list_available_buffers()
    print(f"  å¯ç”¨ç¼“å­˜: {available_buffers}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç¼“å­˜æ•°æ®ä¿¡æ¯
    for buffer_name in available_buffers[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        data = trainer.get_collected_buffer_data(buffer_name)
        if data is not None:
            print(f"  {buffer_name}: å½¢çŠ¶ {data.shape}")
    
    print(f"\nğŸŒŠ æ–°æ•°æ®æµæ¶æ„æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()