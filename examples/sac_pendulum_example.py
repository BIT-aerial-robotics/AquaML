#!/usr/bin/env python3
import torch
import torch.nn as nn
from typing import Dict

from AquaML.learning.model import Model
from AquaML.learning.model.model_cfg import ModelCfg
from AquaML.learning.reinforcement.off_policy.sac import SAC, SACCfg
from AquaML.learning.model.gaussian import GaussianModel
from AquaML.environment.gymnasium_envs import GymnasiumWrapper
from AquaML.learning.trainers.sequential import SequentialTrainer
from AquaML.learning.trainers.base import TrainerConfig

# è‡ªåŠ¨åˆå§‹åŒ–é»˜è®¤æ–‡ä»¶ç³»ç»Ÿ
from AquaML import coordinator


class PendulumSACPolicy(GaussianModel):
    """Pendulumç¯å¢ƒSACç­–ç•¥æ¨¡å‹"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„ - Pendulumè§‚å¯Ÿç©ºé—´: (cos(theta), sin(theta), angular_velocity) = 3
        self.net = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # SACéœ€è¦åˆ†ç¦»çš„å‡å€¼å’Œlogæ ‡å‡†å·®è¾“å‡º
        self.mean_layer = nn.Linear(64, 1)  # PendulumåŠ¨ä½œç©ºé—´: 1
        self.log_std_layer = nn.Linear(64, 1)
    
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
        
        # å‰å‘ä¼ æ’­
        features = self.net(states)
        mean = self.mean_layer(features)
        log_std = self.log_std_layer(features)
        
        # æ•°å€¼ç¨³å®šæ€§çº¦æŸ
        log_std = torch.clamp(log_std, min=-20, max=2)
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if mean.dim() == 1:
            mean = mean.unsqueeze(0)
        if log_std.dim() == 1:
            log_std = log_std.unsqueeze(0)
        
        return {"mean_actions": mean, "log_std": log_std}
    
    def act(self, data_dict: Dict[str, torch.Tensor], deterministic: bool = False) -> Dict[str, torch.Tensor]:
        """ç”ŸæˆåŠ¨ä½œçš„SACå®ç°"""
        outputs = self.compute(data_dict)
        mean = outputs["mean_actions"]
        log_std = outputs["log_std"]
        std = torch.exp(log_std)
        
        if deterministic:
            # è¯„ä¼°æ—¶ä½¿ç”¨å‡å€¼åŠ¨ä½œ
            actions = torch.tanh(mean)
            log_prob = torch.zeros_like(actions)
        else:
            # ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·å¹¶åº”ç”¨tanh
            normal = torch.distributions.Normal(mean, std)
            x_t = normal.rsample()  # é‡å‚æ•°åŒ–æŠ€å·§
            actions = torch.tanh(x_t)
            
            # è®¡ç®—å¸¦tanhä¿®æ­£çš„å¯¹æ•°æ¦‚ç‡
            log_prob = normal.log_prob(x_t)
            log_prob -= torch.log(1 - actions.pow(2) + 1e-6)
            log_prob = log_prob.sum(dim=-1, keepdim=True)
        
        # ç¼©æ”¾åŠ¨ä½œåˆ°Pendulumçš„åŠ¨ä½œèŒƒå›´[-2, 2]
        actions = actions * 2.0
        
        return {
            "actions": actions,
            "log_prob": log_prob,
            "mean_actions": mean,
            "log_std": log_std
        }
    
    def random_act(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ç”ŸæˆéšæœºåŠ¨ä½œç”¨äºæ¢ç´¢"""
        batch_size = list(data_dict.values())[0].shape[0]
        device = list(data_dict.values())[0].device
        
        # Pendulumç¯å¢ƒçš„éšæœºåŠ¨ä½œèŒƒå›´[-2, 2]
        actions = torch.rand(batch_size, 1, device=device) * 4.0 - 2.0
        log_prob = torch.zeros_like(actions)
        
        return {"actions": actions, "log_prob": log_prob}


class PendulumSACCritic(Model):
    """Pendulumç¯å¢ƒSACä»·å€¼æ¨¡å‹(Qå‡½æ•°)"""
    
    def __init__(self, model_cfg: ModelCfg):
        super().__init__(model_cfg)
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„ - è¾“å…¥: çŠ¶æ€(3) + åŠ¨ä½œ(1) = 4
        self.net = nn.Sequential(
            nn.Linear(4, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def compute(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # è·å–çŠ¶æ€æ•°æ®
        if "state" in data_dict:
            states = data_dict["state"]
        else:
            states = list(data_dict.values())[0]
        
        # è·å–åŠ¨ä½œæ•°æ®
        if "actions" in data_dict:
            actions = data_dict["actions"]
        elif "action" in data_dict:
            actions = data_dict["action"]
        else:
            raise ValueError("No action found in data_dict for critic")
        
        # å¤„ç†ç»´åº¦
        if states.dim() == 1:
            states = states.unsqueeze(0)
        elif states.dim() > 2:
            states = states.view(-1, states.size(-1))
            
        if actions.dim() == 1:
            actions = actions.unsqueeze(0)
        elif actions.dim() > 2:
            actions = actions.view(-1, actions.size(-1))
        
        # è¿æ¥çŠ¶æ€å’ŒåŠ¨ä½œ
        state_action = torch.cat([states, actions], dim=-1)
        
        # å‰å‘ä¼ æ’­
        q_value = self.net(state_action)
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        if q_value.dim() == 1:
            q_value = q_value.unsqueeze(-1)
            
        return {"values": q_value}
    
    def act(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        return self.compute(data_dict)


def main():
    # 1. ç®€å•æ³¨å†Œè¿è¡Œå™¨ï¼ˆè‡ªåŠ¨ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆåç§°ï¼Œè‡ªåŠ¨åˆ›å»ºworkspaceç»“æ„ï¼‰
    runner_name = coordinator.registerRunner()
    print(f"âœ“ è¿è¡Œå™¨å·²æ³¨å†Œ: {runner_name}")
    
    # è·å–æ–‡ä»¶ç³»ç»Ÿå®ä¾‹ï¼ˆå·²è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
    fs = coordinator.getFileSystem()
    
    # 2. åˆ›å»ºç¯å¢ƒ
    env = GymnasiumWrapper("Pendulum-v1")
    
    # 3. åˆ›å»ºæ¨¡å‹é…ç½®
    model_cfg = ModelCfg(
        device="cpu",
        inputs_name=["state"],
        concat_dict=False
    )
    
    # 4. åˆ›å»ºSACæ¨¡å‹
    policy = PendulumSACPolicy(model_cfg)
    critic_1 = PendulumSACCritic(model_cfg)
    critic_2 = PendulumSACCritic(model_cfg)
    target_critic_1 = PendulumSACCritic(model_cfg)
    target_critic_2 = PendulumSACCritic(model_cfg)
    
    # 5. é…ç½®SACå‚æ•° - æ–°æ•°æ®æµæ¶æ„çš„å…³é”®å‚æ•°
    sac_cfg = SACCfg()
    sac_cfg.device = "cpu"
    sac_cfg.memory_size = 10000
    sac_cfg.batch_size = 256  # ğŸ“Š å…³é”®å‚æ•°ï¼šæ‰¹é‡å¤§å°
    sac_cfg.gradient_steps = 1
    sac_cfg.learning_starts = 1000
    sac_cfg.random_timesteps = 1000
    
    # å­¦ä¹ ç‡è®¾ç½®
    sac_cfg.actor_learning_rate = 3e-4
    sac_cfg.critic_learning_rate = 3e-4
    sac_cfg.entropy_learning_rate = 3e-4
    
    # SACè¶…å‚æ•°
    sac_cfg.discount_factor = 0.99
    sac_cfg.polyak = 0.005
    sac_cfg.initial_entropy_value = 0.2
    sac_cfg.learn_entropy = True
    sac_cfg.target_entropy = -1.0  # -action_dim for Pendulum
    sac_cfg.mixed_precision = False
    
    # 6. åˆ›å»ºSACæ™ºèƒ½ä½“
    models = {
        "policy": policy,
        "critic_1": critic_1,
        "critic_2": critic_2,
        "target_critic_1": target_critic_1,
        "target_critic_2": target_critic_2
    }
    
    action_space = {"shape": (1,)}  # PendulumåŠ¨ä½œç©ºé—´ç»´åº¦
    agent = SAC(models, sac_cfg, action_space=action_space)
    
    # 7. åˆ›å»ºè®­ç»ƒå™¨é…ç½® - ç®€åŒ–é…ç½®ï¼Œè‡ªåŠ¨ä»agentè¯»å–å‚æ•°
    trainer_cfg = TrainerConfig(
        timesteps=50000,
        headless=True,
        disable_progressbar=False
    )
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ - ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„
    trainer = SequentialTrainer(env, agent, trainer_cfg)
    
    print(f"ğŸŒŠ å¼€å§‹ä½¿ç”¨æ–°æ•°æ®æµæ¶æ„è®­ç»ƒSAC:")
    print(f"  ğŸ“Š å†…å­˜å¤§å°: {sac_cfg.memory_size}")
    print(f"  ğŸ“¦ æ‰¹é‡å¤§å°: {sac_cfg.batch_size}")
    print(f"  ğŸ”„ æ€»æ—¶é—´æ­¥: {trainer_cfg.timesteps}")
    print(f"  ğŸ“¥ æ•°æ®ç¼“å­˜æ ¼å¼: (num_env, steps, dims)")
    
    trainer.train()
    
    # 9. æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    status = trainer.get_enhanced_status()
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"  æ”¶é›†æ­¥æ•°: {status['data_flow_architecture']['collected_steps']}")
    print(f"  è®­ç»ƒè½®æ¬¡: {status['data_flow_architecture']['training_episodes']}")
    print(f"  æ•°æ®æ•ˆç‡: {status['data_flow_architecture']['collected_steps']/trainer_cfg.timesteps:.2f}")
    
    # 10. ä¿å­˜æ¨¡å‹åˆ°å·¥ä½œç›®å½•
    agent.save(fs.getModelPath(runner_name, "trained_sac_model.pt"))
    print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")
    
    # 11. éªŒè¯æ•°æ®ç¼“å­˜åŠŸèƒ½
    print(f"\nğŸ” éªŒè¯æ•°æ®ç¼“å­˜:")
    available_buffers = trainer.list_available_buffers()
    print(f"  å¯ç”¨ç¼“å­˜: {available_buffers}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç¼“å­˜æ•°æ®ä¿¡æ¯
    for buffer_name in available_buffers[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        data = trainer.get_collected_buffer_data(buffer_name)
        if data is not None:
            print(f"  {buffer_name}: å½¢çŠ¶ {data.shape}")
    
    print(f"\nğŸŒŠ æ–°æ•°æ®æµæ¶æ„SACæ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()