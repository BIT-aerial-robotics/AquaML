{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AquaML Features Support reinforcement learning, generative learning algorithm. Support reinforcement learning training with recurrent neural networks. Support RNN for reinforcement learning and provide two basic forms. Support multi-thread sampling and parameters tuning. Support high performance computer(HPC) Data communication has almost zero lat ency when running on a single machine. Install Tutorials Train Pendulum-v0 with soft-actor-critic(SAC) This tutorial is to show how to use AquaML to control pendulum-v0(https://gym.openai.com/envs/Pendulum-v0/). The environment is a continuous action space environment. The action is a 1-dim vector. The observation is a 3-dim vector. All the codes are available in Tutorial/tutorial1.py. Create neural network model for reinforcement learning AquaML just supports 'expert' TF model style, you can learn more in https://tensorflow.google.cn/overview. But in AquaML, the reinforcement learning model must inherit from RLBaseModel . 1. Actor Before creating please do the following things: import tensorflow as tf Then we can create the model. class Actor_net(tf.keras.Model): def __init__(self): super(Actor_net, self).__init__() self.dense1 = tf.keras.layers.Dense(64, activation='relu') self.dense2 = tf.keras.layers.Dense(64, activation='tanh') Point out learning rate. In AuqaML , each model can have its own learning rate self.learning_rate = 2e-4 Our framework can fusion muti type data, please specify the input data name self.input_name = ('obs',) Actor net is special than others, its out may be different. Thus you should specify . self.output_info = {'action': (1,)} Then specify the optimizer of your neural network. _name contains name of data, _info also contains shape. self.optimizer = 'Adam' Then declaim call function: def call(self, obs): x = self.dense1(obs) x = self.dense2(x) x = self.dense3(x) # the output of actor network must be a tuple # and the order of output must be the same as the order of output name return (x,) For actor, the return must be a tuple. reset function is also needed in AquaML . def reset(self): pass Our frame work support adaptive log_std . So the output of the Actor_net also contains log_std . For POMDP version: class Actor_net(tf.keras.Model): def __init__(self): super(Actor_net, self).__init__() self.lstm = tf.keras.layers.LSTM(32, input_shape=(2,), return_sequences=False, return_state=True) self.dense1 = tf.keras.layers.Dense(64, activation='relu') self.dense2 = tf.keras.layers.Dense(64, activation='relu') self.action_layer = tf.keras.layers.Dense(1, activation='tanh') self.log_std = tf.keras.layers.Dense(1) self.learning_rate = 2e-4 self.output_info = {'action': (1,), 'log_std': (1,), 'hidden1': (32,), 'hidden2': (32,)} self.input_name = ('pos', 'hidden1', 'hidden2') self.optimizer = 'Adam' # @tf.function def call(self, vel, hidden1, hidden2): hidden_states = (hidden1, hidden2) vel = tf.expand_dims(vel, axis=1) whole_seq, last_seq, hidden_state = self.lstm(vel, hidden_states) x = self.dense1(whole_seq) x = self.dense2(x) action = self.action_layer(x) log_std = self.log_std(x) return (action, log_std, last_seq, hidden_state) def reset(self): pass 2. Q value network Creating Q value network is similar to creating actor. However, the call 's return of Q is tf tensor not tuple. And in Q, the output_info can not be specified. class Q_net(tf.keras.Model): def __init__(self): super(Q_net, self).__init__() self.dense1 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.orthogonal()) self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.orthogonal()) self.dense3 = tf.keras.layers.Dense(1, activation=None, kernel_initializer=tf.keras.initializers.orthogonal()) # point out leaning rate # each model can have different learning rate self.learning_rate = 2e-3 # point out optimizer # each model can have different optimizer self.optimizer = 'Adam' # point out input data name, this name must be contained in obs_info self.input_name = ('obs', 'action') def reset(self): # This model does not contain RNN, so this function is not necessary, # just pass # If the model contains RNN, you should reset the state of RNN pass @tf.function def call(self, obs, action): x = tf.concat([obs, action], axis=-1) x = self.dense1(x) x = self.dense2(x) x = self.dense3(x) return x 3. Environment Gym environment If you use Gym environment, the you can wrap the environment by the following steps: 1. Inherit from AquaML.BaseClass.RLBaseEnv from AquaML.BaseClass import RLBaseEnv 2. Specify information of environment import gym from AquaML.DataType import DataInfo class PendulumWrapper(RLBaseEnv): def __init__(self, env_name: str): super().__init__() # TODO: update in the future self.env = gym.make(env_name) self.env_name = env_name # our frame work support POMDP env self._obs_info = DataInfo( names=('obs',), shapes=((3,)), dtypes=np.float32 ) If you want change the environment into POMDP, then you can: # If you want specify different observation you can self._obs_info = DataInfo( names=('obs','pos'), shapes=((3,),(2,)), dtypes=np.float32 ) 3. Implement reset def reset(self): observation = self.env.reset() observation = observation.reshape(1, -1) # observation = tf.convert_to_tensor(observation, dtype=tf.float32) obs = {'obs': observation} obs = self.initial_obs(obs) return obs For POMDP version: def reset(self): observation = self.env.reset() observation = observation.reshape(1, -1) # observation = tf.convert_to_tensor(observation, dtype=tf.float32) # obs = {'obs': observation} obs = {'obs': observation, 'pos': observation[:, :2]} obs = self.initial_obs(obs) return obs 4. Implement step def step(self, action_dict): action = action_dict['action'] action *= 2 observation, reward, done, info = self.env.step(action) observation = observation.reshape(1, -1) obs = {'obs': observation} obs = self.check_obs(obs, action_dict) reward = {'total_reward': reward} return obs, reward, done, info For POMDP version: def step(self, action_dict): action = action_dict['action'] action *= 2 observation, reward, done, info = self.env.step(action) observation = observation.reshape(1, -1) obs = {'obs': observation, 'pos': observation[:, :2]} obs = self.check_obs(obs, action_dict) # obs = {'obs': observation} reward = {'total_reward': reward} return obs, reward, done, info 4. Define algorithm parameters This tutorial is about SAC, so : from AquaML.rlalgo.Parameters import SAC2_parameter sac_parameter = SAC2_parameter( epoch_length=200, n_epochs=10, batch_size=32, discount=0.99, alpha=0.2, tau=0.005, buffer_size=100000, mini_buffer_size=1000, update_interval=50, ) model_class_dict = { 'actor': Actor_net, 'qf1': Q_net, 'qf2': Q_net, } 5. Create task starter from AquaML.starter.RLTaskStarter import RLTaskStarter # RL task starter starter = RLTaskStarter( env=env, model_class_dict=model_class_dict, algo=SAC2, algo_hyperparameter=sac_parameter, ) 6. Run task starter.run() 7. Run by MPI You can change the following codes to run parallelly. Configure gpu import sys sys.path.append('..') from AquaML.Tool import allocate_gpu from mpi4py import MPI # get group communicator comm = MPI.COMM_WORLD allocate_gpu(comm) Notice: This block must add at the head of python script. Revise hyper parameters sac_parameter = SAC2_parameter( episode_length=200, n_epochs=200, batch_size=256, discount=0.99, tau=0.005, buffer_size=100000, mini_buffer_size=5000, update_interval=1000, display_interval=1, calculate_episodes=5, alpha_learning_rate=3e-3, update_times=100, ) add MPI.comm to rl starter starter = RLTaskStarter( env=env, model_class_dict=model_class_dict, algo=SAC2, algo_hyperparameter=sac_parameter, mpi_comm=comm, name='SAC' ) After those steps, you can run by the following command in terminal: mpirun -n 6 python Tutorial1.py Create new reinforcement algorithm Create reinforcement learning environment Change logs v1.1 unify MPIRuner API. Add com package, it contains all base class. save_data and load_data are created for supervised learning and expert learning. Gradually convert our framework to next generation like HPC-v0.1. The following algos just use DataCollector (support all type of algo) instead of DataManeger . Add plot tools for rosbag, paper. v2.0 split optimize thread and worker thread. add soft actor critic. Requirement seaborn<=0.9","title":"Home"},{"location":"#aquaml","text":"","title":"AquaML"},{"location":"#features","text":"Support reinforcement learning, generative learning algorithm. Support reinforcement learning training with recurrent neural networks. Support RNN for reinforcement learning and provide two basic forms. Support multi-thread sampling and parameters tuning. Support high performance computer(HPC) Data communication has almost zero lat ency when running on a single machine.","title":"Features"},{"location":"#install","text":"","title":"Install"},{"location":"#tutorials","text":"","title":"Tutorials"},{"location":"#train-pendulum-v0-with-soft-actor-criticsac","text":"This tutorial is to show how to use AquaML to control pendulum-v0(https://gym.openai.com/envs/Pendulum-v0/). The environment is a continuous action space environment. The action is a 1-dim vector. The observation is a 3-dim vector. All the codes are available in Tutorial/tutorial1.py.","title":"Train Pendulum-v0 with soft-actor-critic(SAC)"},{"location":"#create-neural-network-model-for-reinforcement-learning","text":"AquaML just supports 'expert' TF model style, you can learn more in https://tensorflow.google.cn/overview. But in AquaML, the reinforcement learning model must inherit from RLBaseModel .","title":"Create neural network model for reinforcement learning"},{"location":"#1-actor","text":"Before creating please do the following things: import tensorflow as tf Then we can create the model. class Actor_net(tf.keras.Model): def __init__(self): super(Actor_net, self).__init__() self.dense1 = tf.keras.layers.Dense(64, activation='relu') self.dense2 = tf.keras.layers.Dense(64, activation='tanh') Point out learning rate. In AuqaML , each model can have its own learning rate self.learning_rate = 2e-4 Our framework can fusion muti type data, please specify the input data name self.input_name = ('obs',) Actor net is special than others, its out may be different. Thus you should specify . self.output_info = {'action': (1,)} Then specify the optimizer of your neural network. _name contains name of data, _info also contains shape. self.optimizer = 'Adam' Then declaim call function: def call(self, obs): x = self.dense1(obs) x = self.dense2(x) x = self.dense3(x) # the output of actor network must be a tuple # and the order of output must be the same as the order of output name return (x,) For actor, the return must be a tuple. reset function is also needed in AquaML . def reset(self): pass Our frame work support adaptive log_std . So the output of the Actor_net also contains log_std . For POMDP version: class Actor_net(tf.keras.Model): def __init__(self): super(Actor_net, self).__init__() self.lstm = tf.keras.layers.LSTM(32, input_shape=(2,), return_sequences=False, return_state=True) self.dense1 = tf.keras.layers.Dense(64, activation='relu') self.dense2 = tf.keras.layers.Dense(64, activation='relu') self.action_layer = tf.keras.layers.Dense(1, activation='tanh') self.log_std = tf.keras.layers.Dense(1) self.learning_rate = 2e-4 self.output_info = {'action': (1,), 'log_std': (1,), 'hidden1': (32,), 'hidden2': (32,)} self.input_name = ('pos', 'hidden1', 'hidden2') self.optimizer = 'Adam' # @tf.function def call(self, vel, hidden1, hidden2): hidden_states = (hidden1, hidden2) vel = tf.expand_dims(vel, axis=1) whole_seq, last_seq, hidden_state = self.lstm(vel, hidden_states) x = self.dense1(whole_seq) x = self.dense2(x) action = self.action_layer(x) log_std = self.log_std(x) return (action, log_std, last_seq, hidden_state) def reset(self): pass","title":"1. Actor"},{"location":"#2-q-value-network","text":"Creating Q value network is similar to creating actor. However, the call 's return of Q is tf tensor not tuple. And in Q, the output_info can not be specified. class Q_net(tf.keras.Model): def __init__(self): super(Q_net, self).__init__() self.dense1 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.orthogonal()) self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.orthogonal()) self.dense3 = tf.keras.layers.Dense(1, activation=None, kernel_initializer=tf.keras.initializers.orthogonal()) # point out leaning rate # each model can have different learning rate self.learning_rate = 2e-3 # point out optimizer # each model can have different optimizer self.optimizer = 'Adam' # point out input data name, this name must be contained in obs_info self.input_name = ('obs', 'action') def reset(self): # This model does not contain RNN, so this function is not necessary, # just pass # If the model contains RNN, you should reset the state of RNN pass @tf.function def call(self, obs, action): x = tf.concat([obs, action], axis=-1) x = self.dense1(x) x = self.dense2(x) x = self.dense3(x) return x","title":"2. Q value network"},{"location":"#3-environment","text":"","title":"3. Environment"},{"location":"#gym-environment","text":"If you use Gym environment, the you can wrap the environment by the following steps:","title":"Gym environment"},{"location":"#1-inherit-from-aquamlbaseclassrlbaseenv","text":"from AquaML.BaseClass import RLBaseEnv","title":"1. Inherit from AquaML.BaseClass.RLBaseEnv"},{"location":"#2-specify-information-of-environment","text":"import gym from AquaML.DataType import DataInfo class PendulumWrapper(RLBaseEnv): def __init__(self, env_name: str): super().__init__() # TODO: update in the future self.env = gym.make(env_name) self.env_name = env_name # our frame work support POMDP env self._obs_info = DataInfo( names=('obs',), shapes=((3,)), dtypes=np.float32 ) If you want change the environment into POMDP, then you can: # If you want specify different observation you can self._obs_info = DataInfo( names=('obs','pos'), shapes=((3,),(2,)), dtypes=np.float32 )","title":"2. Specify information of environment"},{"location":"#3-implement-reset","text":"def reset(self): observation = self.env.reset() observation = observation.reshape(1, -1) # observation = tf.convert_to_tensor(observation, dtype=tf.float32) obs = {'obs': observation} obs = self.initial_obs(obs) return obs For POMDP version: def reset(self): observation = self.env.reset() observation = observation.reshape(1, -1) # observation = tf.convert_to_tensor(observation, dtype=tf.float32) # obs = {'obs': observation} obs = {'obs': observation, 'pos': observation[:, :2]} obs = self.initial_obs(obs) return obs","title":"3. Implement reset"},{"location":"#4-implement-step","text":"def step(self, action_dict): action = action_dict['action'] action *= 2 observation, reward, done, info = self.env.step(action) observation = observation.reshape(1, -1) obs = {'obs': observation} obs = self.check_obs(obs, action_dict) reward = {'total_reward': reward} return obs, reward, done, info For POMDP version: def step(self, action_dict): action = action_dict['action'] action *= 2 observation, reward, done, info = self.env.step(action) observation = observation.reshape(1, -1) obs = {'obs': observation, 'pos': observation[:, :2]} obs = self.check_obs(obs, action_dict) # obs = {'obs': observation} reward = {'total_reward': reward} return obs, reward, done, info","title":"4. Implement step"},{"location":"#4-define-algorithm-parameters","text":"This tutorial is about SAC, so : from AquaML.rlalgo.Parameters import SAC2_parameter sac_parameter = SAC2_parameter( epoch_length=200, n_epochs=10, batch_size=32, discount=0.99, alpha=0.2, tau=0.005, buffer_size=100000, mini_buffer_size=1000, update_interval=50, ) model_class_dict = { 'actor': Actor_net, 'qf1': Q_net, 'qf2': Q_net, }","title":"4. Define algorithm parameters"},{"location":"#5-create-task-starter","text":"from AquaML.starter.RLTaskStarter import RLTaskStarter # RL task starter starter = RLTaskStarter( env=env, model_class_dict=model_class_dict, algo=SAC2, algo_hyperparameter=sac_parameter, )","title":"5. Create task starter"},{"location":"#6-run-task","text":"starter.run()","title":"6. Run task"},{"location":"#7-run-by-mpi","text":"You can change the following codes to run parallelly.","title":"7. Run by MPI"},{"location":"#configure-gpu","text":"import sys sys.path.append('..') from AquaML.Tool import allocate_gpu from mpi4py import MPI # get group communicator comm = MPI.COMM_WORLD allocate_gpu(comm) Notice: This block must add at the head of python script.","title":"Configure gpu"},{"location":"#revise-hyper-parameters","text":"sac_parameter = SAC2_parameter( episode_length=200, n_epochs=200, batch_size=256, discount=0.99, tau=0.005, buffer_size=100000, mini_buffer_size=5000, update_interval=1000, display_interval=1, calculate_episodes=5, alpha_learning_rate=3e-3, update_times=100, )","title":"Revise hyper parameters"},{"location":"#add-mpicomm-to-rl-starter","text":"starter = RLTaskStarter( env=env, model_class_dict=model_class_dict, algo=SAC2, algo_hyperparameter=sac_parameter, mpi_comm=comm, name='SAC' ) After those steps, you can run by the following command in terminal: mpirun -n 6 python Tutorial1.py","title":"add MPI.comm to rl starter"},{"location":"#create-new-reinforcement-algorithm","text":"","title":"Create new reinforcement algorithm"},{"location":"#create-reinforcement-learning-environment","text":"","title":"Create reinforcement learning environment"},{"location":"#change-logs","text":"","title":"Change logs"},{"location":"#v11","text":"unify MPIRuner API. Add com package, it contains all base class. save_data and load_data are created for supervised learning and expert learning. Gradually convert our framework to next generation like HPC-v0.1. The following algos just use DataCollector (support all type of algo) instead of DataManeger . Add plot tools for rosbag, paper.","title":"v1.1"},{"location":"#v20","text":"split optimize thread and worker thread. add soft actor critic.","title":"v2.0"},{"location":"#requirement","text":"seaborn<=0.9","title":"Requirement"},{"location":"BaseRLAlgo/","text":"Bases: BaseAlgo , abc . ABC create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: 1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function. 2. You need to point out which model is the actor, then in __init__ function, you should write: \"self.actor = actor\" 3. Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in __init__ function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 4. Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. 5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Source code in AquaML/rlalgo/BaseRLAlgo.py class BaseRLAlgo(BaseAlgo, abc.ABC): \"\"\" create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: 1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function. 2. You need to point out which model is the actor, then in __init__ function, you should write: \"self.actor = actor\" 3. Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in __init__ function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 4. Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. 5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. \"\"\" # TODO:\u7edf\u4e00\u8f93\u5165\u63a5\u53e3 # TODO:\u5224\u65ad\u662f\u5426\u542f\u52a8\u591a\u7ebf\u7a0b (done) def __init__(self, env, rl_io_info: RLIOInfo, name: str, update_interval: int = 0, mini_buffer_size: int = 0, calculate_episodes=5, display_interval=1, computer_type: str = 'PC', level: int = 0, thread_ID: int = -1, total_threads: int = 1, policy_type: str = 'off'): \"\"\"create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: 1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function. 2. You need to point out which model is the actor, then in __init__ function, you should write: \"self.actor = actor\" 3. Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in __init__ function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 4. Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. 5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Args: env (AquaML.rlalgo.EnvBase): reinforcement learning environment. rl_io_info (RLIOInfo): reinforcement learning input and output information. name (str): reinforcement learning name. update_interval (int, optional): update interval. This is an important parameter. It determines how many steps to update the model. If update_interval == 1, it means update model after each step. if update_interval == 0, it means update model after buffer is full. Defaults to 0. That is unusually used in on-policy algorithm. if update_interval > 0, it is used in off-policy algorithm. mini_buffer_size (int, optional): mini buffer size. Defaults to 0. calculate_episodes (int): How many episode will be used to summary. We recommend to use 1 when using multi thread. display_interval (int, optional): display interval. Defaults to 1. computer_type (str, optional): 'PC' or 'HPC'. Defaults to 'PC'. level (int, optional): thread level. 0 means main thread, 1 means sub thread. Defaults to 0. thread_ID (int, optional): ID is given by mpi. -1 means single thread. Defaults to -1. total_threads (int, optional): total threads. Defaults to 1. policy_type (str, optional): 'off' or 'on'. Defaults to 'off'. Raises: ValueError: if thread_ID == -1, it means single thread, then level must be 0. ValueError: if computer_type == 'HPC', then level must be 0. ValueError('Sync model must be given.') \"\"\" self.recoder = None self.explore_policy = None self.tf_log_std = None self.log_std = None self.rl_io_info = rl_io_info # if thread_ID == -1, it means single thread self.name = name self.env = env self.update_interval = update_interval self.mini_buffer_size = mini_buffer_size self.display_interval = display_interval self.cache_path = name + '/cache' # cache path self.log_path = name + '/log' # parameter of multithread self._computer_type = computer_type self.level = level self.thread_ID = thread_ID self.total_threads = total_threads # main thread is not included if self.total_threads == 1: self.sample_threads = total_threads else: self.sample_threads = total_threads - 1 self.each_thread_summary_episodes = calculate_episodes ''' If it runs in single thread, self.thread_ID == 0, self.total_threads == 1 ''' # create data pool according to thread level self.data_pool = DataPool(name=self.name, level=self.level, computer_type=self._computer_type) # data_pool is a handle self.actor = None # actor model. Need point out which model is the actor. # TODO: \u79c1\u6709\u53d8\u91cf\u4f1a\u51fa\u95ee\u9898\uff0c\u8c8c\u4f3c\u8fd9\u4e2a\u6ca1\u7528 self._explore_dict = {} # store explore policy, convenient for multi thread # TODO: \u9700\u8981\u5347\u7ea7\u4e3a\u5f02\u6b65\u6267\u884c\u7684\u65b9\u5f0f # TODO: \u9700\u8981\u786e\u8ba4\u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u5f97\u5230\u5f97\u786c\u4ef6\u4e0d\u4e00\u6837\u662f\u5426\u5f71\u54cd\u6267\u884c\u901f\u5ea6 # allocate start index and size for each thread # main thread will part in sample data # just used when computer_type == 'PC' if self._computer_type == 'PC': if self.thread_ID > 0: # thread ID start from 0 self.each_thread_size = int(self.rl_io_info.buffer_size / self.sample_threads) self.each_thread_start_index = int((self.thread_ID - 1) * self.each_thread_size) self.max_buffer_size = self.each_thread_size * self.sample_threads # if mini_buffer_size == 0, it means pre-sample data is disabled self.each_thread_mini_buffer_size = int(self.mini_buffer_size / self.sample_threads) self.mini_buffer_size = int(self.each_thread_mini_buffer_size * self.sample_threads) if self.update_interval == 0: # \u8fd9\u79cd\u60c5\u5f62\u5c5e\u4e8e\u5c06\u6240\u6709buffer\u586b\u5145\u6ee1\u4ee5\u540e\u518d\u66f4\u65b0\u6a21\u578b # if update_interval == 0, it means update model after buffer is full self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: # if update_interval != 0, it means update model after each step # then we need to calculate how many steps to update model for each thread # \u6bcf\u4e2a\u7ebf\u7a0b\u66f4\u65b0\u591a\u5c11\u6b21\u7b49\u5f85\u66f4\u65b0\u6a21\u578b self.each_thread_update_interval = int( self.update_interval / self.sample_threads) # update interval for each thread if self.level > 0: self.sample_id = self.thread_ID - 1 else: self.sample_id = 0 else: self.each_thread_size = self.rl_io_info.buffer_size self.each_thread_start_index = 0 self.each_thread_mini_buffer_size = self.mini_buffer_size if self.update_interval == 0: self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: self.each_thread_update_interval = self.update_interval # update interval for each thread self.max_buffer_size = self.each_thread_size self.thread_ID = 0 self.sample_id = 0 # sample id is used to identify which thread is sampling data # self.each_thread_update_interval = self.update_interval # update interval for each thread else: # TODO: HPC will implement in the future self.each_thread_size = None self.each_thread_start_index = None self.each_thread_update_interval = None # create worker if self.total_threads > 1: if self.level == 0: self.env = None self.worker = None else: self.worker = RLWorker(self) else: self.worker = RLWorker(self) # initial main thread if self.level == 0: # resample action # TODO: \u4f18\u5316\u6b64\u5904\u547d\u540d if self.rl_io_info.explore_info == 'self-std': self.resample_action = self._resample_action_log_std self.resample_log_prob = self._resample_log_prob_with_std elif self.rl_io_info.explore_info == 'global-std': self.resample_action = self._resample_action_no_log_std self.resample_log_prob = self._resample_log_prob_no_std elif self.rl_io_info.explore_info == 'void-std': self.resample_action = self._resample_action_log_prob self.resample_log_prob = None # hyper parameters # the hyper parameters is a dictionary # you should point out the hyper parameters in your algorithm # will be used in optimize function self.hyper_parameters = None # optimizer are created in main thread self.optimizer_dict = {} # store optimizer, convenient search self.total_segment = self.sample_threads # total segment, convenient for multi self.sample_epoch = 0 # sample epoch self.optimize_epoch = 0 # optimize epoch self.policy_type = policy_type # 'off' or 'on' # mini buffer size # according to the type of algorithm, self._sync_model_dict = {} # store sync model, convenient for multi thread self._sync_explore_dict = {} # store sync explore policy, convenient for multi thread self._all_model_dict = {} # store all model, convenient to record model # initial algorithm ############################# key component ############################# def init(self): \"\"\"initial algorithm. This function will be called by starter. \"\"\" # multi thread communication reward_info_dict = {} for name in self.rl_io_info.reward_info: reward_info_dict['summary_' + name] = ( self.total_segment * self.each_thread_summary_episodes, 1) # add summary reward information to data pool for name, shape in reward_info_dict.items(): # this must be first level name buffer = DataUnit(name=self.name + '_' + name, shape=shape, dtype=np.float32, level=self.level, computer_type=self._computer_type) self.rl_io_info.add_info(name=name, shape=shape, dtype=np.float32) self.data_pool.add_unit(name=name, data_unit=buffer) # TODO:\u5b50\u7ebf\u7a0b\u9700\u8981\u7b49\u5f85\u65f6\u95f4 check # multi thread initial if self.total_threads > 1: # multi thread # print(self.rl_io_info.data_info) self.data_pool.multi_init(self.rl_io_info.data_info, type='buffer') else: # single thread self.data_pool.create_buffer_from_dic(self.rl_io_info.data_info) # just do in m main thread if self.level == 0: # initial recoder self.recoder = Recoder(log_folder=self.log_path) else: self.recoder = None # check some information # actor model must be given if self.actor is None: raise ValueError('Actor model must be given.') def optimize(self): # compute current reward information optimize_info = self._optimize_() # all the information update here self.optimize_epoch += 1 total_steps = self.get_current_steps optimize_info['total_steps'] = total_steps if self.optimize_epoch % self.display_interval == 0: # display information epoch = int(self.optimize_epoch / self.display_interval) reward_info = self.summary_reward_info() print(\"###############epoch: {}###############\".format(epoch)) self.recoder.display_text( reward_info ) self.recoder.display_text( optimize_info ) self.recoder.record(reward_info, total_steps, prefix='reward') self.recoder.record(optimize_info, self.optimize_epoch, prefix=self.name) # record weight for key, model in self._all_model_dict.items(): self.recoder.record_weight(model, total_steps, prefix=key) def check(self): \"\"\" check some information. \"\"\" if self.policy_type == 'off': if self.mini_buffer_size is None: raise ValueError('Mini buffer size must be given.') if self._sync_model_dict is None: raise ValueError('Sync model must be given.') ############################# key function ############################# def store_data(self, obs: dict, action: dict, reward: dict, next_obs: dict, mask: int): \"\"\" store data to buffer. Args: obs (dict): observation. eg. {'obs':np.array([1,2,3])} action (dict): action. eg. {'action':np.array([1,2,3])} reward (dict): reward. eg. {'reward':np.array([1,2,3])} next_obs (dict): next observation. eg. {'next_obs':np.array([1,2,3])} mask (int): done. eg. 1 or 0 \"\"\" # store data to buffer # support multi thread idx = (self.worker.step_count - 1) % self.each_thread_size index = self.each_thread_start_index + idx # index in each thread # store obs to buffer self.data_pool.store(obs, index) # store next_obs to buffer self.data_pool.store(next_obs, index, prefix='next_') # store action to buffer self.data_pool.store(action, index) # store reward to buffer self.data_pool.store(reward, index) # store mask to buffer self.data_pool.data_pool['mask'].store(mask, index) @staticmethod def copy_weights(model1, model2): \"\"\" copy weight from model1 to model2. \"\"\" new_weights = [] target_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append(target_weights[i].numpy()) model2.set_weights(new_weights) @staticmethod def soft_update_weights(model1, model2, tau): \"\"\" soft update weight from model1 to model2. args: model1: source model model2: target model \"\"\" new_weights = [] source_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append((1 - tau) * weight.numpy() + tau * source_weights[i].numpy()) model2.set_weights(new_weights) def summary_reward_info(self): \"\"\" summary reward information. \"\"\" # calculate reward information summary_reward_info = {} for name in self.rl_io_info.reward_info: summary_reward_info[name] = np.mean(self.data_pool.get_unit_data('summary_' + name)) summary_reward_info['std'] = np.std(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['max_reward'] = np.max(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['min_reward'] = np.min(self.data_pool.get_unit_data('summary_total_reward')) return summary_reward_info # calculate episode reward information # random sample def random_sample(self, batch_size: int): \"\"\" random sample data from buffer. Args: batch_size (int): batch size. Returns: _type_: dict. data dict. \"\"\" # if using multi thread, then sample data from each segment # sample data from each segment # compute current segment size running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment buffer_size = min(self.max_buffer_size, running_step) batch_size = min(batch_size, buffer_size) sample_index = np.random.choice(range(buffer_size), batch_size, replace=False) # index_bias = (sample_index * 1.0 / self.each_thread_size) * self.each_thread_size index_bias = sample_index / self.each_thread_size index_bias = index_bias.astype(np.int32) index_bias = index_bias * self.each_thread_size sample_index = sample_index + index_bias sample_index = sample_index.astype(np.int32) # get data data_dict = self.data_pool.get_data_by_indices(sample_index, self.rl_io_info.store_data_name) return data_dict def cal_episode_info(self): \"\"\" calculate episode reward information. Returns: _type_: dict. summary reward information. \"\"\" # data_dict = self.get_current_update_data(('reward', 'mask')) # calculate current reward information # get done flag index_done = np.where(self.data_pool.get_unit_data('mask') == 0)[0] + 1 index_done_ = index_done / self.each_thread_size index_done_ = index_done_.astype(np.int32) # config segment segment_index = np.arange((0, self.total_segment)) every_segment_index = [] # split index_done for segment_id in segment_index: segment_index_done = np.where(index_done_ == segment_id)[0] every_segment_index.append(index_done[segment_index_done]) reward_dict = {} for key in self.rl_io_info.reward_info: reward_dict[key] = [] for each_segment_index in every_segment_index: # get index of done compute_index = each_segment_index[-self.each_thread_summary_episodes:] start_index = compute_index[0] for end_index in compute_index[1:]: for key in self.rl_io_info.reward_info: reward_dict[key].append(np.sum(self.data_pool.get_unit_data(key)[start_index:end_index])) start_index = end_index # summary reward information reward_summary = {'std': np.std(reward_dict['total_reward']), 'max_reward': np.max(reward_dict['total_reward']), 'min_reward': np.min(reward_dict['total_reward'])} for key in self.rl_io_info.reward_info: reward_summary[key] = np.mean(reward_dict[key]) # delete list del reward_dict return reward_summary def cal_average_batch_dict(self, data_list: list): \"\"\" calculate average batch dict. Args: data_list (list): store data dict list. Returns: _type_: dict. average batch dict. \"\"\" average_batch_dict = {} for key in data_list[0]: average_batch_dict[key] = [] for data_dict in data_list: for key, value in data_dict.items(): average_batch_dict[key].append(value) # average for key, value in average_batch_dict.items(): average_batch_dict[key] = np.mean(value) return average_batch_dict # TODO: calculate by multi thread ############################# calculate reward information ############################# # calculate general advantage estimation def calculate_GAE(self, rewards, values, next_values, masks, gamma, lamda): \"\"\" calculate general advantage estimation. Reference: ---------- [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8. Args: rewards (np.ndarray): rewards. values (np.ndarray): values. next_values (np.ndarray): next values. masks (np.ndarray): dones. gamma (float): discount factor. lamda (float): general advantage estimation factor. Returns: np.ndarray: general advantage estimation. \"\"\" gae = np.zeros_like(rewards) n_steps_target = np.zeros_like(rewards) cumulated_advantage = 0.0 length = len(rewards) index = length - 1 # td_target = rewards + gamma * next_values * masks # td_delta = td_target - values # advantage = compute_advantage(self.hyper_parameters.gamma, self.hyper_parameters.lambada, td_delta) for i in range(length): index -= 1 delta = rewards[index] + gamma * next_values[index] - values[index] cumulated_advantage = gamma * lamda * masks[index] * cumulated_advantage + delta gae[index] = cumulated_advantage n_steps_target[index] = gae[index] + values[index] return gae, n_steps_target # calculate discounted reward def calculate_discounted_reward(self, rewards, masks, gamma): \"\"\" calculate discounted reward. Args: rewards (np.ndarray): rewards. masks (np.ndarray): dones. if done, mask = 0, else mask = 1. gamma (float): discount factor. Returns: np.ndarray: discounted reward. \"\"\" discounted_reward = np.zeros_like(rewards) cumulated_reward = 0.0 length = len(rewards) index = length - 1 for i in range(length): index = index - 1 cumulated_reward = rewards[index] + gamma * cumulated_reward * masks[index] discounted_reward[index] = cumulated_reward return discounted_reward ############################# create function ############################# # create keras optimizer def create_optimizer(self, name: str, optimizer: str, lr: float): \"\"\" create keras optimizer for each model. Reference: https://keras.io/optimizers/ Args: name (str): name of this optimizer, you can call by this name. if name is 'actor', then you can call self.actor_optimizer optimizer (str): type of optimizer. eg. 'Adam'. For more information, please refer to keras.optimizers. lr (float): learning rate. \"\"\" attribute_name = name + '_optimizer' # in main thread, create optimizer if self.level == 0: # create optimizer optimizer = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr) else: # None optimizer = None # set attribute setattr(self, attribute_name, optimizer) self.optimizer_dict[name] = getattr(self, attribute_name) def create_none_optimizer(self, name: str): \"\"\" create none optimizer for each model. Args: name (str): name of this optimizer, you can call by this name. \"\"\" attribute_name = name + '_optimizer' optimizer = None setattr(self, attribute_name, optimizer) # Gaussian exploration policy def create_gaussian_exploration_policy(self): # TODO: sync with tf_std # verity the style of log_std if self.rl_io_info.explore_info == 'self-std': # log_std provided by actor # create explore policy # self.__explore_dict = None self.explore_policy = GaussianExplorePolicy(shape=self.rl_io_info.actor_out_info['action']) elif self.rl_io_info.explore_info == 'global-std': # log_std provided by auxiliary variable # create args by data unit self.log_std = DataUnit(name=self.name + '_log_std', dtype=np.float32, shape=self.rl_io_info.actor_out_info['action'], level=self.level, computer_type=self._computer_type) self.log_std.set_value(np.zeros(self.rl_io_info.actor_out_info['action'], dtype=np.float32) - 0.5) self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True) self._explore_dict = {'log_std': self.tf_log_std} self.rl_io_info.add_info(name='log_std', shape=self.log_std.shape, dtype=self.log_std.dtype) self.data_pool.add_unit(name='log_std', data_unit=self.log_std) self.explore_policy = GaussianExplorePolicy(shape=self.rl_io_info.actor_out_info['action']) elif self.rl_io_info.explore_info == 'void-std': # log_std is void self.explore_policy = VoidExplorePolicy(shape=self.rl_io_info.actor_out_info['action']) ############################# get function ################################ def get_action_train(self, obs: dict): \"\"\" sample action in the training process. Args: obs (dict): observation from environment. eg. {'obs':data}. The data must be tensor. And its shape is (batch, feature). Returns: _type_: _description_ \"\"\" input_data = [] # get actor input for key in self.actor.input_name: input_data.append(tf.cast(obs[key], dtype=tf.float32)) actor_out = self.actor(*input_data) # out is a tuple policy_out = dict(zip(self.actor.output_info, actor_out)) for name, value in self._explore_dict.items(): policy_out[name] = value action, prob = self.explore_policy(policy_out) policy_out['action'] = action policy_out['prob'] = prob # create return dict according to rl_io_info.actor_out_name return_dict = dict() for name in self.rl_io_info.actor_out_name: return_dict[name] = policy_out[name] return return_dict def get_batch_data(self, data_dict: dict, start_index, end_index): \"\"\" Get batch data from data dict. The data type stored in data_dict must be tuple or tensor or array. Example: >>> data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)} >>> start_index = 0 >>> end_index = 5 >>> self.get_batch_data(data_dict, start_index, end_index) {'obs': (array([1, 2, 3, 4, 5]),)} Args: data_dict (dict): data dict. start_index (int): start index. end_index (int): end index. Returns: batch data. dict. \"\"\" batch_data = dict() for key, values in data_dict.items(): if isinstance(values, tuple) or isinstance(values, list): buffer = [] for value in values: buffer.append(value[start_index:end_index]) batch_data[key] = tuple(buffer) else: batch_data[key] = values[start_index:end_index] return batch_data # get trainable actor @property def get_trainable_actor(self): \"\"\" get trainable weights of this model. actor model is special, it has two parts, actor and explore policy. Maybe in some times, explore policy is independent on actor model. \"\"\" train_vars = self.actor.trainable_variables for key, value in self._explore_dict.items(): train_vars += [value] return train_vars # optimize in the main thread def get_corresponding_data(self, data_dict: dict, names: tuple, prefix: str = '', tf_tensor: bool = True): \"\"\" Get corresponding data from data dict. Args: data_dict (dict): data dict. names (tuple): name of data. prefix (str): prefix of data name. tf_tensor (bool): if return tf tensor. Returns: corresponding data. list or tuple. \"\"\" data = [] for name in names: name = prefix + name buffer = data_dict[name] if tf_tensor: buffer = tf.cast(buffer, dtype=tf.float32) data.append(buffer) return data # acquire current update buffer def get_current_update_data(self, names: tuple): \"\"\" Get current update data. Args: names (tuple): data name. Returns: _type_: dict. data dict. \"\"\" # running after optimize # compute sampling interval start_index = (self.optimize_epoch - 1) * self.each_thread_update_interval end_index = self.optimize_epoch * self.each_thread_update_interval index_bias = np.arange(0, self.total_segment) * self.each_thread_size return_dict = {} for name in names: return_dict[name] = [] for bias in index_bias: start_index = start_index + bias end_index = end_index + bias data_dict = self.data_pool.get_data_by_indices(np.arange(start_index, end_index).tolist(), names) for key, ls in return_dict.items(): ls.append(data_dict[key]) # concat data for key, ls in return_dict.items(): return_dict[key] = np.concatenate(ls, axis=0) return return_dict @property def get_all_data(self): \"\"\" get all data in buffer. \"\"\" return_dict = {} for key, unit in self.data_pool.data_pool.items(): return_dict[key] = unit.buffer return return_dict @property def get_current_buffer_size(self): \"\"\" compute current step. \"\"\" running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment buffer_size = min(self.max_buffer_size, running_step) return buffer_size @property def get_current_steps(self): \"\"\" compute current step. \"\"\" running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.sample_threads return running_step ############################# resample function ################################ # resample action method @tf.function def _resample_action_no_log_std(self, actor_obs: tuple): \"\"\" Explore policy in SAC2 is Gaussian exploration policy. _resample_action_no_log_std is used when actor model's out has no log_std. The output of actor model is (mu,). Args: actor_obs (tuple): actor model's input Returns: action (tf.Tensor): action log_pi (tf.Tensor): log_pi \"\"\" mu = self.actor(*actor_obs)[0] noise, prob = self.explore_policy.noise_and_prob(self.hyper_parameters.batch_size) sigma = tf.exp(self.tf_log_std) action = mu + noise * sigma log_pi = tf.math.log(prob) return action, log_pi @tf.function def _resample_action_log_std(self, actor_obs: tuple): \"\"\" Explore policy in SAC2 is Gaussian exploration policy. _resample_action_log_std is used when actor model's out has log_std. The output of actor model is (mu, log_std). Args: actor_obs (tuple): actor model's input Returns: action (tf.Tensor): action log_pi (tf.Tensor): log_pi \"\"\" out = self.actor(*actor_obs) mu, log_std = out[0], out[1] noise, prob = self.explore_policy.noise_and_prob(self.hyper_parameters.batch_size) sigma = tf.exp(log_std) action = mu + noise * sigma log_prob = tf.math.log(prob) return action, log_prob # @tf.function def _resample_action_log_prob(self, actor_obs: tuple): \"\"\" Explore policy in SAC2 is Gaussian exploration policy. _resample_action_log_prob is used when actor model's out has log_prob. The output of actor model is (mu, log_std). Args: actor_obs (tuple): actor model's input Returns: action (tf.Tensor): action log_pi (tf.Tensor): log_pi \"\"\" action, log_prob = self.actor(*actor_obs) return action, log_prob def _resample_log_prob_no_std(self, obs, action): \"\"\" Re get log_prob of action. The output of actor model is (mu,). It is different from resample_action. Args: obs (tuple): observation. action (tf.Tensor): action. \"\"\" out = self.actor(*obs) mu = out[0] std = tf.exp(self.tf_log_std) log_prob = self.explore_policy.resample_prob(mu, std, action) return log_prob # def _resample_log_prob_with_std(self, obs, action): def _resample_log_prob_with_std(self, obs, action): \"\"\" Re get log_prob of action. The output of actor model is (mu, log_std,). It is different from resample_action. \"\"\" out = self.actor(*obs) mu = out[0] log_std = out[1] std = tf.exp(log_std) log_prob = self.explore_policy.resample_prob(mu, std, action) return log_prob def concat_dict(self, dict_tuple: tuple): \"\"\" concat dict. Args: dict_tuple (tuple): dict tuple. Returns: _type_: dict. concat dict. \"\"\" concat_dict = {} for data_dict in dict_tuple: for key, value in data_dict.items(): if key in concat_dict: Warning('key {} is already in concat dict'.format(key)) else: concat_dict[key] = value return concat_dict def initialize_model_weights(self, model): \"\"\" initial model. \"\"\" input_data_name = model.input_name # create tensor according to input data name input_data = [] for name in input_data_name: shape, _ = self.rl_io_info.get_data_info(name) data = tf.zeros(shape=shape, dtype=tf.float32) input_data.append(data) model(*input_data) def sync(self): \"\"\" sync. Used in multi thread. \"\"\" if self.level == 0: for key, model in self._sync_model_dict.items(): model.save_weights(self.cache_path + '/' + key + '.h5') else: for key, model in self._sync_model_dict.items(): model.load_weights(self.cache_path + '/' + key + '.h5') if self.log_std is not None: self.sync_log_std() def close(self): \"\"\" close. \"\"\" self.data_pool.close() def sync_log_std(self): \"\"\" sync log std. \"\"\" if self.level == 0: self.log_std.set_value(self.tf_log_std.numpy()) # write log std to shared memory else: self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True) # read log std from shared memory # optimize model @abc.abstractmethod def _optimize_(self, *args, **kwargs): \"\"\" optimize model. It is a abstract method. Recommend when you implement this method, input of this method should be hyperparameters. The hyperparameters can be tuned in the training process. Returns: _type_: dict. Optimizer information. eg. {'loss':data, 'total_reward':data} \"\"\" each_thread_summary_episodes = calculate_episodes instance-attribute If it runs in single thread, self.thread_ID == 0, self.total_threads == 1 get_all_data property get all data in buffer. get_current_buffer_size property compute current step. get_current_steps property compute current step. get_trainable_actor property get trainable weights of this model. actor model is special, it has two parts, actor and explore policy. Maybe in some times, explore policy is independent on actor model. __init__(env, rl_io_info, name, update_interval=0, mini_buffer_size=0, calculate_episodes=5, display_interval=1, computer_type='PC', level=0, thread_ID=-1, total_threads=1, policy_type='off') create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: You should run init() function in your init function. The position of init() function is at the end of init function. You need to point out which model is the actor, then in init function, you should write: \"self.actor = actor\" Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in init function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Parameters: Name Type Description Default env AquaML . rlalgo . EnvBase reinforcement learning environment. required rl_io_info RLIOInfo reinforcement learning input and output information. required name str reinforcement learning name. required update_interval int update interval. This is an important parameter. 0 mini_buffer_size int mini buffer size. Defaults to 0. 0 calculate_episodes int How many episode will be used to summary. We recommend to use 1 when using multi thread. 5 display_interval int display interval. Defaults to 1. 1 computer_type str 'PC' or 'HPC'. Defaults to 'PC'. 'PC' level int thread level. 0 means main thread, 1 means sub thread. Defaults to 0. 0 thread_ID int ID is given by mpi. -1 means single thread. Defaults to -1. -1 total_threads int total threads. Defaults to 1. 1 policy_type str 'off' or 'on'. Defaults to 'off'. 'off' Raises: Type Description ValueError if thread_ID == -1, it means single thread, then level must be 0. ValueError if computer_type == 'HPC', then level must be 0. Source code in AquaML/rlalgo/BaseRLAlgo.py def __init__(self, env, rl_io_info: RLIOInfo, name: str, update_interval: int = 0, mini_buffer_size: int = 0, calculate_episodes=5, display_interval=1, computer_type: str = 'PC', level: int = 0, thread_ID: int = -1, total_threads: int = 1, policy_type: str = 'off'): \"\"\"create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: 1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function. 2. You need to point out which model is the actor, then in __init__ function, you should write: \"self.actor = actor\" 3. Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in __init__ function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 4. Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. 5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Args: env (AquaML.rlalgo.EnvBase): reinforcement learning environment. rl_io_info (RLIOInfo): reinforcement learning input and output information. name (str): reinforcement learning name. update_interval (int, optional): update interval. This is an important parameter. It determines how many steps to update the model. If update_interval == 1, it means update model after each step. if update_interval == 0, it means update model after buffer is full. Defaults to 0. That is unusually used in on-policy algorithm. if update_interval > 0, it is used in off-policy algorithm. mini_buffer_size (int, optional): mini buffer size. Defaults to 0. calculate_episodes (int): How many episode will be used to summary. We recommend to use 1 when using multi thread. display_interval (int, optional): display interval. Defaults to 1. computer_type (str, optional): 'PC' or 'HPC'. Defaults to 'PC'. level (int, optional): thread level. 0 means main thread, 1 means sub thread. Defaults to 0. thread_ID (int, optional): ID is given by mpi. -1 means single thread. Defaults to -1. total_threads (int, optional): total threads. Defaults to 1. policy_type (str, optional): 'off' or 'on'. Defaults to 'off'. Raises: ValueError: if thread_ID == -1, it means single thread, then level must be 0. ValueError: if computer_type == 'HPC', then level must be 0. ValueError('Sync model must be given.') \"\"\" self.recoder = None self.explore_policy = None self.tf_log_std = None self.log_std = None self.rl_io_info = rl_io_info # if thread_ID == -1, it means single thread self.name = name self.env = env self.update_interval = update_interval self.mini_buffer_size = mini_buffer_size self.display_interval = display_interval self.cache_path = name + '/cache' # cache path self.log_path = name + '/log' # parameter of multithread self._computer_type = computer_type self.level = level self.thread_ID = thread_ID self.total_threads = total_threads # main thread is not included if self.total_threads == 1: self.sample_threads = total_threads else: self.sample_threads = total_threads - 1 self.each_thread_summary_episodes = calculate_episodes ''' If it runs in single thread, self.thread_ID == 0, self.total_threads == 1 ''' # create data pool according to thread level self.data_pool = DataPool(name=self.name, level=self.level, computer_type=self._computer_type) # data_pool is a handle self.actor = None # actor model. Need point out which model is the actor. # TODO: \u79c1\u6709\u53d8\u91cf\u4f1a\u51fa\u95ee\u9898\uff0c\u8c8c\u4f3c\u8fd9\u4e2a\u6ca1\u7528 self._explore_dict = {} # store explore policy, convenient for multi thread # TODO: \u9700\u8981\u5347\u7ea7\u4e3a\u5f02\u6b65\u6267\u884c\u7684\u65b9\u5f0f # TODO: \u9700\u8981\u786e\u8ba4\u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u5f97\u5230\u5f97\u786c\u4ef6\u4e0d\u4e00\u6837\u662f\u5426\u5f71\u54cd\u6267\u884c\u901f\u5ea6 # allocate start index and size for each thread # main thread will part in sample data # just used when computer_type == 'PC' if self._computer_type == 'PC': if self.thread_ID > 0: # thread ID start from 0 self.each_thread_size = int(self.rl_io_info.buffer_size / self.sample_threads) self.each_thread_start_index = int((self.thread_ID - 1) * self.each_thread_size) self.max_buffer_size = self.each_thread_size * self.sample_threads # if mini_buffer_size == 0, it means pre-sample data is disabled self.each_thread_mini_buffer_size = int(self.mini_buffer_size / self.sample_threads) self.mini_buffer_size = int(self.each_thread_mini_buffer_size * self.sample_threads) if self.update_interval == 0: # \u8fd9\u79cd\u60c5\u5f62\u5c5e\u4e8e\u5c06\u6240\u6709buffer\u586b\u5145\u6ee1\u4ee5\u540e\u518d\u66f4\u65b0\u6a21\u578b # if update_interval == 0, it means update model after buffer is full self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: # if update_interval != 0, it means update model after each step # then we need to calculate how many steps to update model for each thread # \u6bcf\u4e2a\u7ebf\u7a0b\u66f4\u65b0\u591a\u5c11\u6b21\u7b49\u5f85\u66f4\u65b0\u6a21\u578b self.each_thread_update_interval = int( self.update_interval / self.sample_threads) # update interval for each thread if self.level > 0: self.sample_id = self.thread_ID - 1 else: self.sample_id = 0 else: self.each_thread_size = self.rl_io_info.buffer_size self.each_thread_start_index = 0 self.each_thread_mini_buffer_size = self.mini_buffer_size if self.update_interval == 0: self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: self.each_thread_update_interval = self.update_interval # update interval for each thread self.max_buffer_size = self.each_thread_size self.thread_ID = 0 self.sample_id = 0 # sample id is used to identify which thread is sampling data # self.each_thread_update_interval = self.update_interval # update interval for each thread else: # TODO: HPC will implement in the future self.each_thread_size = None self.each_thread_start_index = None self.each_thread_update_interval = None # create worker if self.total_threads > 1: if self.level == 0: self.env = None self.worker = None else: self.worker = RLWorker(self) else: self.worker = RLWorker(self) # initial main thread if self.level == 0: # resample action # TODO: \u4f18\u5316\u6b64\u5904\u547d\u540d if self.rl_io_info.explore_info == 'self-std': self.resample_action = self._resample_action_log_std self.resample_log_prob = self._resample_log_prob_with_std elif self.rl_io_info.explore_info == 'global-std': self.resample_action = self._resample_action_no_log_std self.resample_log_prob = self._resample_log_prob_no_std elif self.rl_io_info.explore_info == 'void-std': self.resample_action = self._resample_action_log_prob self.resample_log_prob = None # hyper parameters # the hyper parameters is a dictionary # you should point out the hyper parameters in your algorithm # will be used in optimize function self.hyper_parameters = None # optimizer are created in main thread self.optimizer_dict = {} # store optimizer, convenient search self.total_segment = self.sample_threads # total segment, convenient for multi self.sample_epoch = 0 # sample epoch self.optimize_epoch = 0 # optimize epoch self.policy_type = policy_type # 'off' or 'on' # mini buffer size # according to the type of algorithm, self._sync_model_dict = {} # store sync model, convenient for multi thread self._sync_explore_dict = {} # store sync explore policy, convenient for multi thread self._all_model_dict = {} # store all model, convenient to record model cal_average_batch_dict(data_list) calculate average batch dict. Parameters: Name Type Description Default data_list list store data dict list. required Returns: Name Type Description _type_ dict. average batch dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def cal_average_batch_dict(self, data_list: list): \"\"\" calculate average batch dict. Args: data_list (list): store data dict list. Returns: _type_: dict. average batch dict. \"\"\" average_batch_dict = {} for key in data_list[0]: average_batch_dict[key] = [] for data_dict in data_list: for key, value in data_dict.items(): average_batch_dict[key].append(value) # average for key, value in average_batch_dict.items(): average_batch_dict[key] = np.mean(value) return average_batch_dict cal_episode_info() calculate episode reward information. Returns: Name Type Description _type_ dict. summary reward information. Source code in AquaML/rlalgo/BaseRLAlgo.py def cal_episode_info(self): \"\"\" calculate episode reward information. Returns: _type_: dict. summary reward information. \"\"\" # data_dict = self.get_current_update_data(('reward', 'mask')) # calculate current reward information # get done flag index_done = np.where(self.data_pool.get_unit_data('mask') == 0)[0] + 1 index_done_ = index_done / self.each_thread_size index_done_ = index_done_.astype(np.int32) # config segment segment_index = np.arange((0, self.total_segment)) every_segment_index = [] # split index_done for segment_id in segment_index: segment_index_done = np.where(index_done_ == segment_id)[0] every_segment_index.append(index_done[segment_index_done]) reward_dict = {} for key in self.rl_io_info.reward_info: reward_dict[key] = [] for each_segment_index in every_segment_index: # get index of done compute_index = each_segment_index[-self.each_thread_summary_episodes:] start_index = compute_index[0] for end_index in compute_index[1:]: for key in self.rl_io_info.reward_info: reward_dict[key].append(np.sum(self.data_pool.get_unit_data(key)[start_index:end_index])) start_index = end_index # summary reward information reward_summary = {'std': np.std(reward_dict['total_reward']), 'max_reward': np.max(reward_dict['total_reward']), 'min_reward': np.min(reward_dict['total_reward'])} for key in self.rl_io_info.reward_info: reward_summary[key] = np.mean(reward_dict[key]) # delete list del reward_dict return reward_summary calculate_GAE(rewards, values, next_values, masks, gamma, lamda) calculate general advantage estimation. Reference: [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8. Parameters: Name Type Description Default rewards np . ndarray rewards. required values np . ndarray values. required next_values np . ndarray next values. required masks np . ndarray dones. required gamma float discount factor. required lamda float general advantage estimation factor. required Returns: Type Description np.ndarray: general advantage estimation. Source code in AquaML/rlalgo/BaseRLAlgo.py def calculate_GAE(self, rewards, values, next_values, masks, gamma, lamda): \"\"\" calculate general advantage estimation. Reference: ---------- [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8. Args: rewards (np.ndarray): rewards. values (np.ndarray): values. next_values (np.ndarray): next values. masks (np.ndarray): dones. gamma (float): discount factor. lamda (float): general advantage estimation factor. Returns: np.ndarray: general advantage estimation. \"\"\" gae = np.zeros_like(rewards) n_steps_target = np.zeros_like(rewards) cumulated_advantage = 0.0 length = len(rewards) index = length - 1 # td_target = rewards + gamma * next_values * masks # td_delta = td_target - values # advantage = compute_advantage(self.hyper_parameters.gamma, self.hyper_parameters.lambada, td_delta) for i in range(length): index -= 1 delta = rewards[index] + gamma * next_values[index] - values[index] cumulated_advantage = gamma * lamda * masks[index] * cumulated_advantage + delta gae[index] = cumulated_advantage n_steps_target[index] = gae[index] + values[index] return gae, n_steps_target calculate_discounted_reward(rewards, masks, gamma) calculate discounted reward. Parameters: Name Type Description Default rewards np . ndarray rewards. required masks np . ndarray dones. if done, mask = 0, else mask = 1. required gamma float discount factor. required Returns: Type Description np.ndarray: discounted reward. Source code in AquaML/rlalgo/BaseRLAlgo.py def calculate_discounted_reward(self, rewards, masks, gamma): \"\"\" calculate discounted reward. Args: rewards (np.ndarray): rewards. masks (np.ndarray): dones. if done, mask = 0, else mask = 1. gamma (float): discount factor. Returns: np.ndarray: discounted reward. \"\"\" discounted_reward = np.zeros_like(rewards) cumulated_reward = 0.0 length = len(rewards) index = length - 1 for i in range(length): index = index - 1 cumulated_reward = rewards[index] + gamma * cumulated_reward * masks[index] discounted_reward[index] = cumulated_reward return discounted_reward check() check some information. Source code in AquaML/rlalgo/BaseRLAlgo.py def check(self): \"\"\" check some information. \"\"\" if self.policy_type == 'off': if self.mini_buffer_size is None: raise ValueError('Mini buffer size must be given.') if self._sync_model_dict is None: raise ValueError('Sync model must be given.') close() close. Source code in AquaML/rlalgo/BaseRLAlgo.py def close(self): \"\"\" close. \"\"\" self.data_pool.close() concat_dict(dict_tuple) concat dict. Parameters: Name Type Description Default dict_tuple tuple dict tuple. required Returns: Name Type Description _type_ dict. concat dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def concat_dict(self, dict_tuple: tuple): \"\"\" concat dict. Args: dict_tuple (tuple): dict tuple. Returns: _type_: dict. concat dict. \"\"\" concat_dict = {} for data_dict in dict_tuple: for key, value in data_dict.items(): if key in concat_dict: Warning('key {} is already in concat dict'.format(key)) else: concat_dict[key] = value return concat_dict copy_weights(model1, model2) staticmethod copy weight from model1 to model2. Source code in AquaML/rlalgo/BaseRLAlgo.py @staticmethod def copy_weights(model1, model2): \"\"\" copy weight from model1 to model2. \"\"\" new_weights = [] target_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append(target_weights[i].numpy()) model2.set_weights(new_weights) create_none_optimizer(name) create none optimizer for each model. Parameters: Name Type Description Default name str name of this optimizer, you can call by this name. required Source code in AquaML/rlalgo/BaseRLAlgo.py def create_none_optimizer(self, name: str): \"\"\" create none optimizer for each model. Args: name (str): name of this optimizer, you can call by this name. \"\"\" attribute_name = name + '_optimizer' optimizer = None setattr(self, attribute_name, optimizer) create_optimizer(name, optimizer, lr) create keras optimizer for each model. Reference https://keras.io/optimizers/ Parameters: Name Type Description Default name str name of this optimizer, you can call by this name. required optimizer str type of optimizer. eg. 'Adam'. For more information, required lr float learning rate. required Source code in AquaML/rlalgo/BaseRLAlgo.py def create_optimizer(self, name: str, optimizer: str, lr: float): \"\"\" create keras optimizer for each model. Reference: https://keras.io/optimizers/ Args: name (str): name of this optimizer, you can call by this name. if name is 'actor', then you can call self.actor_optimizer optimizer (str): type of optimizer. eg. 'Adam'. For more information, please refer to keras.optimizers. lr (float): learning rate. \"\"\" attribute_name = name + '_optimizer' # in main thread, create optimizer if self.level == 0: # create optimizer optimizer = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr) else: # None optimizer = None # set attribute setattr(self, attribute_name, optimizer) self.optimizer_dict[name] = getattr(self, attribute_name) get_action_train(obs) sample action in the training process. Parameters: Name Type Description Default obs dict observation from environment. eg. {'obs':data}. The data must be tensor. And its shape is (batch, feature). required Returns: Name Type Description _type_ description Source code in AquaML/rlalgo/BaseRLAlgo.py def get_action_train(self, obs: dict): \"\"\" sample action in the training process. Args: obs (dict): observation from environment. eg. {'obs':data}. The data must be tensor. And its shape is (batch, feature). Returns: _type_: _description_ \"\"\" input_data = [] # get actor input for key in self.actor.input_name: input_data.append(tf.cast(obs[key], dtype=tf.float32)) actor_out = self.actor(*input_data) # out is a tuple policy_out = dict(zip(self.actor.output_info, actor_out)) for name, value in self._explore_dict.items(): policy_out[name] = value action, prob = self.explore_policy(policy_out) policy_out['action'] = action policy_out['prob'] = prob # create return dict according to rl_io_info.actor_out_name return_dict = dict() for name in self.rl_io_info.actor_out_name: return_dict[name] = policy_out[name] return return_dict get_batch_data(data_dict, start_index, end_index) Get batch data from data dict. The data type stored in data_dict must be tuple or tensor or array. Example data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)} start_index = 0 end_index = 5 self.get_batch_data(data_dict, start_index, end_index) {'obs': (array([1, 2, 3, 4, 5]),)} Parameters: Name Type Description Default data_dict dict data dict. required start_index int start index. required end_index int end index. required Returns: Type Description batch data. dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_batch_data(self, data_dict: dict, start_index, end_index): \"\"\" Get batch data from data dict. The data type stored in data_dict must be tuple or tensor or array. Example: >>> data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)} >>> start_index = 0 >>> end_index = 5 >>> self.get_batch_data(data_dict, start_index, end_index) {'obs': (array([1, 2, 3, 4, 5]),)} Args: data_dict (dict): data dict. start_index (int): start index. end_index (int): end index. Returns: batch data. dict. \"\"\" batch_data = dict() for key, values in data_dict.items(): if isinstance(values, tuple) or isinstance(values, list): buffer = [] for value in values: buffer.append(value[start_index:end_index]) batch_data[key] = tuple(buffer) else: batch_data[key] = values[start_index:end_index] return batch_data get_corresponding_data(data_dict, names, prefix='', tf_tensor=True) Get corresponding data from data dict. Parameters: Name Type Description Default data_dict dict data dict. required names tuple name of data. required prefix str prefix of data name. '' tf_tensor bool if return tf tensor. True Returns: Type Description corresponding data. list or tuple. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_corresponding_data(self, data_dict: dict, names: tuple, prefix: str = '', tf_tensor: bool = True): \"\"\" Get corresponding data from data dict. Args: data_dict (dict): data dict. names (tuple): name of data. prefix (str): prefix of data name. tf_tensor (bool): if return tf tensor. Returns: corresponding data. list or tuple. \"\"\" data = [] for name in names: name = prefix + name buffer = data_dict[name] if tf_tensor: buffer = tf.cast(buffer, dtype=tf.float32) data.append(buffer) return data get_current_update_data(names) Get current update data. Parameters: Name Type Description Default names tuple data name. required Returns: Name Type Description _type_ dict. data dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_current_update_data(self, names: tuple): \"\"\" Get current update data. Args: names (tuple): data name. Returns: _type_: dict. data dict. \"\"\" # running after optimize # compute sampling interval start_index = (self.optimize_epoch - 1) * self.each_thread_update_interval end_index = self.optimize_epoch * self.each_thread_update_interval index_bias = np.arange(0, self.total_segment) * self.each_thread_size return_dict = {} for name in names: return_dict[name] = [] for bias in index_bias: start_index = start_index + bias end_index = end_index + bias data_dict = self.data_pool.get_data_by_indices(np.arange(start_index, end_index).tolist(), names) for key, ls in return_dict.items(): ls.append(data_dict[key]) # concat data for key, ls in return_dict.items(): return_dict[key] = np.concatenate(ls, axis=0) return return_dict init() initial algorithm. This function will be called by starter. Source code in AquaML/rlalgo/BaseRLAlgo.py def init(self): \"\"\"initial algorithm. This function will be called by starter. \"\"\" # multi thread communication reward_info_dict = {} for name in self.rl_io_info.reward_info: reward_info_dict['summary_' + name] = ( self.total_segment * self.each_thread_summary_episodes, 1) # add summary reward information to data pool for name, shape in reward_info_dict.items(): # this must be first level name buffer = DataUnit(name=self.name + '_' + name, shape=shape, dtype=np.float32, level=self.level, computer_type=self._computer_type) self.rl_io_info.add_info(name=name, shape=shape, dtype=np.float32) self.data_pool.add_unit(name=name, data_unit=buffer) # TODO:\u5b50\u7ebf\u7a0b\u9700\u8981\u7b49\u5f85\u65f6\u95f4 check # multi thread initial if self.total_threads > 1: # multi thread # print(self.rl_io_info.data_info) self.data_pool.multi_init(self.rl_io_info.data_info, type='buffer') else: # single thread self.data_pool.create_buffer_from_dic(self.rl_io_info.data_info) # just do in m main thread if self.level == 0: # initial recoder self.recoder = Recoder(log_folder=self.log_path) else: self.recoder = None # check some information # actor model must be given if self.actor is None: raise ValueError('Actor model must be given.') initialize_model_weights(model) initial model. Source code in AquaML/rlalgo/BaseRLAlgo.py def initialize_model_weights(self, model): \"\"\" initial model. \"\"\" input_data_name = model.input_name # create tensor according to input data name input_data = [] for name in input_data_name: shape, _ = self.rl_io_info.get_data_info(name) data = tf.zeros(shape=shape, dtype=tf.float32) input_data.append(data) model(*input_data) random_sample(batch_size) random sample data from buffer. Parameters: Name Type Description Default batch_size int batch size. required Returns: Name Type Description _type_ dict. data dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def random_sample(self, batch_size: int): \"\"\" random sample data from buffer. Args: batch_size (int): batch size. Returns: _type_: dict. data dict. \"\"\" # if using multi thread, then sample data from each segment # sample data from each segment # compute current segment size running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment buffer_size = min(self.max_buffer_size, running_step) batch_size = min(batch_size, buffer_size) sample_index = np.random.choice(range(buffer_size), batch_size, replace=False) # index_bias = (sample_index * 1.0 / self.each_thread_size) * self.each_thread_size index_bias = sample_index / self.each_thread_size index_bias = index_bias.astype(np.int32) index_bias = index_bias * self.each_thread_size sample_index = sample_index + index_bias sample_index = sample_index.astype(np.int32) # get data data_dict = self.data_pool.get_data_by_indices(sample_index, self.rl_io_info.store_data_name) return data_dict soft_update_weights(model1, model2, tau) staticmethod soft update weight from model1 to model2. model1: source model model2: target model Source code in AquaML/rlalgo/BaseRLAlgo.py @staticmethod def soft_update_weights(model1, model2, tau): \"\"\" soft update weight from model1 to model2. args: model1: source model model2: target model \"\"\" new_weights = [] source_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append((1 - tau) * weight.numpy() + tau * source_weights[i].numpy()) model2.set_weights(new_weights) store_data(obs, action, reward, next_obs, mask) store data to buffer. Parameters: Name Type Description Default obs dict observation. eg. {'obs':np.array([1,2,3])} required action dict action. eg. {'action':np.array([1,2,3])} required reward dict reward. eg. {'reward':np.array([1,2,3])} required next_obs dict next observation. eg. {'next_obs':np.array([1,2,3])} required mask int done. eg. 1 or 0 required Source code in AquaML/rlalgo/BaseRLAlgo.py def store_data(self, obs: dict, action: dict, reward: dict, next_obs: dict, mask: int): \"\"\" store data to buffer. Args: obs (dict): observation. eg. {'obs':np.array([1,2,3])} action (dict): action. eg. {'action':np.array([1,2,3])} reward (dict): reward. eg. {'reward':np.array([1,2,3])} next_obs (dict): next observation. eg. {'next_obs':np.array([1,2,3])} mask (int): done. eg. 1 or 0 \"\"\" # store data to buffer # support multi thread idx = (self.worker.step_count - 1) % self.each_thread_size index = self.each_thread_start_index + idx # index in each thread # store obs to buffer self.data_pool.store(obs, index) # store next_obs to buffer self.data_pool.store(next_obs, index, prefix='next_') # store action to buffer self.data_pool.store(action, index) # store reward to buffer self.data_pool.store(reward, index) # store mask to buffer self.data_pool.data_pool['mask'].store(mask, index) summary_reward_info() summary reward information. Source code in AquaML/rlalgo/BaseRLAlgo.py def summary_reward_info(self): \"\"\" summary reward information. \"\"\" # calculate reward information summary_reward_info = {} for name in self.rl_io_info.reward_info: summary_reward_info[name] = np.mean(self.data_pool.get_unit_data('summary_' + name)) summary_reward_info['std'] = np.std(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['max_reward'] = np.max(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['min_reward'] = np.min(self.data_pool.get_unit_data('summary_total_reward')) return summary_reward_info sync() sync. Used in multi thread. Source code in AquaML/rlalgo/BaseRLAlgo.py def sync(self): \"\"\" sync. Used in multi thread. \"\"\" if self.level == 0: for key, model in self._sync_model_dict.items(): model.save_weights(self.cache_path + '/' + key + '.h5') else: for key, model in self._sync_model_dict.items(): model.load_weights(self.cache_path + '/' + key + '.h5') if self.log_std is not None: self.sync_log_std() sync_log_std() sync log std. Source code in AquaML/rlalgo/BaseRLAlgo.py def sync_log_std(self): \"\"\" sync log std. \"\"\" if self.level == 0: self.log_std.set_value(self.tf_log_std.numpy()) # write log std to shared memory else: self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True) # read log std from shared memory options: show_source: false","title":"rlalgo:BaseRLAlgo"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.each_thread_summary_episodes","text":"If it runs in single thread, self.thread_ID == 0, self.total_threads == 1","title":"each_thread_summary_episodes"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_all_data","text":"get all data in buffer.","title":"get_all_data"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_buffer_size","text":"compute current step.","title":"get_current_buffer_size"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_steps","text":"compute current step.","title":"get_current_steps"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_trainable_actor","text":"get trainable weights of this model. actor model is special, it has two parts, actor and explore policy. Maybe in some times, explore policy is independent on actor model.","title":"get_trainable_actor"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.__init__","text":"create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: You should run init() function in your init function. The position of init() function is at the end of init function. You need to point out which model is the actor, then in init function, you should write: \"self.actor = actor\" Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in init function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Parameters: Name Type Description Default env AquaML . rlalgo . EnvBase reinforcement learning environment. required rl_io_info RLIOInfo reinforcement learning input and output information. required name str reinforcement learning name. required update_interval int update interval. This is an important parameter. 0 mini_buffer_size int mini buffer size. Defaults to 0. 0 calculate_episodes int How many episode will be used to summary. We recommend to use 1 when using multi thread. 5 display_interval int display interval. Defaults to 1. 1 computer_type str 'PC' or 'HPC'. Defaults to 'PC'. 'PC' level int thread level. 0 means main thread, 1 means sub thread. Defaults to 0. 0 thread_ID int ID is given by mpi. -1 means single thread. Defaults to -1. -1 total_threads int total threads. Defaults to 1. 1 policy_type str 'off' or 'on'. Defaults to 'off'. 'off' Raises: Type Description ValueError if thread_ID == -1, it means single thread, then level must be 0. ValueError if computer_type == 'HPC', then level must be 0. Source code in AquaML/rlalgo/BaseRLAlgo.py def __init__(self, env, rl_io_info: RLIOInfo, name: str, update_interval: int = 0, mini_buffer_size: int = 0, calculate_episodes=5, display_interval=1, computer_type: str = 'PC', level: int = 0, thread_ID: int = -1, total_threads: int = 1, policy_type: str = 'off'): \"\"\"create base for reinforcement learning algorithm. This base class provides exploration policy, data pool(multi thread). Some tools are also provided for reinforcement learning algorithm such as calculate general advantage estimation. When you create a reinforcement learning algorithm, you should inherit this class. And do the following things: 1. You should run init() function in your __init__ function. The position of init() function is at the end of __init__ function. 2. You need to point out which model is the actor, then in __init__ function, you should write: \"self.actor = actor\" 3. Explore policy is a function which is used to generate action. You should use or create a explore policy. Then in __init__ function, you should write: \"self.explore_policy = explore_policy\" You can create a explore policy by inherit ExplorePolicyBase class(AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase). 4. Notice: after optimize the model, you should update optimize_epoch. The same as sample_epoch. 5. Notice: if you want to use multi thread, please specify which model needs to be synchronized by setting self._sync_model_dict Some recommends for off-policy algorithm: 1. mini_buffer_size should have given out. Args: env (AquaML.rlalgo.EnvBase): reinforcement learning environment. rl_io_info (RLIOInfo): reinforcement learning input and output information. name (str): reinforcement learning name. update_interval (int, optional): update interval. This is an important parameter. It determines how many steps to update the model. If update_interval == 1, it means update model after each step. if update_interval == 0, it means update model after buffer is full. Defaults to 0. That is unusually used in on-policy algorithm. if update_interval > 0, it is used in off-policy algorithm. mini_buffer_size (int, optional): mini buffer size. Defaults to 0. calculate_episodes (int): How many episode will be used to summary. We recommend to use 1 when using multi thread. display_interval (int, optional): display interval. Defaults to 1. computer_type (str, optional): 'PC' or 'HPC'. Defaults to 'PC'. level (int, optional): thread level. 0 means main thread, 1 means sub thread. Defaults to 0. thread_ID (int, optional): ID is given by mpi. -1 means single thread. Defaults to -1. total_threads (int, optional): total threads. Defaults to 1. policy_type (str, optional): 'off' or 'on'. Defaults to 'off'. Raises: ValueError: if thread_ID == -1, it means single thread, then level must be 0. ValueError: if computer_type == 'HPC', then level must be 0. ValueError('Sync model must be given.') \"\"\" self.recoder = None self.explore_policy = None self.tf_log_std = None self.log_std = None self.rl_io_info = rl_io_info # if thread_ID == -1, it means single thread self.name = name self.env = env self.update_interval = update_interval self.mini_buffer_size = mini_buffer_size self.display_interval = display_interval self.cache_path = name + '/cache' # cache path self.log_path = name + '/log' # parameter of multithread self._computer_type = computer_type self.level = level self.thread_ID = thread_ID self.total_threads = total_threads # main thread is not included if self.total_threads == 1: self.sample_threads = total_threads else: self.sample_threads = total_threads - 1 self.each_thread_summary_episodes = calculate_episodes ''' If it runs in single thread, self.thread_ID == 0, self.total_threads == 1 ''' # create data pool according to thread level self.data_pool = DataPool(name=self.name, level=self.level, computer_type=self._computer_type) # data_pool is a handle self.actor = None # actor model. Need point out which model is the actor. # TODO: \u79c1\u6709\u53d8\u91cf\u4f1a\u51fa\u95ee\u9898\uff0c\u8c8c\u4f3c\u8fd9\u4e2a\u6ca1\u7528 self._explore_dict = {} # store explore policy, convenient for multi thread # TODO: \u9700\u8981\u5347\u7ea7\u4e3a\u5f02\u6b65\u6267\u884c\u7684\u65b9\u5f0f # TODO: \u9700\u8981\u786e\u8ba4\u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u5f97\u5230\u5f97\u786c\u4ef6\u4e0d\u4e00\u6837\u662f\u5426\u5f71\u54cd\u6267\u884c\u901f\u5ea6 # allocate start index and size for each thread # main thread will part in sample data # just used when computer_type == 'PC' if self._computer_type == 'PC': if self.thread_ID > 0: # thread ID start from 0 self.each_thread_size = int(self.rl_io_info.buffer_size / self.sample_threads) self.each_thread_start_index = int((self.thread_ID - 1) * self.each_thread_size) self.max_buffer_size = self.each_thread_size * self.sample_threads # if mini_buffer_size == 0, it means pre-sample data is disabled self.each_thread_mini_buffer_size = int(self.mini_buffer_size / self.sample_threads) self.mini_buffer_size = int(self.each_thread_mini_buffer_size * self.sample_threads) if self.update_interval == 0: # \u8fd9\u79cd\u60c5\u5f62\u5c5e\u4e8e\u5c06\u6240\u6709buffer\u586b\u5145\u6ee1\u4ee5\u540e\u518d\u66f4\u65b0\u6a21\u578b # if update_interval == 0, it means update model after buffer is full self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: # if update_interval != 0, it means update model after each step # then we need to calculate how many steps to update model for each thread # \u6bcf\u4e2a\u7ebf\u7a0b\u66f4\u65b0\u591a\u5c11\u6b21\u7b49\u5f85\u66f4\u65b0\u6a21\u578b self.each_thread_update_interval = int( self.update_interval / self.sample_threads) # update interval for each thread if self.level > 0: self.sample_id = self.thread_ID - 1 else: self.sample_id = 0 else: self.each_thread_size = self.rl_io_info.buffer_size self.each_thread_start_index = 0 self.each_thread_mini_buffer_size = self.mini_buffer_size if self.update_interval == 0: self.each_thread_update_interval = self.each_thread_size # update interval for each thread else: self.each_thread_update_interval = self.update_interval # update interval for each thread self.max_buffer_size = self.each_thread_size self.thread_ID = 0 self.sample_id = 0 # sample id is used to identify which thread is sampling data # self.each_thread_update_interval = self.update_interval # update interval for each thread else: # TODO: HPC will implement in the future self.each_thread_size = None self.each_thread_start_index = None self.each_thread_update_interval = None # create worker if self.total_threads > 1: if self.level == 0: self.env = None self.worker = None else: self.worker = RLWorker(self) else: self.worker = RLWorker(self) # initial main thread if self.level == 0: # resample action # TODO: \u4f18\u5316\u6b64\u5904\u547d\u540d if self.rl_io_info.explore_info == 'self-std': self.resample_action = self._resample_action_log_std self.resample_log_prob = self._resample_log_prob_with_std elif self.rl_io_info.explore_info == 'global-std': self.resample_action = self._resample_action_no_log_std self.resample_log_prob = self._resample_log_prob_no_std elif self.rl_io_info.explore_info == 'void-std': self.resample_action = self._resample_action_log_prob self.resample_log_prob = None # hyper parameters # the hyper parameters is a dictionary # you should point out the hyper parameters in your algorithm # will be used in optimize function self.hyper_parameters = None # optimizer are created in main thread self.optimizer_dict = {} # store optimizer, convenient search self.total_segment = self.sample_threads # total segment, convenient for multi self.sample_epoch = 0 # sample epoch self.optimize_epoch = 0 # optimize epoch self.policy_type = policy_type # 'off' or 'on' # mini buffer size # according to the type of algorithm, self._sync_model_dict = {} # store sync model, convenient for multi thread self._sync_explore_dict = {} # store sync explore policy, convenient for multi thread self._all_model_dict = {} # store all model, convenient to record model","title":"__init__()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.cal_average_batch_dict","text":"calculate average batch dict. Parameters: Name Type Description Default data_list list store data dict list. required Returns: Name Type Description _type_ dict. average batch dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def cal_average_batch_dict(self, data_list: list): \"\"\" calculate average batch dict. Args: data_list (list): store data dict list. Returns: _type_: dict. average batch dict. \"\"\" average_batch_dict = {} for key in data_list[0]: average_batch_dict[key] = [] for data_dict in data_list: for key, value in data_dict.items(): average_batch_dict[key].append(value) # average for key, value in average_batch_dict.items(): average_batch_dict[key] = np.mean(value) return average_batch_dict","title":"cal_average_batch_dict()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.cal_episode_info","text":"calculate episode reward information. Returns: Name Type Description _type_ dict. summary reward information. Source code in AquaML/rlalgo/BaseRLAlgo.py def cal_episode_info(self): \"\"\" calculate episode reward information. Returns: _type_: dict. summary reward information. \"\"\" # data_dict = self.get_current_update_data(('reward', 'mask')) # calculate current reward information # get done flag index_done = np.where(self.data_pool.get_unit_data('mask') == 0)[0] + 1 index_done_ = index_done / self.each_thread_size index_done_ = index_done_.astype(np.int32) # config segment segment_index = np.arange((0, self.total_segment)) every_segment_index = [] # split index_done for segment_id in segment_index: segment_index_done = np.where(index_done_ == segment_id)[0] every_segment_index.append(index_done[segment_index_done]) reward_dict = {} for key in self.rl_io_info.reward_info: reward_dict[key] = [] for each_segment_index in every_segment_index: # get index of done compute_index = each_segment_index[-self.each_thread_summary_episodes:] start_index = compute_index[0] for end_index in compute_index[1:]: for key in self.rl_io_info.reward_info: reward_dict[key].append(np.sum(self.data_pool.get_unit_data(key)[start_index:end_index])) start_index = end_index # summary reward information reward_summary = {'std': np.std(reward_dict['total_reward']), 'max_reward': np.max(reward_dict['total_reward']), 'min_reward': np.min(reward_dict['total_reward'])} for key in self.rl_io_info.reward_info: reward_summary[key] = np.mean(reward_dict[key]) # delete list del reward_dict return reward_summary","title":"cal_episode_info()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_GAE","text":"calculate general advantage estimation.","title":"calculate_GAE()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_GAE--reference","text":"[1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8. Parameters: Name Type Description Default rewards np . ndarray rewards. required values np . ndarray values. required next_values np . ndarray next values. required masks np . ndarray dones. required gamma float discount factor. required lamda float general advantage estimation factor. required Returns: Type Description np.ndarray: general advantage estimation. Source code in AquaML/rlalgo/BaseRLAlgo.py def calculate_GAE(self, rewards, values, next_values, masks, gamma, lamda): \"\"\" calculate general advantage estimation. Reference: ---------- [1] Schulman J, Moritz P, Levine S, Jordan M, Abbeel P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. 2015 Jun 8. Args: rewards (np.ndarray): rewards. values (np.ndarray): values. next_values (np.ndarray): next values. masks (np.ndarray): dones. gamma (float): discount factor. lamda (float): general advantage estimation factor. Returns: np.ndarray: general advantage estimation. \"\"\" gae = np.zeros_like(rewards) n_steps_target = np.zeros_like(rewards) cumulated_advantage = 0.0 length = len(rewards) index = length - 1 # td_target = rewards + gamma * next_values * masks # td_delta = td_target - values # advantage = compute_advantage(self.hyper_parameters.gamma, self.hyper_parameters.lambada, td_delta) for i in range(length): index -= 1 delta = rewards[index] + gamma * next_values[index] - values[index] cumulated_advantage = gamma * lamda * masks[index] * cumulated_advantage + delta gae[index] = cumulated_advantage n_steps_target[index] = gae[index] + values[index] return gae, n_steps_target","title":"Reference:"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.calculate_discounted_reward","text":"calculate discounted reward. Parameters: Name Type Description Default rewards np . ndarray rewards. required masks np . ndarray dones. if done, mask = 0, else mask = 1. required gamma float discount factor. required Returns: Type Description np.ndarray: discounted reward. Source code in AquaML/rlalgo/BaseRLAlgo.py def calculate_discounted_reward(self, rewards, masks, gamma): \"\"\" calculate discounted reward. Args: rewards (np.ndarray): rewards. masks (np.ndarray): dones. if done, mask = 0, else mask = 1. gamma (float): discount factor. Returns: np.ndarray: discounted reward. \"\"\" discounted_reward = np.zeros_like(rewards) cumulated_reward = 0.0 length = len(rewards) index = length - 1 for i in range(length): index = index - 1 cumulated_reward = rewards[index] + gamma * cumulated_reward * masks[index] discounted_reward[index] = cumulated_reward return discounted_reward","title":"calculate_discounted_reward()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.check","text":"check some information. Source code in AquaML/rlalgo/BaseRLAlgo.py def check(self): \"\"\" check some information. \"\"\" if self.policy_type == 'off': if self.mini_buffer_size is None: raise ValueError('Mini buffer size must be given.') if self._sync_model_dict is None: raise ValueError('Sync model must be given.')","title":"check()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.close","text":"close. Source code in AquaML/rlalgo/BaseRLAlgo.py def close(self): \"\"\" close. \"\"\" self.data_pool.close()","title":"close()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.concat_dict","text":"concat dict. Parameters: Name Type Description Default dict_tuple tuple dict tuple. required Returns: Name Type Description _type_ dict. concat dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def concat_dict(self, dict_tuple: tuple): \"\"\" concat dict. Args: dict_tuple (tuple): dict tuple. Returns: _type_: dict. concat dict. \"\"\" concat_dict = {} for data_dict in dict_tuple: for key, value in data_dict.items(): if key in concat_dict: Warning('key {} is already in concat dict'.format(key)) else: concat_dict[key] = value return concat_dict","title":"concat_dict()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.copy_weights","text":"copy weight from model1 to model2. Source code in AquaML/rlalgo/BaseRLAlgo.py @staticmethod def copy_weights(model1, model2): \"\"\" copy weight from model1 to model2. \"\"\" new_weights = [] target_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append(target_weights[i].numpy()) model2.set_weights(new_weights)","title":"copy_weights()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.create_none_optimizer","text":"create none optimizer for each model. Parameters: Name Type Description Default name str name of this optimizer, you can call by this name. required Source code in AquaML/rlalgo/BaseRLAlgo.py def create_none_optimizer(self, name: str): \"\"\" create none optimizer for each model. Args: name (str): name of this optimizer, you can call by this name. \"\"\" attribute_name = name + '_optimizer' optimizer = None setattr(self, attribute_name, optimizer)","title":"create_none_optimizer()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.create_optimizer","text":"create keras optimizer for each model. Reference https://keras.io/optimizers/ Parameters: Name Type Description Default name str name of this optimizer, you can call by this name. required optimizer str type of optimizer. eg. 'Adam'. For more information, required lr float learning rate. required Source code in AquaML/rlalgo/BaseRLAlgo.py def create_optimizer(self, name: str, optimizer: str, lr: float): \"\"\" create keras optimizer for each model. Reference: https://keras.io/optimizers/ Args: name (str): name of this optimizer, you can call by this name. if name is 'actor', then you can call self.actor_optimizer optimizer (str): type of optimizer. eg. 'Adam'. For more information, please refer to keras.optimizers. lr (float): learning rate. \"\"\" attribute_name = name + '_optimizer' # in main thread, create optimizer if self.level == 0: # create optimizer optimizer = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr) else: # None optimizer = None # set attribute setattr(self, attribute_name, optimizer) self.optimizer_dict[name] = getattr(self, attribute_name)","title":"create_optimizer()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_action_train","text":"sample action in the training process. Parameters: Name Type Description Default obs dict observation from environment. eg. {'obs':data}. The data must be tensor. And its shape is (batch, feature). required Returns: Name Type Description _type_ description Source code in AquaML/rlalgo/BaseRLAlgo.py def get_action_train(self, obs: dict): \"\"\" sample action in the training process. Args: obs (dict): observation from environment. eg. {'obs':data}. The data must be tensor. And its shape is (batch, feature). Returns: _type_: _description_ \"\"\" input_data = [] # get actor input for key in self.actor.input_name: input_data.append(tf.cast(obs[key], dtype=tf.float32)) actor_out = self.actor(*input_data) # out is a tuple policy_out = dict(zip(self.actor.output_info, actor_out)) for name, value in self._explore_dict.items(): policy_out[name] = value action, prob = self.explore_policy(policy_out) policy_out['action'] = action policy_out['prob'] = prob # create return dict according to rl_io_info.actor_out_name return_dict = dict() for name in self.rl_io_info.actor_out_name: return_dict[name] = policy_out[name] return return_dict","title":"get_action_train()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_batch_data","text":"Get batch data from data dict. The data type stored in data_dict must be tuple or tensor or array. Example data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)} start_index = 0 end_index = 5 self.get_batch_data(data_dict, start_index, end_index) {'obs': (array([1, 2, 3, 4, 5]),)} Parameters: Name Type Description Default data_dict dict data dict. required start_index int start index. required end_index int end index. required Returns: Type Description batch data. dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_batch_data(self, data_dict: dict, start_index, end_index): \"\"\" Get batch data from data dict. The data type stored in data_dict must be tuple or tensor or array. Example: >>> data_dict = {'obs':(np.array([1,2,3,4,5,6,7,8,9,10]),)} >>> start_index = 0 >>> end_index = 5 >>> self.get_batch_data(data_dict, start_index, end_index) {'obs': (array([1, 2, 3, 4, 5]),)} Args: data_dict (dict): data dict. start_index (int): start index. end_index (int): end index. Returns: batch data. dict. \"\"\" batch_data = dict() for key, values in data_dict.items(): if isinstance(values, tuple) or isinstance(values, list): buffer = [] for value in values: buffer.append(value[start_index:end_index]) batch_data[key] = tuple(buffer) else: batch_data[key] = values[start_index:end_index] return batch_data","title":"get_batch_data()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_corresponding_data","text":"Get corresponding data from data dict. Parameters: Name Type Description Default data_dict dict data dict. required names tuple name of data. required prefix str prefix of data name. '' tf_tensor bool if return tf tensor. True Returns: Type Description corresponding data. list or tuple. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_corresponding_data(self, data_dict: dict, names: tuple, prefix: str = '', tf_tensor: bool = True): \"\"\" Get corresponding data from data dict. Args: data_dict (dict): data dict. names (tuple): name of data. prefix (str): prefix of data name. tf_tensor (bool): if return tf tensor. Returns: corresponding data. list or tuple. \"\"\" data = [] for name in names: name = prefix + name buffer = data_dict[name] if tf_tensor: buffer = tf.cast(buffer, dtype=tf.float32) data.append(buffer) return data","title":"get_corresponding_data()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.get_current_update_data","text":"Get current update data. Parameters: Name Type Description Default names tuple data name. required Returns: Name Type Description _type_ dict. data dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def get_current_update_data(self, names: tuple): \"\"\" Get current update data. Args: names (tuple): data name. Returns: _type_: dict. data dict. \"\"\" # running after optimize # compute sampling interval start_index = (self.optimize_epoch - 1) * self.each_thread_update_interval end_index = self.optimize_epoch * self.each_thread_update_interval index_bias = np.arange(0, self.total_segment) * self.each_thread_size return_dict = {} for name in names: return_dict[name] = [] for bias in index_bias: start_index = start_index + bias end_index = end_index + bias data_dict = self.data_pool.get_data_by_indices(np.arange(start_index, end_index).tolist(), names) for key, ls in return_dict.items(): ls.append(data_dict[key]) # concat data for key, ls in return_dict.items(): return_dict[key] = np.concatenate(ls, axis=0) return return_dict","title":"get_current_update_data()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.init","text":"initial algorithm. This function will be called by starter. Source code in AquaML/rlalgo/BaseRLAlgo.py def init(self): \"\"\"initial algorithm. This function will be called by starter. \"\"\" # multi thread communication reward_info_dict = {} for name in self.rl_io_info.reward_info: reward_info_dict['summary_' + name] = ( self.total_segment * self.each_thread_summary_episodes, 1) # add summary reward information to data pool for name, shape in reward_info_dict.items(): # this must be first level name buffer = DataUnit(name=self.name + '_' + name, shape=shape, dtype=np.float32, level=self.level, computer_type=self._computer_type) self.rl_io_info.add_info(name=name, shape=shape, dtype=np.float32) self.data_pool.add_unit(name=name, data_unit=buffer) # TODO:\u5b50\u7ebf\u7a0b\u9700\u8981\u7b49\u5f85\u65f6\u95f4 check # multi thread initial if self.total_threads > 1: # multi thread # print(self.rl_io_info.data_info) self.data_pool.multi_init(self.rl_io_info.data_info, type='buffer') else: # single thread self.data_pool.create_buffer_from_dic(self.rl_io_info.data_info) # just do in m main thread if self.level == 0: # initial recoder self.recoder = Recoder(log_folder=self.log_path) else: self.recoder = None # check some information # actor model must be given if self.actor is None: raise ValueError('Actor model must be given.')","title":"init()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.initialize_model_weights","text":"initial model. Source code in AquaML/rlalgo/BaseRLAlgo.py def initialize_model_weights(self, model): \"\"\" initial model. \"\"\" input_data_name = model.input_name # create tensor according to input data name input_data = [] for name in input_data_name: shape, _ = self.rl_io_info.get_data_info(name) data = tf.zeros(shape=shape, dtype=tf.float32) input_data.append(data) model(*input_data)","title":"initialize_model_weights()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.random_sample","text":"random sample data from buffer. Parameters: Name Type Description Default batch_size int batch size. required Returns: Name Type Description _type_ dict. data dict. Source code in AquaML/rlalgo/BaseRLAlgo.py def random_sample(self, batch_size: int): \"\"\" random sample data from buffer. Args: batch_size (int): batch size. Returns: _type_: dict. data dict. \"\"\" # if using multi thread, then sample data from each segment # sample data from each segment # compute current segment size running_step = self.mini_buffer_size + self.optimize_epoch * self.each_thread_update_interval * self.total_segment buffer_size = min(self.max_buffer_size, running_step) batch_size = min(batch_size, buffer_size) sample_index = np.random.choice(range(buffer_size), batch_size, replace=False) # index_bias = (sample_index * 1.0 / self.each_thread_size) * self.each_thread_size index_bias = sample_index / self.each_thread_size index_bias = index_bias.astype(np.int32) index_bias = index_bias * self.each_thread_size sample_index = sample_index + index_bias sample_index = sample_index.astype(np.int32) # get data data_dict = self.data_pool.get_data_by_indices(sample_index, self.rl_io_info.store_data_name) return data_dict","title":"random_sample()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.soft_update_weights","text":"soft update weight from model1 to model2. model1: source model model2: target model Source code in AquaML/rlalgo/BaseRLAlgo.py @staticmethod def soft_update_weights(model1, model2, tau): \"\"\" soft update weight from model1 to model2. args: model1: source model model2: target model \"\"\" new_weights = [] source_weights = model1.weights for i, weight in enumerate(model2.weights): new_weights.append((1 - tau) * weight.numpy() + tau * source_weights[i].numpy()) model2.set_weights(new_weights)","title":"soft_update_weights()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.store_data","text":"store data to buffer. Parameters: Name Type Description Default obs dict observation. eg. {'obs':np.array([1,2,3])} required action dict action. eg. {'action':np.array([1,2,3])} required reward dict reward. eg. {'reward':np.array([1,2,3])} required next_obs dict next observation. eg. {'next_obs':np.array([1,2,3])} required mask int done. eg. 1 or 0 required Source code in AquaML/rlalgo/BaseRLAlgo.py def store_data(self, obs: dict, action: dict, reward: dict, next_obs: dict, mask: int): \"\"\" store data to buffer. Args: obs (dict): observation. eg. {'obs':np.array([1,2,3])} action (dict): action. eg. {'action':np.array([1,2,3])} reward (dict): reward. eg. {'reward':np.array([1,2,3])} next_obs (dict): next observation. eg. {'next_obs':np.array([1,2,3])} mask (int): done. eg. 1 or 0 \"\"\" # store data to buffer # support multi thread idx = (self.worker.step_count - 1) % self.each_thread_size index = self.each_thread_start_index + idx # index in each thread # store obs to buffer self.data_pool.store(obs, index) # store next_obs to buffer self.data_pool.store(next_obs, index, prefix='next_') # store action to buffer self.data_pool.store(action, index) # store reward to buffer self.data_pool.store(reward, index) # store mask to buffer self.data_pool.data_pool['mask'].store(mask, index)","title":"store_data()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.summary_reward_info","text":"summary reward information. Source code in AquaML/rlalgo/BaseRLAlgo.py def summary_reward_info(self): \"\"\" summary reward information. \"\"\" # calculate reward information summary_reward_info = {} for name in self.rl_io_info.reward_info: summary_reward_info[name] = np.mean(self.data_pool.get_unit_data('summary_' + name)) summary_reward_info['std'] = np.std(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['max_reward'] = np.max(self.data_pool.get_unit_data('summary_total_reward')) summary_reward_info['min_reward'] = np.min(self.data_pool.get_unit_data('summary_total_reward')) return summary_reward_info","title":"summary_reward_info()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.sync","text":"sync. Used in multi thread. Source code in AquaML/rlalgo/BaseRLAlgo.py def sync(self): \"\"\" sync. Used in multi thread. \"\"\" if self.level == 0: for key, model in self._sync_model_dict.items(): model.save_weights(self.cache_path + '/' + key + '.h5') else: for key, model in self._sync_model_dict.items(): model.load_weights(self.cache_path + '/' + key + '.h5') if self.log_std is not None: self.sync_log_std()","title":"sync()"},{"location":"BaseRLAlgo/#AquaML.rlalgo.BaseRLAlgo.BaseRLAlgo.sync_log_std","text":"sync log std. Source code in AquaML/rlalgo/BaseRLAlgo.py def sync_log_std(self): \"\"\" sync log std. \"\"\" if self.level == 0: self.log_std.set_value(self.tf_log_std.numpy()) # write log std to shared memory else: self.tf_log_std = tf.Variable(self.log_std.buffer, trainable=True) # read log std from shared memory options: show_source: false","title":"sync_log_std()"},{"location":"DataPool/","text":"Create and manage data units. When using supervised learning, the datapool contains features and lables. As for reinforcement learning, (s,a,r,s') will be contained. It can be used in parameter tuning Source code in AquaML/data/DataPool.py class DataPool: \"\"\"Create and manage data units. When using supervised learning, the datapool contains features and lables. As for reinforcement learning, (s,a,r,s') will be contained. It can be used in parameter tuning \"\"\" def __init__(self, name, level: int, computer_type: str = 'PC'): self.name = name # first level name self.data_pool = dict() self._computer_type = computer_type self.level = level # TODO: \u591a\u7ebf\u7a0b\u683c\u5f0f\u7684\u7edf\u4e00 def copy_from_exist_array(self, dataset: np.ndarray, name: str): \"\"\"copy data from np array and create data units. This function works in main thread. Args: dataset (np.ndarray): _description_ name (str): second level name. Algo serch param via this name. \"\"\" unit_name = self.name + '_' + name self.data_pool[name] = DataUnit(unit_name, dataset=dataset, level=self.level) def create_buffer_from_dic(self, info_dic: DataInfo): \"\"\" Create buffer. Usually, used in reinforcement learning. Main thread. Args: info_dic (DataInfo): store data information. \"\"\" for key, shape in info_dic.shape_dict.items(): # print(key, shape) self.data_pool[key] = DataUnit(self.name + '_' + key, shape=shape, dtype=info_dic.type_dict[key], computer_type=self._computer_type, level=self.level) def create_share_memory(self): \"\"\"create shared memory! \"\"\" for key, data_unit in self.data_pool.items(): data_unit.create_shared_memory() def read_shared_memory(self, info_dic: DataInfo): \"\"\"read shared memory. Sub thread. Args: info_dic (DataInfo): store data information. \"\"\" # create void data unit for name in info_dic.names: self.data_pool[name] = DataUnit(self.name + '_' + name, computer_type=self._computer_type, level=self.level, dtype=info_dic.type_dict[name]) # read shared memory for name, data_unit in self.data_pool.items(): data_unit.read_shared_memory(info_dic.shape_dict[name]) # TODO: \u5f53\u524d\u8fd8\u9700\u8981\u6307\u5b9ashape\uff0c\u5347\u7ea7\u81ea\u52a8\u83b7\u53d6\u7248\u672c def multi_init(self, info_dic: DataInfo, type: str = 'dataset'): \"\"\"multi thread initial. Args: info_dic (DataInfo): store data information. type (str, optional): 'dataset' or 'buffer'. Defaults to 'dataset'. \"\"\" if type == 'dataset': \"\"\" 1. copy from exist array 2. create share memory 3. read shared memory \"\"\" if self.level == 0: for key, dataset in info_dic.dataset_dict.items(): self.copy_from_exist_array(dataset, key) if self._computer_type == 'PC': self.create_share_memory() else: # waite for main thread to create share memory import time time.sleep(6) self.read_shared_memory(info_dic) elif type == 'buffer': \"\"\" Used for reinforcement learning. 1. create buffer from dic 2. create share memory 3. read shared memory \"\"\" if self.level == 0: self.create_buffer_from_dic(info_dic) if self._computer_type == 'PC': self.create_share_memory() else: import time time.sleep(6) self.read_shared_memory(info_dic) def get_unit(self, name: str): \"\"\"get data unit. Args: name (str): second level name. Algo search param via this name. Returns: DataUnit: data unit. \"\"\" return self.data_pool[name] # get data from data unit def get_unit_data(self, name: str): \"\"\"get data from data unit. Args: name (str): second level name. Algo search param via this name. Returns: np.ndarray: data. \"\"\" return self.data_pool[name].buffer # close shared memory buffer def close(self): \"\"\"close shared memory buffer. \"\"\" for name, data_unit in self.data_pool.items(): self.data_pool[name].close() def add_unit(self, name: str, data_unit: DataUnit): \"\"\"add data unit. Args: name (str): second level name. Algo search param via this name. data_unit (DataUnit): data unit. \"\"\" self.data_pool[name] = data_unit def store(self, data_dict: dict, index: int, prefix: str = ''): \"\"\"store data. This function stores data in data units. data_dict no need to contain datapool's all data units. Args: data_dict (dict): data. index (int): index. prefix (str, optional): prefix. Defaults to ''. \"\"\" for name, data in data_dict.items(): self.data_pool[prefix + name].store(data, index) def get_data_by_indices(self, indices, names: tuple): \"\"\"get data by indices. Args: indices (list): indice. names (tuple): second level name. Algo search param via this name. Returns: dict: data. \"\"\" data_dict = dict() for name in names: data_dict[name] = self.data_pool[name].get_data_by_indices(indices) return data_dict add_unit(name, data_unit) add data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required data_unit DataUnit data unit. required Source code in AquaML/data/DataPool.py def add_unit(self, name: str, data_unit: DataUnit): \"\"\"add data unit. Args: name (str): second level name. Algo search param via this name. data_unit (DataUnit): data unit. \"\"\" self.data_pool[name] = data_unit close() close shared memory buffer. Source code in AquaML/data/DataPool.py def close(self): \"\"\"close shared memory buffer. \"\"\" for name, data_unit in self.data_pool.items(): self.data_pool[name].close() copy_from_exist_array(dataset, name) copy data from np array and create data units. This function works in main thread. Parameters: Name Type Description Default dataset np . ndarray description required name str second level name. Algo serch param via this name. required Source code in AquaML/data/DataPool.py def copy_from_exist_array(self, dataset: np.ndarray, name: str): \"\"\"copy data from np array and create data units. This function works in main thread. Args: dataset (np.ndarray): _description_ name (str): second level name. Algo serch param via this name. \"\"\" unit_name = self.name + '_' + name self.data_pool[name] = DataUnit(unit_name, dataset=dataset, level=self.level) create_buffer_from_dic(info_dic) Create buffer. Usually, used in reinforcement learning. Main thread. Parameters: Name Type Description Default info_dic DataInfo store data information. required Source code in AquaML/data/DataPool.py def create_buffer_from_dic(self, info_dic: DataInfo): \"\"\" Create buffer. Usually, used in reinforcement learning. Main thread. Args: info_dic (DataInfo): store data information. \"\"\" for key, shape in info_dic.shape_dict.items(): # print(key, shape) self.data_pool[key] = DataUnit(self.name + '_' + key, shape=shape, dtype=info_dic.type_dict[key], computer_type=self._computer_type, level=self.level) create_share_memory() create shared memory! Source code in AquaML/data/DataPool.py def create_share_memory(self): \"\"\"create shared memory! \"\"\" for key, data_unit in self.data_pool.items(): data_unit.create_shared_memory() get_data_by_indices(indices, names) get data by indices. Parameters: Name Type Description Default indices list indice. required names tuple second level name. Algo search param via this name. required Returns: Name Type Description dict data. Source code in AquaML/data/DataPool.py def get_data_by_indices(self, indices, names: tuple): \"\"\"get data by indices. Args: indices (list): indice. names (tuple): second level name. Algo search param via this name. Returns: dict: data. \"\"\" data_dict = dict() for name in names: data_dict[name] = self.data_pool[name].get_data_by_indices(indices) return data_dict get_unit(name) get data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required Returns: Name Type Description DataUnit data unit. Source code in AquaML/data/DataPool.py def get_unit(self, name: str): \"\"\"get data unit. Args: name (str): second level name. Algo search param via this name. Returns: DataUnit: data unit. \"\"\" return self.data_pool[name] get_unit_data(name) get data from data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required Returns: Type Description np.ndarray: data. Source code in AquaML/data/DataPool.py def get_unit_data(self, name: str): \"\"\"get data from data unit. Args: name (str): second level name. Algo search param via this name. Returns: np.ndarray: data. \"\"\" return self.data_pool[name].buffer multi_init(info_dic, type='dataset') multi thread initial. Parameters: Name Type Description Default info_dic DataInfo store data information. required type str 'dataset' or 'buffer'. Defaults to 'dataset'. 'dataset' Source code in AquaML/data/DataPool.py def multi_init(self, info_dic: DataInfo, type: str = 'dataset'): \"\"\"multi thread initial. Args: info_dic (DataInfo): store data information. type (str, optional): 'dataset' or 'buffer'. Defaults to 'dataset'. \"\"\" if type == 'dataset': \"\"\" 1. copy from exist array 2. create share memory 3. read shared memory \"\"\" if self.level == 0: for key, dataset in info_dic.dataset_dict.items(): self.copy_from_exist_array(dataset, key) if self._computer_type == 'PC': self.create_share_memory() else: # waite for main thread to create share memory import time time.sleep(6) self.read_shared_memory(info_dic) elif type == 'buffer': \"\"\" Used for reinforcement learning. 1. create buffer from dic 2. create share memory 3. read shared memory \"\"\" if self.level == 0: self.create_buffer_from_dic(info_dic) if self._computer_type == 'PC': self.create_share_memory() else: import time time.sleep(6) self.read_shared_memory(info_dic) read_shared_memory(info_dic) read shared memory. Sub thread. Parameters: Name Type Description Default info_dic DataInfo store data information. required Source code in AquaML/data/DataPool.py def read_shared_memory(self, info_dic: DataInfo): \"\"\"read shared memory. Sub thread. Args: info_dic (DataInfo): store data information. \"\"\" # create void data unit for name in info_dic.names: self.data_pool[name] = DataUnit(self.name + '_' + name, computer_type=self._computer_type, level=self.level, dtype=info_dic.type_dict[name]) # read shared memory for name, data_unit in self.data_pool.items(): data_unit.read_shared_memory(info_dic.shape_dict[name]) store(data_dict, index, prefix='') store data. This function stores data in data units. data_dict no need to contain datapool's all data units. Parameters: Name Type Description Default data_dict dict data. required index int index. required prefix str prefix. Defaults to ''. '' Source code in AquaML/data/DataPool.py def store(self, data_dict: dict, index: int, prefix: str = ''): \"\"\"store data. This function stores data in data units. data_dict no need to contain datapool's all data units. Args: data_dict (dict): data. index (int): index. prefix (str, optional): prefix. Defaults to ''. \"\"\" for name, data in data_dict.items(): self.data_pool[prefix + name].store(data, index) options: show_source: false","title":"data:DataPool"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.add_unit","text":"add data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required data_unit DataUnit data unit. required Source code in AquaML/data/DataPool.py def add_unit(self, name: str, data_unit: DataUnit): \"\"\"add data unit. Args: name (str): second level name. Algo search param via this name. data_unit (DataUnit): data unit. \"\"\" self.data_pool[name] = data_unit","title":"add_unit()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.close","text":"close shared memory buffer. Source code in AquaML/data/DataPool.py def close(self): \"\"\"close shared memory buffer. \"\"\" for name, data_unit in self.data_pool.items(): self.data_pool[name].close()","title":"close()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.copy_from_exist_array","text":"copy data from np array and create data units. This function works in main thread. Parameters: Name Type Description Default dataset np . ndarray description required name str second level name. Algo serch param via this name. required Source code in AquaML/data/DataPool.py def copy_from_exist_array(self, dataset: np.ndarray, name: str): \"\"\"copy data from np array and create data units. This function works in main thread. Args: dataset (np.ndarray): _description_ name (str): second level name. Algo serch param via this name. \"\"\" unit_name = self.name + '_' + name self.data_pool[name] = DataUnit(unit_name, dataset=dataset, level=self.level)","title":"copy_from_exist_array()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.create_buffer_from_dic","text":"Create buffer. Usually, used in reinforcement learning. Main thread. Parameters: Name Type Description Default info_dic DataInfo store data information. required Source code in AquaML/data/DataPool.py def create_buffer_from_dic(self, info_dic: DataInfo): \"\"\" Create buffer. Usually, used in reinforcement learning. Main thread. Args: info_dic (DataInfo): store data information. \"\"\" for key, shape in info_dic.shape_dict.items(): # print(key, shape) self.data_pool[key] = DataUnit(self.name + '_' + key, shape=shape, dtype=info_dic.type_dict[key], computer_type=self._computer_type, level=self.level)","title":"create_buffer_from_dic()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.create_share_memory","text":"create shared memory! Source code in AquaML/data/DataPool.py def create_share_memory(self): \"\"\"create shared memory! \"\"\" for key, data_unit in self.data_pool.items(): data_unit.create_shared_memory()","title":"create_share_memory()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.get_data_by_indices","text":"get data by indices. Parameters: Name Type Description Default indices list indice. required names tuple second level name. Algo search param via this name. required Returns: Name Type Description dict data. Source code in AquaML/data/DataPool.py def get_data_by_indices(self, indices, names: tuple): \"\"\"get data by indices. Args: indices (list): indice. names (tuple): second level name. Algo search param via this name. Returns: dict: data. \"\"\" data_dict = dict() for name in names: data_dict[name] = self.data_pool[name].get_data_by_indices(indices) return data_dict","title":"get_data_by_indices()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.get_unit","text":"get data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required Returns: Name Type Description DataUnit data unit. Source code in AquaML/data/DataPool.py def get_unit(self, name: str): \"\"\"get data unit. Args: name (str): second level name. Algo search param via this name. Returns: DataUnit: data unit. \"\"\" return self.data_pool[name]","title":"get_unit()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.get_unit_data","text":"get data from data unit. Parameters: Name Type Description Default name str second level name. Algo search param via this name. required Returns: Type Description np.ndarray: data. Source code in AquaML/data/DataPool.py def get_unit_data(self, name: str): \"\"\"get data from data unit. Args: name (str): second level name. Algo search param via this name. Returns: np.ndarray: data. \"\"\" return self.data_pool[name].buffer","title":"get_unit_data()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.multi_init","text":"multi thread initial. Parameters: Name Type Description Default info_dic DataInfo store data information. required type str 'dataset' or 'buffer'. Defaults to 'dataset'. 'dataset' Source code in AquaML/data/DataPool.py def multi_init(self, info_dic: DataInfo, type: str = 'dataset'): \"\"\"multi thread initial. Args: info_dic (DataInfo): store data information. type (str, optional): 'dataset' or 'buffer'. Defaults to 'dataset'. \"\"\" if type == 'dataset': \"\"\" 1. copy from exist array 2. create share memory 3. read shared memory \"\"\" if self.level == 0: for key, dataset in info_dic.dataset_dict.items(): self.copy_from_exist_array(dataset, key) if self._computer_type == 'PC': self.create_share_memory() else: # waite for main thread to create share memory import time time.sleep(6) self.read_shared_memory(info_dic) elif type == 'buffer': \"\"\" Used for reinforcement learning. 1. create buffer from dic 2. create share memory 3. read shared memory \"\"\" if self.level == 0: self.create_buffer_from_dic(info_dic) if self._computer_type == 'PC': self.create_share_memory() else: import time time.sleep(6) self.read_shared_memory(info_dic)","title":"multi_init()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.read_shared_memory","text":"read shared memory. Sub thread. Parameters: Name Type Description Default info_dic DataInfo store data information. required Source code in AquaML/data/DataPool.py def read_shared_memory(self, info_dic: DataInfo): \"\"\"read shared memory. Sub thread. Args: info_dic (DataInfo): store data information. \"\"\" # create void data unit for name in info_dic.names: self.data_pool[name] = DataUnit(self.name + '_' + name, computer_type=self._computer_type, level=self.level, dtype=info_dic.type_dict[name]) # read shared memory for name, data_unit in self.data_pool.items(): data_unit.read_shared_memory(info_dic.shape_dict[name])","title":"read_shared_memory()"},{"location":"DataPool/#AquaML.data.DataPool.DataPool.store","text":"store data. This function stores data in data units. data_dict no need to contain datapool's all data units. Parameters: Name Type Description Default data_dict dict data. required index int index. required prefix str prefix. Defaults to ''. '' Source code in AquaML/data/DataPool.py def store(self, data_dict: dict, index: int, prefix: str = ''): \"\"\"store data. This function stores data in data units. data_dict no need to contain datapool's all data units. Args: data_dict (dict): data. index (int): index. prefix (str, optional): prefix. Defaults to ''. \"\"\" for name, data in data_dict.items(): self.data_pool[prefix + name].store(data, index) options: show_source: false","title":"store()"},{"location":"DataUnit/","text":"The smallest data storage unit. It can be used in HPC (MPI) and shared memory system. It can load from exit numpy array for data set learning. Source code in AquaML/data/DataUnit.py class DataUnit: \"\"\" The smallest data storage unit. It can be used in HPC (MPI) and shared memory system. It can load from exit numpy array for data set learning. \"\"\" def __init__(self, name: str, shape=None, dtype=np.float32, computer_type='PC', dataset: np.ndarray = None, level=None): \"\"\"Create data unit. If shape is not none, this data unit is used in main thread. The unit is created depend on your computer type. If you use in high performance computer(HPC), shared memmory isn't used. Args: name (str): _description_ shape (tuple, None): shape of data unit.If shape is none, the unit is in sub thread. (buffersize, dims) Defaults to None. dtype (nd.type): type of this data unit. Defaults to np.float32. computer_type(str):When computer type is 'PC', mutlti thread is based on shared memory.Defaults to 'PC'. dataset(np.ndarry, None): create unit from dataset. Defaults to None level(int, None): Clarify level. \"\"\" self.name = name self._shape = shape self._dtype = dtype self._computer_type = computer_type if shape is not None: self.level = 0 self._buffer = np.zeros(shape=shape, dtype=self._dtype) self.__nbytes = self._buffer.nbytes else: self.level = 1 self._buffer = None self.__nbytes = None if dataset is not None: self.copy_from_exist_array(dataset) if level is not None: self.level = level self.shm_buffer = None def copy_from_exist_array(self, dataset: np.ndarray, level: int = 0): \"\"\"Copy data from exist np array. Args: dataset (np.ndarray): Dataset. level (int): Thread level. Defaults to 0. \"\"\" self._buffer = copy.deepcopy(dataset) del dataset self.level = level self.__nbytes = self._buffer.nbytes self._dtype = self._buffer.dtype def create_shared_memory(self): \"\"\"Create shared-memory. \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 0: self.shm_buffer = shared_memory.SharedMemory(create=True, size=self.__nbytes, name=self.name) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is sub thread!\") def read_shared_memory(self, shape: tuple): \"\"\"Read shared memory. Args: shape (tuple): Buffer shape. Raises: Exception: can't be used in main thread! \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 1: self.__nbytes = self.compute_nbytes(shape) self._shape = shape self.shm_buffer = shared_memory.SharedMemory(name=self.name, size=self.__nbytes) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is main thread!\") def compute_nbytes(self, shape: tuple) -> int: \"\"\"Compute numpy array nbytes. Args: shape (tuple): _description_ Returns: _type_: int \"\"\" a = np.arange(1, dtype=self._dtype) single_nbytes = a.nbytes total_size = 1 for size in shape: total_size = total_size * size total_size = total_size * single_nbytes return total_size def store(self, data, index: int): \"\"\"Store data in buffer. Args: data (any): feature in the training. index (int): index of data. \"\"\" self._buffer[index] = data # set value. This is for args def set_value(self, value): \"\"\"Set value. Args: value (any): value. \"\"\" if self.level == 0: self._buffer[:] = value[:] else: raise Exception(\"Current thread is sub thread!\") # get data slice def get_slice(self, start: int, end: int): \"\"\"Get slice. Args: start (int): start index. end (int): end index. Returns: _type_: np.ndarray \"\"\" return self._buffer[start:end] def get_data_by_indices(self, indices): \"\"\"Get data by indenes. Args: indices : indices. Returns: _type_: np.ndarray \"\"\" return self._buffer[indices] @property def buffer(self): \"\"\"Get buffer. Returns: _type_: np.ndarray \"\"\" return self._buffer def close(self): # TODO: \u5143\u6570\u636e\u91cc\u9762\u4e0d\u8981\u52a0\u5165\u7b49\u5f85\u6307\u4ee4 \"\"\" delete data. \"\"\" del self._buffer if self.shm_buffer is not None: if self.level == 1: self.shm_buffer.close() self.shm_buffer.unlink() else: import time time.sleep(0.5) self.shm_buffer.close() @property def shape(self): \"\"\"Get shape. Returns: _type_: tuple \"\"\" return self._shape @property def dtype(self): \"\"\"Get dtype. Returns: _type_: np.type \"\"\" return self._dtype buffer property Get buffer. Returns: Name Type Description _type_ np.ndarray dtype property Get dtype. Returns: Name Type Description _type_ np.type shape property Get shape. Returns: Name Type Description _type_ tuple __init__(name, shape=None, dtype=np.float32, computer_type='PC', dataset=None, level=None) Create data unit. If shape is not none, this data unit is used in main thread. The unit is created depend on your computer type. If you use in high performance computer(HPC), shared memmory isn't used. Parameters: Name Type Description Default name str description required shape tuple , None shape of data unit.If shape is none, the unit is in sub thread. None dtype nd . type type of this data unit. Defaults to np.float32. np.float32 computer_type(str) When computer type is 'PC', mutlti thread is based on shared memory.Defaults to 'PC'. required dataset(np.ndarry, None create unit from dataset. Defaults to None required level(int, None Clarify level. required Source code in AquaML/data/DataUnit.py def __init__(self, name: str, shape=None, dtype=np.float32, computer_type='PC', dataset: np.ndarray = None, level=None): \"\"\"Create data unit. If shape is not none, this data unit is used in main thread. The unit is created depend on your computer type. If you use in high performance computer(HPC), shared memmory isn't used. Args: name (str): _description_ shape (tuple, None): shape of data unit.If shape is none, the unit is in sub thread. (buffersize, dims) Defaults to None. dtype (nd.type): type of this data unit. Defaults to np.float32. computer_type(str):When computer type is 'PC', mutlti thread is based on shared memory.Defaults to 'PC'. dataset(np.ndarry, None): create unit from dataset. Defaults to None level(int, None): Clarify level. \"\"\" self.name = name self._shape = shape self._dtype = dtype self._computer_type = computer_type if shape is not None: self.level = 0 self._buffer = np.zeros(shape=shape, dtype=self._dtype) self.__nbytes = self._buffer.nbytes else: self.level = 1 self._buffer = None self.__nbytes = None if dataset is not None: self.copy_from_exist_array(dataset) if level is not None: self.level = level self.shm_buffer = None close() delete data. Source code in AquaML/data/DataUnit.py def close(self): # TODO: \u5143\u6570\u636e\u91cc\u9762\u4e0d\u8981\u52a0\u5165\u7b49\u5f85\u6307\u4ee4 \"\"\" delete data. \"\"\" del self._buffer if self.shm_buffer is not None: if self.level == 1: self.shm_buffer.close() self.shm_buffer.unlink() else: import time time.sleep(0.5) self.shm_buffer.close() compute_nbytes(shape) Compute numpy array nbytes. Parameters: Name Type Description Default shape tuple description required Returns: Name Type Description _type_ int int Source code in AquaML/data/DataUnit.py def compute_nbytes(self, shape: tuple) -> int: \"\"\"Compute numpy array nbytes. Args: shape (tuple): _description_ Returns: _type_: int \"\"\" a = np.arange(1, dtype=self._dtype) single_nbytes = a.nbytes total_size = 1 for size in shape: total_size = total_size * size total_size = total_size * single_nbytes return total_size copy_from_exist_array(dataset, level=0) Copy data from exist np array. Parameters: Name Type Description Default dataset np . ndarray Dataset. required level int Thread level. Defaults to 0. 0 Source code in AquaML/data/DataUnit.py def copy_from_exist_array(self, dataset: np.ndarray, level: int = 0): \"\"\"Copy data from exist np array. Args: dataset (np.ndarray): Dataset. level (int): Thread level. Defaults to 0. \"\"\" self._buffer = copy.deepcopy(dataset) del dataset self.level = level self.__nbytes = self._buffer.nbytes self._dtype = self._buffer.dtype create_shared_memory() Create shared-memory. Source code in AquaML/data/DataUnit.py def create_shared_memory(self): \"\"\"Create shared-memory. \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 0: self.shm_buffer = shared_memory.SharedMemory(create=True, size=self.__nbytes, name=self.name) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is sub thread!\") get_data_by_indices(indices) Get data by indenes. Parameters: Name Type Description Default indices indices. required Returns: Name Type Description _type_ np.ndarray Source code in AquaML/data/DataUnit.py def get_data_by_indices(self, indices): \"\"\"Get data by indenes. Args: indices : indices. Returns: _type_: np.ndarray \"\"\" return self._buffer[indices] get_slice(start, end) Get slice. Parameters: Name Type Description Default start int start index. required end int end index. required Returns: Name Type Description _type_ np.ndarray Source code in AquaML/data/DataUnit.py def get_slice(self, start: int, end: int): \"\"\"Get slice. Args: start (int): start index. end (int): end index. Returns: _type_: np.ndarray \"\"\" return self._buffer[start:end] read_shared_memory(shape) Read shared memory. Parameters: Name Type Description Default shape tuple Buffer shape. required Raises: Type Description Exception can't be used in main thread! Source code in AquaML/data/DataUnit.py def read_shared_memory(self, shape: tuple): \"\"\"Read shared memory. Args: shape (tuple): Buffer shape. Raises: Exception: can't be used in main thread! \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 1: self.__nbytes = self.compute_nbytes(shape) self._shape = shape self.shm_buffer = shared_memory.SharedMemory(name=self.name, size=self.__nbytes) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is main thread!\") set_value(value) Set value. Parameters: Name Type Description Default value any value. required Source code in AquaML/data/DataUnit.py def set_value(self, value): \"\"\"Set value. Args: value (any): value. \"\"\" if self.level == 0: self._buffer[:] = value[:] else: raise Exception(\"Current thread is sub thread!\") store(data, index) Store data in buffer. Parameters: Name Type Description Default data any feature in the training. required index int index of data. required Source code in AquaML/data/DataUnit.py def store(self, data, index: int): \"\"\"Store data in buffer. Args: data (any): feature in the training. index (int): index of data. \"\"\" self._buffer[index] = data options: show_source: false","title":"data:DataUnit"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.buffer","text":"Get buffer. Returns: Name Type Description _type_ np.ndarray","title":"buffer"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.dtype","text":"Get dtype. Returns: Name Type Description _type_ np.type","title":"dtype"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.shape","text":"Get shape. Returns: Name Type Description _type_ tuple","title":"shape"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.__init__","text":"Create data unit. If shape is not none, this data unit is used in main thread. The unit is created depend on your computer type. If you use in high performance computer(HPC), shared memmory isn't used. Parameters: Name Type Description Default name str description required shape tuple , None shape of data unit.If shape is none, the unit is in sub thread. None dtype nd . type type of this data unit. Defaults to np.float32. np.float32 computer_type(str) When computer type is 'PC', mutlti thread is based on shared memory.Defaults to 'PC'. required dataset(np.ndarry, None create unit from dataset. Defaults to None required level(int, None Clarify level. required Source code in AquaML/data/DataUnit.py def __init__(self, name: str, shape=None, dtype=np.float32, computer_type='PC', dataset: np.ndarray = None, level=None): \"\"\"Create data unit. If shape is not none, this data unit is used in main thread. The unit is created depend on your computer type. If you use in high performance computer(HPC), shared memmory isn't used. Args: name (str): _description_ shape (tuple, None): shape of data unit.If shape is none, the unit is in sub thread. (buffersize, dims) Defaults to None. dtype (nd.type): type of this data unit. Defaults to np.float32. computer_type(str):When computer type is 'PC', mutlti thread is based on shared memory.Defaults to 'PC'. dataset(np.ndarry, None): create unit from dataset. Defaults to None level(int, None): Clarify level. \"\"\" self.name = name self._shape = shape self._dtype = dtype self._computer_type = computer_type if shape is not None: self.level = 0 self._buffer = np.zeros(shape=shape, dtype=self._dtype) self.__nbytes = self._buffer.nbytes else: self.level = 1 self._buffer = None self.__nbytes = None if dataset is not None: self.copy_from_exist_array(dataset) if level is not None: self.level = level self.shm_buffer = None","title":"__init__()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.close","text":"delete data. Source code in AquaML/data/DataUnit.py def close(self): # TODO: \u5143\u6570\u636e\u91cc\u9762\u4e0d\u8981\u52a0\u5165\u7b49\u5f85\u6307\u4ee4 \"\"\" delete data. \"\"\" del self._buffer if self.shm_buffer is not None: if self.level == 1: self.shm_buffer.close() self.shm_buffer.unlink() else: import time time.sleep(0.5) self.shm_buffer.close()","title":"close()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.compute_nbytes","text":"Compute numpy array nbytes. Parameters: Name Type Description Default shape tuple description required Returns: Name Type Description _type_ int int Source code in AquaML/data/DataUnit.py def compute_nbytes(self, shape: tuple) -> int: \"\"\"Compute numpy array nbytes. Args: shape (tuple): _description_ Returns: _type_: int \"\"\" a = np.arange(1, dtype=self._dtype) single_nbytes = a.nbytes total_size = 1 for size in shape: total_size = total_size * size total_size = total_size * single_nbytes return total_size","title":"compute_nbytes()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.copy_from_exist_array","text":"Copy data from exist np array. Parameters: Name Type Description Default dataset np . ndarray Dataset. required level int Thread level. Defaults to 0. 0 Source code in AquaML/data/DataUnit.py def copy_from_exist_array(self, dataset: np.ndarray, level: int = 0): \"\"\"Copy data from exist np array. Args: dataset (np.ndarray): Dataset. level (int): Thread level. Defaults to 0. \"\"\" self._buffer = copy.deepcopy(dataset) del dataset self.level = level self.__nbytes = self._buffer.nbytes self._dtype = self._buffer.dtype","title":"copy_from_exist_array()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.create_shared_memory","text":"Create shared-memory. Source code in AquaML/data/DataUnit.py def create_shared_memory(self): \"\"\"Create shared-memory. \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 0: self.shm_buffer = shared_memory.SharedMemory(create=True, size=self.__nbytes, name=self.name) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is sub thread!\")","title":"create_shared_memory()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.get_data_by_indices","text":"Get data by indenes. Parameters: Name Type Description Default indices indices. required Returns: Name Type Description _type_ np.ndarray Source code in AquaML/data/DataUnit.py def get_data_by_indices(self, indices): \"\"\"Get data by indenes. Args: indices : indices. Returns: _type_: np.ndarray \"\"\" return self._buffer[indices]","title":"get_data_by_indices()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.get_slice","text":"Get slice. Parameters: Name Type Description Default start int start index. required end int end index. required Returns: Name Type Description _type_ np.ndarray Source code in AquaML/data/DataUnit.py def get_slice(self, start: int, end: int): \"\"\"Get slice. Args: start (int): start index. end (int): end index. Returns: _type_: np.ndarray \"\"\" return self._buffer[start:end]","title":"get_slice()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.read_shared_memory","text":"Read shared memory. Parameters: Name Type Description Default shape tuple Buffer shape. required Raises: Type Description Exception can't be used in main thread! Source code in AquaML/data/DataUnit.py def read_shared_memory(self, shape: tuple): \"\"\"Read shared memory. Args: shape (tuple): Buffer shape. Raises: Exception: can't be used in main thread! \"\"\" if self._computer_type == 'HPC': warnings.warn(\"HPC can't support shared memory!\") if self.level == 1: self.__nbytes = self.compute_nbytes(shape) self._shape = shape self.shm_buffer = shared_memory.SharedMemory(name=self.name, size=self.__nbytes) self._buffer = np.ndarray(self._shape, dtype=self._dtype, buffer=self.shm_buffer.buf) else: raise Exception(\"Current thread is main thread!\")","title":"read_shared_memory()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.set_value","text":"Set value. Parameters: Name Type Description Default value any value. required Source code in AquaML/data/DataUnit.py def set_value(self, value): \"\"\"Set value. Args: value (any): value. \"\"\" if self.level == 0: self._buffer[:] = value[:] else: raise Exception(\"Current thread is sub thread!\")","title":"set_value()"},{"location":"DataUnit/#AquaML.data.DataUnit.DataUnit.store","text":"Store data in buffer. Parameters: Name Type Description Default data any feature in the training. required index int index of data. required Source code in AquaML/data/DataUnit.py def store(self, data, index: int): \"\"\"Store data in buffer. Args: data (any): feature in the training. index (int): index of data. \"\"\" self._buffer[index] = data options: show_source: false","title":"store()"},{"location":"ExplorePolicy/","text":"ExplorePolicyBase Bases: abc . ABC Explore policy base class. Example of using this class: explore_policy = GaussianExplorePolicy(shape) action, prob = explore_policy(mu, log_std) New explore policy should inherit this class. Another things you should do: 1. declare the input_info. It is a tuple. It is used to get the input of explore policy. Notice: the order of input_info is the same as the order of input of scale_out function. Source code in AquaML/rlalgo/ExplorePolicy.py class ExplorePolicyBase(abc.ABC): \"\"\"Explore policy base class. Example of using this class: explore_policy = GaussianExplorePolicy(shape) action, prob = explore_policy(mu, log_std) New explore policy should inherit this class. Another things you should do: 1. declare the input_info. It is a tuple. It is used to get the input of explore policy. Notice: the order of input_info is the same as the order of input of scale_out function. \"\"\" def __init__(self, shape): self.shape = shape self.input_name = None # tuple @abc.abstractmethod def noise_and_prob(self, batch_size=1): \"\"\" This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. \"\"\" @abc.abstractmethod def scale_out(self, *args, **kwargs): \"\"\" Scale the action to the range of environment. \"\"\" def __call__(self, inputs_dict: dict): \"\"\"inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} args: inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. \"\"\" # get inputs from inputs_dict inputs = [] for key in self.input_name: inputs.append(inputs_dict[key]) return self.scale_out(*inputs) @abc.abstractmethod def resample_prob(self, mu, log_std, action): \"\"\" Resample the probability of action. \"\"\" __call__(inputs_dict) inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. Source code in AquaML/rlalgo/ExplorePolicy.py def __call__(self, inputs_dict: dict): \"\"\"inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} args: inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. \"\"\" # get inputs from inputs_dict inputs = [] for key in self.input_name: inputs.append(inputs_dict[key]) return self.scale_out(*inputs) noise_and_prob(batch_size=1) abstractmethod This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def noise_and_prob(self, batch_size=1): \"\"\" This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. \"\"\" resample_prob(mu, log_std, action) abstractmethod Resample the probability of action. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def resample_prob(self, mu, log_std, action): \"\"\" Resample the probability of action. \"\"\" scale_out(*args, **kwargs) abstractmethod Scale the action to the range of environment. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def scale_out(self, *args, **kwargs): \"\"\" Scale the action to the range of environment. \"\"\" options: show_source: false Bases: ExplorePolicyBase Source code in AquaML/rlalgo/ExplorePolicy.py class GaussianExplorePolicy(ExplorePolicyBase): def __init__(self, shape): super().__init__(shape) mu = tf.zeros(shape, dtype=tf.float32) sigma = tf.ones(shape, dtype=tf.float32) self.dist = tfp.distributions.Normal(loc=mu, scale=sigma) self.input_name = ('action', 'log_std') @tf.function def noise_and_prob(self, batch_size=1): noise = self.dist.sample(batch_size) prob = self.dist.prob(noise) return noise, prob def scale_out(self, mu, log_std): sigma = tf.exp(log_std) noise, prob = self.noise_and_prob() action = mu + sigma * noise return action, prob def resample_prob(self, mu, std, action): # sigma = tf.exp(log_std) dist = tfp.distributions.Normal(loc=mu, scale=std) log_prob = dist.log_prob(action) return log_prob options: show_source: false Bases: ExplorePolicyBase Source code in AquaML/rlalgo/ExplorePolicy.py class VoidExplorePolicy(ExplorePolicyBase): def __init__(self, shape): super().__init__(shape) self.input_name = ('action',) @tf.function def noise_and_prob(self, batch_size=1): return tf.zeros((batch_size, *self.shape)), tf.ones((batch_size, *self.shape)) def scale_out(self, mu): return mu, tf.ones((1, *self.shape)) def resample_prob(self, mu, log_std, action): return tf.ones((1, *self.shape)) options: show_source: false","title":"rlalgo:ExplorePolicy"},{"location":"ExplorePolicy/#AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase","text":"Bases: abc . ABC Explore policy base class. Example of using this class: explore_policy = GaussianExplorePolicy(shape) action, prob = explore_policy(mu, log_std) New explore policy should inherit this class. Another things you should do: 1. declare the input_info. It is a tuple. It is used to get the input of explore policy. Notice: the order of input_info is the same as the order of input of scale_out function. Source code in AquaML/rlalgo/ExplorePolicy.py class ExplorePolicyBase(abc.ABC): \"\"\"Explore policy base class. Example of using this class: explore_policy = GaussianExplorePolicy(shape) action, prob = explore_policy(mu, log_std) New explore policy should inherit this class. Another things you should do: 1. declare the input_info. It is a tuple. It is used to get the input of explore policy. Notice: the order of input_info is the same as the order of input of scale_out function. \"\"\" def __init__(self, shape): self.shape = shape self.input_name = None # tuple @abc.abstractmethod def noise_and_prob(self, batch_size=1): \"\"\" This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. \"\"\" @abc.abstractmethod def scale_out(self, *args, **kwargs): \"\"\" Scale the action to the range of environment. \"\"\" def __call__(self, inputs_dict: dict): \"\"\"inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} args: inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. \"\"\" # get inputs from inputs_dict inputs = [] for key in self.input_name: inputs.append(inputs_dict[key]) return self.scale_out(*inputs) @abc.abstractmethod def resample_prob(self, mu, log_std, action): \"\"\" Resample the probability of action. \"\"\"","title":"ExplorePolicyBase"},{"location":"ExplorePolicy/#AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase.__call__","text":"inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. Source code in AquaML/rlalgo/ExplorePolicy.py def __call__(self, inputs_dict: dict): \"\"\"inputs_dict is a dict. The key is the name of input. The value is the input. inputs_dict must contain all the output of actor model. such as: inputs_dict = {'mu':mu, 'log_std':log_std} args: inputs_dict (dict): dict of inputs. The key is the name of input. The value is the input. return: action (tf.Tensor): action of environment. prob (tf.Tensor): probability of action. \"\"\" # get inputs from inputs_dict inputs = [] for key in self.input_name: inputs.append(inputs_dict[key]) return self.scale_out(*inputs)","title":"__call__()"},{"location":"ExplorePolicy/#AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase.noise_and_prob","text":"This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def noise_and_prob(self, batch_size=1): \"\"\" This function must use tf.function to accelerate. All the exploration noise use reparameter tricks. \"\"\"","title":"noise_and_prob()"},{"location":"ExplorePolicy/#AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase.resample_prob","text":"Resample the probability of action. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def resample_prob(self, mu, log_std, action): \"\"\" Resample the probability of action. \"\"\"","title":"resample_prob()"},{"location":"ExplorePolicy/#AquaML.rlalgo.ExplorePolicy.ExplorePolicyBase.scale_out","text":"Scale the action to the range of environment. Source code in AquaML/rlalgo/ExplorePolicy.py @abc.abstractmethod def scale_out(self, *args, **kwargs): \"\"\" Scale the action to the range of environment. \"\"\" options: show_source: false Bases: ExplorePolicyBase Source code in AquaML/rlalgo/ExplorePolicy.py class GaussianExplorePolicy(ExplorePolicyBase): def __init__(self, shape): super().__init__(shape) mu = tf.zeros(shape, dtype=tf.float32) sigma = tf.ones(shape, dtype=tf.float32) self.dist = tfp.distributions.Normal(loc=mu, scale=sigma) self.input_name = ('action', 'log_std') @tf.function def noise_and_prob(self, batch_size=1): noise = self.dist.sample(batch_size) prob = self.dist.prob(noise) return noise, prob def scale_out(self, mu, log_std): sigma = tf.exp(log_std) noise, prob = self.noise_and_prob() action = mu + sigma * noise return action, prob def resample_prob(self, mu, std, action): # sigma = tf.exp(log_std) dist = tfp.distributions.Normal(loc=mu, scale=std) log_prob = dist.log_prob(action) return log_prob options: show_source: false Bases: ExplorePolicyBase Source code in AquaML/rlalgo/ExplorePolicy.py class VoidExplorePolicy(ExplorePolicyBase): def __init__(self, shape): super().__init__(shape) self.input_name = ('action',) @tf.function def noise_and_prob(self, batch_size=1): return tf.zeros((batch_size, *self.shape)), tf.ones((batch_size, *self.shape)) def scale_out(self, mu): return mu, tf.ones((1, *self.shape)) def resample_prob(self, mu, log_std, action): return tf.ones((1, *self.shape)) options: show_source: false","title":"scale_out()"}]}